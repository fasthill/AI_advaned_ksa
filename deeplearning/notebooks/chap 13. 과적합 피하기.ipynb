{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "musical-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-surgery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "classical-incident",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       9   ...      51      52      53      54      55      56      57  \\\n",
       "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "\n",
       "       58      59  60  \n",
       "0  0.0090  0.0032   R  \n",
       "1  0.0052  0.0044   R  \n",
       "2  0.0095  0.0078   R  \n",
       "3  0.0040  0.0117   R  \n",
       "4  0.0107  0.0094   R  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/sonar.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "american-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.values\n",
    "X = dataset[:, :60].astype(np.float64) # 숫자라도 astype을 해 주는 것이 좋음.\n",
    "Y_obj = dataset[:, 60]\n",
    "e = LabelEncoder() # \n",
    "e.fit(Y_obj)\n",
    "Y = e.transform(Y_obj)\n",
    "# Y_encoded = to_categorical(Y) # one hit encoding을 하면 안됨. 이중 분류이기 때문에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fabulous-tunisia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02  , 0.0371, 0.0428, ..., 0.0084, 0.009 , 0.0032],\n",
       "       [0.0453, 0.0523, 0.0843, ..., 0.0049, 0.0052, 0.0044],\n",
       "       [0.0262, 0.0582, 0.1099, ..., 0.0164, 0.0095, 0.0078],\n",
       "       ...,\n",
       "       [0.0522, 0.0437, 0.018 , ..., 0.0138, 0.0077, 0.0031],\n",
       "       [0.0303, 0.0353, 0.049 , ..., 0.0079, 0.0036, 0.0048],\n",
       "       [0.026 , 0.0363, 0.0136, ..., 0.0036, 0.0061, 0.0115]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "productive-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "tf.random.set_seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "selected-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim=60, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "introductory-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "comparative-tucson",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "42/42 [==============================] - 2s 3ms/step - loss: 0.6795 - accuracy: 0.5913\n",
      "Epoch 2/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.6517 - accuracy: 0.6250\n",
      "Epoch 3/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.6328 - accuracy: 0.6587\n",
      "Epoch 4/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.5994 - accuracy: 0.7067\n",
      "Epoch 5/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.5664 - accuracy: 0.7644\n",
      "Epoch 6/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.5420 - accuracy: 0.7500\n",
      "Epoch 7/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.5088 - accuracy: 0.7644\n",
      "Epoch 8/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4829 - accuracy: 0.7740\n",
      "Epoch 9/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4707 - accuracy: 0.7788\n",
      "Epoch 10/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4490 - accuracy: 0.7788\n",
      "Epoch 11/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.4448 - accuracy: 0.8029\n",
      "Epoch 12/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.4182 - accuracy: 0.7981\n",
      "Epoch 13/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4082 - accuracy: 0.8221\n",
      "Epoch 14/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.4029 - accuracy: 0.8077\n",
      "Epoch 15/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.3934 - accuracy: 0.7933\n",
      "Epoch 16/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.3954 - accuracy: 0.8317\n",
      "Epoch 17/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.3793 - accuracy: 0.8413\n",
      "Epoch 18/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.3734 - accuracy: 0.8125\n",
      "Epoch 19/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3787 - accuracy: 0.8413\n",
      "Epoch 20/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3660 - accuracy: 0.8125\n",
      "Epoch 21/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.3573 - accuracy: 0.8365\n",
      "Epoch 22/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3578 - accuracy: 0.8558\n",
      "Epoch 23/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3419 - accuracy: 0.8510\n",
      "Epoch 24/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.3388 - accuracy: 0.8606\n",
      "Epoch 25/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.3367 - accuracy: 0.8510\n",
      "Epoch 26/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.3287 - accuracy: 0.8510\n",
      "Epoch 27/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.3406 - accuracy: 0.8413\n",
      "Epoch 28/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3336 - accuracy: 0.8654\n",
      "Epoch 29/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3196 - accuracy: 0.8606\n",
      "Epoch 30/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3097 - accuracy: 0.8750\n",
      "Epoch 31/200\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2925 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3032 - accuracy: 0.8654\n",
      "Epoch 32/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.3231 - accuracy: 0.8510\n",
      "Epoch 33/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2880 - accuracy: 0.8894\n",
      "Epoch 34/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3018 - accuracy: 0.8462\n",
      "Epoch 35/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2927 - accuracy: 0.8750: 0s - loss: 0.2919 - accuracy: 0.\n",
      "Epoch 36/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.2884 - accuracy: 0.8654\n",
      "Epoch 37/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.2824 - accuracy: 0.8990\n",
      "Epoch 38/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.2861 - accuracy: 0.8846\n",
      "Epoch 39/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.2645 - accuracy: 0.9038: 0s - loss: 0.2164 - accuracy: \n",
      "Epoch 40/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2625 - accuracy: 0.8990\n",
      "Epoch 41/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.2716 - accuracy: 0.8894\n",
      "Epoch 42/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.2572 - accuracy: 0.9038\n",
      "Epoch 43/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2678 - accuracy: 0.8846\n",
      "Epoch 44/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.2736 - accuracy: 0.8942\n",
      "Epoch 45/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2565 - accuracy: 0.8894\n",
      "Epoch 46/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.2363 - accuracy: 0.9327\n",
      "Epoch 47/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.2366 - accuracy: 0.9279\n",
      "Epoch 48/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.2417 - accuracy: 0.9183\n",
      "Epoch 49/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.2414 - accuracy: 0.9231\n",
      "Epoch 50/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.2376 - accuracy: 0.9279\n",
      "Epoch 51/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.2237 - accuracy: 0.9135\n",
      "Epoch 52/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.2195 - accuracy: 0.9375: 0s - loss: 0.2569 - accuracy: 0.91\n",
      "Epoch 53/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.2340 - accuracy: 0.9038\n",
      "Epoch 54/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.2180 - accuracy: 0.9327\n",
      "Epoch 55/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.2205 - accuracy: 0.9087\n",
      "Epoch 56/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.2139 - accuracy: 0.9231\n",
      "Epoch 57/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.2017 - accuracy: 0.9375\n",
      "Epoch 58/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.2030 - accuracy: 0.9231: 0s - loss: 0.2012 - accuracy: 0.91\n",
      "Epoch 59/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.2060 - accuracy: 0.9279\n",
      "Epoch 60/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.2093 - accuracy: 0.9231\n",
      "Epoch 61/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1930 - accuracy: 0.9231\n",
      "Epoch 62/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.2053 - accuracy: 0.9038\n",
      "Epoch 63/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1835 - accuracy: 0.9567\n",
      "Epoch 64/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1813 - accuracy: 0.9423\n",
      "Epoch 65/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1737 - accuracy: 0.9519\n",
      "Epoch 66/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1747 - accuracy: 0.9471\n",
      "Epoch 67/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1833 - accuracy: 0.9279\n",
      "Epoch 68/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1665 - accuracy: 0.9471\n",
      "Epoch 69/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1693 - accuracy: 0.9471\n",
      "Epoch 70/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1587 - accuracy: 0.9567\n",
      "Epoch 71/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1605 - accuracy: 0.9471\n",
      "Epoch 72/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1563 - accuracy: 0.9615\n",
      "Epoch 73/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1564 - accuracy: 0.9519\n",
      "Epoch 74/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1568 - accuracy: 0.9615\n",
      "Epoch 75/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1577 - accuracy: 0.9471\n",
      "Epoch 76/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1534 - accuracy: 0.9423\n",
      "Epoch 77/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1501 - accuracy: 0.9519\n",
      "Epoch 78/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1418 - accuracy: 0.9663\n",
      "Epoch 79/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1482 - accuracy: 0.9471\n",
      "Epoch 80/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1305 - accuracy: 0.9567\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1334 - accuracy: 0.9471\n",
      "Epoch 82/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1357 - accuracy: 0.9567\n",
      "Epoch 83/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1252 - accuracy: 0.9615\n",
      "Epoch 84/200\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1269 - accuracy: 0.9471\n",
      "Epoch 85/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1247 - accuracy: 0.9567\n",
      "Epoch 86/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1225 - accuracy: 0.9567\n",
      "Epoch 87/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1137 - accuracy: 0.9663\n",
      "Epoch 88/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1556 - accuracy: 0.9327\n",
      "Epoch 89/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1131 - accuracy: 0.9712\n",
      "Epoch 90/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1113 - accuracy: 0.9615\n",
      "Epoch 91/200\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0956 - accuracy: 0.97 - 0s 3ms/step - loss: 0.1100 - accuracy: 0.9663\n",
      "Epoch 92/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1097 - accuracy: 0.9519: 0s - loss: 0.1201 - accuracy: 0.94\n",
      "Epoch 93/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1060 - accuracy: 0.9663\n",
      "Epoch 94/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1030 - accuracy: 0.9760\n",
      "Epoch 95/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0988 - accuracy: 0.9808\n",
      "Epoch 96/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1002 - accuracy: 0.9663\n",
      "Epoch 97/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1106 - accuracy: 0.9615\n",
      "Epoch 98/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1007 - accuracy: 0.9663\n",
      "Epoch 99/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0988 - accuracy: 0.9663\n",
      "Epoch 100/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0928 - accuracy: 0.9808\n",
      "Epoch 101/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0901 - accuracy: 0.9808\n",
      "Epoch 102/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.0868 - accuracy: 0.9808\n",
      "Epoch 103/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0878 - accuracy: 0.9712\n",
      "Epoch 104/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.0996 - accuracy: 0.9712\n",
      "Epoch 105/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0831 - accuracy: 0.9856\n",
      "Epoch 106/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0924 - accuracy: 0.9712\n",
      "Epoch 107/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0797 - accuracy: 0.9760\n",
      "Epoch 108/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0795 - accuracy: 0.9808\n",
      "Epoch 109/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0759 - accuracy: 0.9760\n",
      "Epoch 110/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0791 - accuracy: 0.9856\n",
      "Epoch 111/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0845 - accuracy: 0.9615: 0s - loss: 0.0778 - accuracy: 0.96\n",
      "Epoch 112/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0692 - accuracy: 0.9808\n",
      "Epoch 113/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0720 - accuracy: 0.9808\n",
      "Epoch 114/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0700 - accuracy: 0.9856\n",
      "Epoch 115/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0647 - accuracy: 0.9856\n",
      "Epoch 116/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0643 - accuracy: 0.9808\n",
      "Epoch 117/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0769 - accuracy: 0.9712\n",
      "Epoch 118/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0721 - accuracy: 0.9760\n",
      "Epoch 119/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0682 - accuracy: 0.9760\n",
      "Epoch 120/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.9856\n",
      "Epoch 121/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0556 - accuracy: 0.9952\n",
      "Epoch 122/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9904\n",
      "Epoch 123/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0561 - accuracy: 0.9856\n",
      "Epoch 124/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0560 - accuracy: 0.9952\n",
      "Epoch 125/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0464 - accuracy: 0.9952\n",
      "Epoch 126/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0551 - accuracy: 0.9904\n",
      "Epoch 127/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0569 - accuracy: 0.9904\n",
      "Epoch 128/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0506 - accuracy: 0.9856\n",
      "Epoch 129/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0482 - accuracy: 0.9904: 0s - loss: 0.0808 - accuracy\n",
      "Epoch 130/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0439 - accuracy: 0.9904\n",
      "Epoch 131/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0447 - accuracy: 0.9952\n",
      "Epoch 132/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0413 - accuracy: 0.9952\n",
      "Epoch 133/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0436 - accuracy: 0.9904\n",
      "Epoch 134/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0429 - accuracy: 0.9952\n",
      "Epoch 135/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0375 - accuracy: 0.9952\n",
      "Epoch 136/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0357 - accuracy: 0.9952\n",
      "Epoch 137/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0355 - accuracy: 0.9952: 0s - loss: 0.0453 - accuracy: 0.\n",
      "Epoch 138/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0353 - accuracy: 0.9952\n",
      "Epoch 139/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0341 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0391 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.9952\n",
      "Epoch 142/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.0356 - accuracy: 0.9952\n",
      "Epoch 143/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0314 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0345 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0399 - accuracy: 0.9904\n",
      "Epoch 146/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0301 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 0.9952\n",
      "Epoch 149/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0270 - accuracy: 0.9952\n",
      "Epoch 150/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0259 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0237 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0234 - accuracy: 1.0000: 0s - loss: 0.0143 - accuracy: \n",
      "Epoch 154/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.9952\n",
      "Epoch 155/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0247 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0210 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0205 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0245 - accuracy: 1.0000\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0194 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.9952\n",
      "Epoch 163/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0295 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0324 - accuracy: 0.9952\n",
      "Epoch 165/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0166 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0453 - accuracy: 0.9904\n",
      "Epoch 178/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 0.9952\n",
      "Epoch 179/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b24f16c940>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=200, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "actual-stopping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.0051,  accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X,Y, verbose=0)\n",
    "print('loss: % .4f,  accuracy: %.4f' %(result[0], result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "likely-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "useful-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "arctic-monster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((145, 60), (63, 60), (145,), (63,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "scheduled-turkish",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim=60, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "wanted-salon",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "designing-groove",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "29/29 [==============================] - 2s 3ms/step - loss: 0.6792 - accuracy: 0.6276\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.6613 - accuracy: 0.6759\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.6437 - accuracy: 0.7379\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.6241 - accuracy: 0.7379\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.6027 - accuracy: 0.7655\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.5806 - accuracy: 0.7724\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.5633 - accuracy: 0.7517\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.5486 - accuracy: 0.7724\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.5358 - accuracy: 0.7586\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.5289 - accuracy: 0.7655\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4947 - accuracy: 0.8000\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4893 - accuracy: 0.8000\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4869 - accuracy: 0.7655\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4620 - accuracy: 0.8069\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4487 - accuracy: 0.8207\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4495 - accuracy: 0.8138\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4342 - accuracy: 0.7931\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.4327 - accuracy: 0.8000\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.4167 - accuracy: 0.8138\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.4120 - accuracy: 0.8138\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4039 - accuracy: 0.8207\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4026 - accuracy: 0.8276\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.3861 - accuracy: 0.8207\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.3793 - accuracy: 0.8207\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.3775 - accuracy: 0.8414\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.3646 - accuracy: 0.8483\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.3604 - accuracy: 0.8483\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.3705 - accuracy: 0.8414\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.3535 - accuracy: 0.8414\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.3438 - accuracy: 0.8483\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.3314 - accuracy: 0.8414\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.3204 - accuracy: 0.8621\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.3213 - accuracy: 0.8552\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.3272 - accuracy: 0.8552\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.3181 - accuracy: 0.8759\n",
      "Epoch 36/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.2974 - accuracy: 0.9034\n",
      "Epoch 37/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.2885 - accuracy: 0.8690\n",
      "Epoch 38/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.2838 - accuracy: 0.8966\n",
      "Epoch 39/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.2876 - accuracy: 0.8690\n",
      "Epoch 40/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.2784 - accuracy: 0.9103\n",
      "Epoch 41/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.2758 - accuracy: 0.8966\n",
      "Epoch 42/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.2614 - accuracy: 0.8828\n",
      "Epoch 43/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.2592 - accuracy: 0.8966\n",
      "Epoch 44/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.2679 - accuracy: 0.8966\n",
      "Epoch 45/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.2454 - accuracy: 0.9103\n",
      "Epoch 46/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.2309 - accuracy: 0.9172\n",
      "Epoch 47/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.2327 - accuracy: 0.9379\n",
      "Epoch 48/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.2305 - accuracy: 0.9172\n",
      "Epoch 49/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.2189 - accuracy: 0.9310\n",
      "Epoch 50/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.2123 - accuracy: 0.9379\n",
      "Epoch 51/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1996 - accuracy: 0.9586\n",
      "Epoch 52/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1983 - accuracy: 0.9586\n",
      "Epoch 53/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1940 - accuracy: 0.9586\n",
      "Epoch 54/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.1929 - accuracy: 0.9448\n",
      "Epoch 55/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.1867 - accuracy: 0.9448\n",
      "Epoch 56/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1782 - accuracy: 0.9655\n",
      "Epoch 57/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1711 - accuracy: 0.9448\n",
      "Epoch 58/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1665 - accuracy: 0.9724\n",
      "Epoch 59/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1654 - accuracy: 0.9724\n",
      "Epoch 60/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1615 - accuracy: 0.9724\n",
      "Epoch 61/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1532 - accuracy: 0.9655\n",
      "Epoch 62/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1501 - accuracy: 0.9862\n",
      "Epoch 63/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.1485 - accuracy: 0.9586\n",
      "Epoch 64/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1792 - accuracy: 0.9379\n",
      "Epoch 65/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.1201 - accuracy: 0.98 - 0s 3ms/step - loss: 0.1400 - accuracy: 0.9793\n",
      "Epoch 66/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1300 - accuracy: 0.9793\n",
      "Epoch 67/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1245 - accuracy: 0.9793\n",
      "Epoch 68/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1310 - accuracy: 0.9793\n",
      "Epoch 69/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1220 - accuracy: 0.9793\n",
      "Epoch 70/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1247 - accuracy: 0.9793\n",
      "Epoch 71/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1122 - accuracy: 0.9862\n",
      "Epoch 72/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1071 - accuracy: 0.9931\n",
      "Epoch 73/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1078 - accuracy: 0.9862\n",
      "Epoch 74/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0997 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1000 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0960 - accuracy: 0.9862\n",
      "Epoch 77/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0956 - accuracy: 0.9931\n",
      "Epoch 78/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0995 - accuracy: 0.9793\n",
      "Epoch 79/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.0956 - accuracy: 1.00 - 0s 5ms/step - loss: 0.0881 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0860 - accuracy: 0.9931\n",
      "Epoch 81/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0770 - accuracy: 1.0000\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0835 - accuracy: 0.9931\n",
      "Epoch 83/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0844 - accuracy: 0.9793\n",
      "Epoch 84/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0860 - accuracy: 0.9931\n",
      "Epoch 85/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0598 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0563 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0587 - accuracy: 0.9931\n",
      "Epoch 92/100\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0505 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 1.0000: 0s - loss: 0.0535 - accuracy: 1.00\n",
      "Epoch 95/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0475 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0466 - accuracy: 1.0000: 0s - loss: 0.0343 - accuracy: \n",
      "Epoch 97/100\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0429 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0410 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0407 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0420 - accuracy: 1.0000: 0s - loss: 0.0472 - accuracy: 1.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b250a8f1c0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=100, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "oriental-times",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.3787, accuracy: 0.8730\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('loss: %.4f, accuracy: %.4f' %(result[0], result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "lonely-fighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "norwegian-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "residential-applicant",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-1f8a688cae5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "experienced-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "creative-attribute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x1b2505225b0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "rental-chair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.3787, accuracy: 0.8730\n"
     ]
    }
   ],
   "source": [
    "result = new_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('loss: %.4f, accuracy: %.4f' %(result[0], result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "effective-advice",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim=60, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "herbal-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "continental-medication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:  10, loss: 0.5997, accuracy: 0.6667\n",
      "--------------------------------------------------\n",
      "epochs:  20, loss: 0.5076, accuracy: 0.7143\n",
      "--------------------------------------------------\n",
      "epochs:  30, loss: 0.4501, accuracy: 0.7460\n",
      "--------------------------------------------------\n",
      "epochs:  40, loss: 0.4081, accuracy: 0.7619\n",
      "--------------------------------------------------\n",
      "epochs:  50, loss: 0.3806, accuracy: 0.7778\n",
      "--------------------------------------------------\n",
      "epochs:  60, loss: 0.3573, accuracy: 0.8413\n",
      "--------------------------------------------------\n",
      "epochs:  70, loss: 0.3578, accuracy: 0.8889\n",
      "--------------------------------------------------\n",
      "epochs:  80, loss: 0.3606, accuracy: 0.8889\n",
      "--------------------------------------------------\n",
      "epochs:  90, loss: 0.3721, accuracy: 0.8889\n",
      "--------------------------------------------------\n",
      "epochs: 100, loss: 0.3828, accuracy: 0.8889\n",
      "--------------------------------------------------\n",
      "epochs: 110, loss: 0.3941, accuracy: 0.8889\n",
      "--------------------------------------------------\n",
      "epochs: 120, loss: 0.4173, accuracy: 0.8730\n",
      "--------------------------------------------------\n",
      "epochs: 130, loss: 0.4391, accuracy: 0.8730\n",
      "--------------------------------------------------\n",
      "epochs: 140, loss: 0.4686, accuracy: 0.8730\n",
      "--------------------------------------------------\n",
      "epochs: 150, loss: 0.4928, accuracy: 0.8730\n",
      "--------------------------------------------------\n",
      "epochs: 160, loss: 0.5264, accuracy: 0.8730\n",
      "--------------------------------------------------\n",
      "epochs: 170, loss: 0.5482, accuracy: 0.8730\n",
      "--------------------------------------------------\n",
      "epochs: 180, loss: 0.5770, accuracy: 0.8730\n",
      "--------------------------------------------------\n",
      "epochs: 190, loss: 0.6058, accuracy: 0.8730\n",
      "--------------------------------------------------\n",
      "epochs: 200, loss: 0.6312, accuracy: 0.8730\n",
      "--------------------------------------------------\n",
      "epochs: 210, loss: 0.6559, accuracy: 0.8730\n",
      "--------------------------------------------------\n",
      "epochs: 220, loss: 0.6841, accuracy: 0.8730\n",
      "--------------------------------------------------\n",
      "epochs: 230, loss: 0.7071, accuracy: 0.8730\n",
      "--------------------------------------------------\n",
      "epochs: 240, loss: 0.7278, accuracy: 0.8730\n",
      "--------------------------------------------------\n",
      "epochs: 250, loss: 0.7488, accuracy: 0.8730\n",
      "--------------------------------------------------\n",
      "epochs: 260, loss: 0.7726, accuracy: 0.8730\n",
      "--------------------------------------------------\n",
      "epochs: 270, loss: 0.8016, accuracy: 0.8730\n",
      "--------------------------------------------------\n",
      "epochs: 280, loss: 0.8266, accuracy: 0.8730\n",
      "--------------------------------------------------\n",
      "epochs: 290, loss: 0.8507, accuracy: 0.8730\n",
      "--------------------------------------------------\n",
      "epochs: 300, loss: 0.8719, accuracy: 0.8730\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,30):\n",
    "    model.fit(X_train, Y_train, epochs=10, batch_size=5, verbose=0)\n",
    "    result = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('epochs: %3d, loss: %.4f, accuracy: %.4f' %((i+1)*10, result[0], result[1]))\n",
    "    print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = []\n",
    "y1= []\n",
    "y2 = []\n",
    "for i in range(0,30):\n",
    "    model.fit(X_train, Y_train, epochs=10, batch_size=5, verbose=0)\n",
    "    result = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('epochs: %3d, loss: %.4f, accuracy: %.4f' %((i+1)*10, result[0], result[1]))\n",
    "    x1.append((i+1)*10)\n",
    "    y1.append(result[0])\n",
    "    y2.append(result[1])\n",
    "    print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "detailed-round",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b251fe9d90>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x1,y1)\n",
    "plt.plot(x1,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "parental-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 10\n",
    "skf = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "settled-pixel",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [  0   1   2   3   4   5   6   7   8   9  11  12  13  14  15  17  18  19\n",
      "  20  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
      "  39  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  58  59\n",
      "  60  61  63  64  65  66  67  71  72  73  74  75  76  77  78  79  80  81\n",
      "  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  98 100 101\n",
      " 102 103 104 105 107 108 111 112 113 114 116 117 118 119 120 121 122 123\n",
      " 124 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142\n",
      " 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160\n",
      " 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178\n",
      " 179 180 181 182 183 184 185 187 188 189 190 192 193 194 195 196 197 198\n",
      " 199 200 201 202 204 206 207]\n",
      "test: [ 10  16  21  40  41  50  62  68  69  70  97  99 106 109 110 115 125 186\n",
      " 191 203 205]\n",
      "---------------------------\n",
      "train: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  23  24  25  26  27  28  29  30  31  32  33  34  35  37\n",
      "  38  39  40  41  42  44  45  46  47  48  49  50  51  52  53  55  57  58\n",
      "  59  60  61  62  63  65  68  69  70  72  73  74  75  76  77  78  79  80\n",
      "  81  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98  99\n",
      " 100 101 102 103 104 105 106 107 108 109 110 112 113 114 115 116 117 118\n",
      " 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136\n",
      " 137 138 140 141 142 143 144 145 146 147 148 149 150 151 154 155 156 157\n",
      " 158 159 160 161 162 163 164 165 166 167 168 170 171 172 174 175 176 177\n",
      " 178 179 180 181 182 183 184 186 187 189 190 191 192 193 194 195 196 197\n",
      " 198 200 202 203 205 206 207]\n",
      "test: [ 22  36  43  54  56  64  66  67  71  90 111 139 152 153 169 173 185 188\n",
      " 199 201 204]\n",
      "---------------------------\n",
      "train: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  26  27  28  29  30  31  32  33  34  36  37  38\n",
      "  40  41  42  43  46  47  48  49  50  51  52  53  54  55  56  58  59  60\n",
      "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
      "  79  80  81  82  83  84  85  86  89  90  92  93  94  95  96  97  98  99\n",
      " 100 101 102 103 104 105 106 107 108 109 110 111 112 114 115 116 117 118\n",
      " 119 120 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 144 145 146 147 149 150 151 152 153 154 156 157 158\n",
      " 159 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178\n",
      " 179 180 181 182 183 184 185 186 187 188 190 191 192 193 195 196 197 199\n",
      " 200 201 203 204 205 206 207]\n",
      "test: [  8  25  35  39  44  45  57  87  88  91 113 121 143 148 155 160 161 189\n",
      " 194 198 202]\n",
      "---------------------------\n",
      "train: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37  38  39\n",
      "  40  41  42  43  44  45  46  47  49  50  51  53  54  55  56  57  58  59\n",
      "  60  61  62  63  64  65  66  67  68  69  70  71  72  73  76  77  78  79\n",
      "  80  81  82  83  84  85  87  88  89  90  91  92  93  95  96  97  98  99\n",
      " 100 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
      " 119 120 121 123 124 125 126 127 128 130 131 132 133 134 135 136 137 138\n",
      " 139 140 141 142 143 144 145 146 148 151 152 153 154 155 156 157 158 159\n",
      " 160 161 162 163 164 165 166 167 168 169 170 171 172 173 175 179 180 181\n",
      " 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199\n",
      " 200 201 202 203 204 205 207]\n",
      "test: [  6  19  20  28  48  52  74  75  86  94 103 122 129 147 149 150 174 176\n",
      " 177 178 206]\n",
      "---------------------------\n",
      "train: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  25  26  28  29  30  31  32  33  34  35  36  37  38\n",
      "  39  40  41  43  44  45  47  48  49  50  51  52  53  54  55  56  57  59\n",
      "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
      "  79  80  81  82  83  84  86  87  88  90  91  92  93  94  95  97  98  99\n",
      " 100 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 119\n",
      " 120 121 122 123 124 125 126 128 129 132 134 135 136 137 138 139 141 142\n",
      " 143 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
      " 182 183 184 185 186 187 188 189 191 192 193 194 195 196 197 198 199 200\n",
      " 201 202 203 204 205 206 207]\n",
      "test: [  0  24  27  42  46  58  60  85  89  96 101 118 127 130 131 133 140 144\n",
      " 164 181 190]\n",
      "---------------------------\n",
      "train: [  0   1   2   3   4   5   6   7   8  10  11  15  16  17  18  19  20  21\n",
      "  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37  38  39  40\n",
      "  41  42  43  44  45  46  47  48  49  50  51  52  54  55  56  57  58  59\n",
      "  60  61  62  64  65  66  67  68  69  70  71  72  73  74  75  76  78  79\n",
      "  80  81  83  84  85  86  87  88  89  90  91  92  94  95  96  97  98  99\n",
      " 100 101 102 103 104 105 106 107 108 109 110 111 112 113 115 116 118 119\n",
      " 120 121 122 123 125 126 127 128 129 130 131 132 133 134 135 137 138 139\n",
      " 140 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158\n",
      " 159 160 161 162 164 165 167 168 169 170 171 172 173 174 175 176 177 178\n",
      " 181 182 183 184 185 186 187 188 189 190 191 192 194 196 197 198 199 200\n",
      " 201 202 203 204 205 206 207]\n",
      "test: [  9  12  13  14  26  53  63  77  82  93 114 117 124 136 141 163 166 179\n",
      " 180 193 195]\n",
      "---------------------------\n",
      "train: [  0   1   2   3   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n",
      "  20  21  22  24  25  26  27  28  29  31  32  34  35  36  37  38  39  40\n",
      "  41  42  43  44  45  46  47  48  49  50  52  53  54  55  56  57  58  59\n",
      "  60  62  63  64  66  67  68  69  70  71  73  74  75  76  77  78  79  80\n",
      "  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  99 100\n",
      " 101 102 103 104 106 107 108 109 110 111 113 114 115 116 117 118 119 121\n",
      " 122 123 124 125 126 127 128 129 130 131 132 133 135 136 137 138 139 140\n",
      " 141 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159\n",
      " 160 161 162 163 164 165 166 167 168 169 170 171 173 174 175 176 177 178\n",
      " 179 180 181 183 184 185 186 188 189 190 191 192 193 194 195 197 198 199\n",
      " 201 202 203 204 205 206 207]\n",
      "test: [  4   5  23  30  33  51  61  65  72  81  98 105 112 120 134 142 172 182\n",
      " 187 196 200]\n",
      "---------------------------\n",
      "train: [  0   2   3   4   5   6   7   8   9  10  11  12  13  14  16  17  19  20\n",
      "  21  22  23  24  25  26  27  28  29  30  32  33  35  36  38  39  40  41\n",
      "  42  43  44  45  46  48  49  50  51  52  53  54  55  56  57  58  59  60\n",
      "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
      "  80  81  82  83  84  85  86  87  88  89  90  91  93  94  95  96  97  98\n",
      "  99 101 103 104 105 106 107 109 110 111 112 113 114 115 116 117 118 120\n",
      " 121 122 123 124 125 126 127 128 129 130 131 132 133 134 136 137 138 139\n",
      " 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 159\n",
      " 160 161 162 163 164 166 167 169 170 171 172 173 174 176 177 178 179 180\n",
      " 181 182 184 185 186 187 188 189 190 191 193 194 195 196 197 198 199 200\n",
      " 201 202 203 204 205 206 207]\n",
      "test: [  1  15  18  31  34  37  47  79  92 100 102 108 119 135 157 158 165 168\n",
      " 175 183 192]\n",
      "---------------------------\n",
      "train: [  0   1   2   3   4   5   6   8   9  10  12  13  14  15  16  18  19  20\n",
      "  21  22  23  24  25  26  27  28  29  30  31  33  34  35  36  37  38  39\n",
      "  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
      "  58  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76\n",
      "  77  78  79  81  82  85  86  87  88  89  90  91  92  93  94  96  97  98\n",
      "  99 100 101 102 103 104 105 106 108 109 110 111 112 113 114 115 116 117\n",
      " 118 119 120 121 122 123 124 125 126 127 128 129 130 131 133 134 135 136\n",
      " 137 139 140 141 142 143 144 145 147 148 149 150 151 152 153 154 155 157\n",
      " 158 159 160 161 163 164 165 166 168 169 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 185 186 187 188 189 190 191 192 193 194 195 196 198 199\n",
      " 200 201 202 203 204 205 206 207]\n",
      "test: [  7  11  17  32  59  80  83  84  95 107 132 138 146 156 162 167 170 171\n",
      " 184 197]\n",
      "---------------------------\n",
      "train: [  0   1   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n",
      "  20  21  22  23  24  25  26  27  28  30  31  32  33  34  35  36  37  39\n",
      "  40  41  42  43  44  45  46  47  48  50  51  52  53  54  56  57  58  59\n",
      "  60  61  62  63  64  65  66  67  68  69  70  71  72  74  75  77  79  80\n",
      "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
      "  99 100 101 102 103 105 106 107 108 109 110 111 112 113 114 115 117 118\n",
      " 119 120 121 122 124 125 127 129 130 131 132 133 134 135 136 138 139 140\n",
      " 141 142 143 144 146 147 148 149 150 152 153 155 156 157 158 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
      " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      " 199 200 201 202 203 204 205 206]\n",
      "test: [  2   3  29  38  49  55  73  76  78 104 116 123 126 128 137 145 151 154\n",
      " 159 207]\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "for train, test in skf.split(X,Y): # train, test 는 index list임 데이터가 아님.\n",
    "    print('train:', train)\n",
    "    print('test:', test)\n",
    "    print('---------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "reverse-conviction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 fold, test accuracy list:  [0.8571428656578064, 0.9047619104385376, 0.9047619104385376, 0.6666666865348816, 0.8571428656578064, 0.761904776096344, 0.9523809552192688, 0.9523809552192688, 0.800000011920929, 0.800000011920929]\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "for train, test in skf.split(X,Y):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=60, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X[train], Y[train], epochs=100, batch_size=5, verbose=0)\n",
    "    \n",
    "    accuracy.append(model.evaluate(X[test], Y[test], verbose=0)[1])\n",
    "    \n",
    "print('%d fold, test accuracy list: ' %(n_fold), accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "executed-madison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 fold, test accuracy list:  [0.8571428656578064, 0.9047619104385376, 0.9047619104385376, 0.6666666865348816, 0.8571428656578064, 0.761904776096344, 0.9523809552192688, 0.9523809552192688, 0.800000011920929, 0.800000011920929]\n"
     ]
    }
   ],
   "source": [
    "print('%d fold, test accuracy list: ' %(n_fold), accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "vanilla-provider",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.8457142949104309\n"
     ]
    }
   ],
   "source": [
    "print('test accuracy:', np.array(accuracy).mean()) # 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "yellow-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.array([[1,2,3,4],[5,6,7,8], [9,10,11,12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "interim-soviet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "neural-major",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  6,  7,  8],\n",
       "       [ 9, 10, 11, 12]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa[[1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "conservative-pilot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 fold, test accuracy list:  [0.7142857313156128, 0.8571428656578064, 0.9047619104385376, 0.8095238208770752, 0.8571428656578064, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim=60, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
    "\n",
    "for train, test in skf.split(X,Y):\n",
    "    model.fit(X[train], Y[train], epochs=20, batch_size=5, verbose=0)\n",
    "    accuracy.append(model.evaluate(X[test], Y[test], verbose=0)[1])\n",
    "    \n",
    "print('%d fold, test accuracy list: ' %(n_fold), accuracy) \n",
    "# k fold 교차 검증을 할때 과적합이 많이 나타난다. epochs도 크게 잡지 말아야 한다. n-fold도 크게 잡지 말아야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "advised-company",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.9142857193946838\n"
     ]
    }
   ],
   "source": [
    "print('test accuracy:', np.array(accuracy).mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-windows",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
