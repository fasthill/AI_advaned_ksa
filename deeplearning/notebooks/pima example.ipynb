{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e6ac5891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a933b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_data_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
    "dataset = np.loadtxt(pima_data_url, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "318fb0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f734f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:, :8]\n",
    "Y = dataset[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8890045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "MODEL_DIR = './pima_model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "else:\n",
    "    shutil.rmtree(MODEL_DIR)  # remove directory with all its contents\n",
    "    os.mkdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "087ddeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = MODEL_DIR + \"{epoch:02d}-{loss:.4f}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e71425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./pima_model/{epoch:02d}-{loss:.4f}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "58b686f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=model_path, monitor='loss', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f4dffdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss', patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8cc94644",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "804d9971",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48ba3b07",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.4978 - accuracy: 0.7630\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5052 - accuracy: 0.7448\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.5150 - accuracy: 0.7513\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.5073 - accuracy: 0.7409\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.5015 - accuracy: 0.7526\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4983 - accuracy: 0.7643\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.5052 - accuracy: 0.7591\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4967 - accuracy: 0.7630\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4970 - accuracy: 0.7617\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5027 - accuracy: 0.7591\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4991 - accuracy: 0.7604\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.5010 - accuracy: 0.7448\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4959 - accuracy: 0.7578\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.5052 - accuracy: 0.7435\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4989 - accuracy: 0.7539\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 0s 732us/step - loss: 0.4976 - accuracy: 0.7500\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5021 - accuracy: 0.7474\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.5167 - accuracy: 0.7305\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5117 - accuracy: 0.7461\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5013 - accuracy: 0.7448\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4940 - accuracy: 0.7578\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4918 - accuracy: 0.7643\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 0s 732us/step - loss: 0.4927 - accuracy: 0.7604\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4910 - accuracy: 0.7604\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4942 - accuracy: 0.7643\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4978 - accuracy: 0.7630\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4935 - accuracy: 0.7682\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5061 - accuracy: 0.7500\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4872 - accuracy: 0.7617\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4970 - accuracy: 0.7539\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4904 - accuracy: 0.7578\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4905 - accuracy: 0.7552\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4886 - accuracy: 0.7656\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4954 - accuracy: 0.7578\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4893 - accuracy: 0.7682\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4901 - accuracy: 0.7604\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4859 - accuracy: 0.7630\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4843 - accuracy: 0.7643\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4927 - accuracy: 0.7604\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4987 - accuracy: 0.7513\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4941 - accuracy: 0.7721\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5077 - accuracy: 0.7539\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4903 - accuracy: 0.7591\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4854 - accuracy: 0.7721\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4851 - accuracy: 0.7617\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4822 - accuracy: 0.7630\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4836 - accuracy: 0.7643\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4837 - accuracy: 0.7669\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4836 - accuracy: 0.7695\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4797 - accuracy: 0.7630\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4838 - accuracy: 0.7617\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4853 - accuracy: 0.7604\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4857 - accuracy: 0.7591\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4799 - accuracy: 0.7591\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.4896 - accuracy: 0.7695\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4985 - accuracy: 0.7565\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.5033 - accuracy: 0.7500\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4869 - accuracy: 0.7682\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4789 - accuracy: 0.7643\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4788 - accuracy: 0.7734\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4806 - accuracy: 0.7643\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4879 - accuracy: 0.7591\n",
      "Epoch 63/500\n",
      "16/16 [==============================] - 0s 599us/step - loss: 0.4770 - accuracy: 0.7656\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 0s 732us/step - loss: 0.4751 - accuracy: 0.7669\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4889 - accuracy: 0.7578\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4826 - accuracy: 0.7643\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 0s 667us/step - loss: 0.4789 - accuracy: 0.7591\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 0s 732us/step - loss: 0.4704 - accuracy: 0.7760\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 0s 796us/step - loss: 0.4751 - accuracy: 0.7708\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4850 - accuracy: 0.7578\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4825 - accuracy: 0.7552\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4738 - accuracy: 0.7695\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4756 - accuracy: 0.7591\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4841 - accuracy: 0.7474\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4881 - accuracy: 0.7747\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4734 - accuracy: 0.7669\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4705 - accuracy: 0.7708\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4773 - accuracy: 0.7682\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 0s 667us/step - loss: 0.4696 - accuracy: 0.7760\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 0s 733us/step - loss: 0.4765 - accuracy: 0.7630\n",
      "Epoch 81/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 798us/step - loss: 0.4807 - accuracy: 0.7695\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - 0s 797us/step - loss: 0.4734 - accuracy: 0.7773\n",
      "Epoch 83/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4841 - accuracy: 0.7604\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4658 - accuracy: 0.7656\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4738 - accuracy: 0.7565\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4698 - accuracy: 0.7799\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4678 - accuracy: 0.7773\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4862 - accuracy: 0.7591\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 0s 796us/step - loss: 0.4675 - accuracy: 0.7682\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4672 - accuracy: 0.7695\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4654 - accuracy: 0.7826\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4634 - accuracy: 0.7734\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - 0s 733us/step - loss: 0.4783 - accuracy: 0.7656\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4794 - accuracy: 0.7513\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4824 - accuracy: 0.7604\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4790 - accuracy: 0.7591\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 0s 796us/step - loss: 0.4729 - accuracy: 0.7695\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4705 - accuracy: 0.7669\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4654 - accuracy: 0.7747\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4651 - accuracy: 0.7786\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4652 - accuracy: 0.7786\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 0s 796us/step - loss: 0.4738 - accuracy: 0.7708\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 0s 733us/step - loss: 0.4665 - accuracy: 0.7786\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 0s 732us/step - loss: 0.4669 - accuracy: 0.7708\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - 0s 733us/step - loss: 0.4637 - accuracy: 0.7760\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 0s 796us/step - loss: 0.4634 - accuracy: 0.7682\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4324 - accuracy: 0.82 - 0s 795us/step - loss: 0.4636 - accuracy: 0.7656\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 0s 733us/step - loss: 0.4624 - accuracy: 0.7721\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 0s 732us/step - loss: 0.4628 - accuracy: 0.7865\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 0s 733us/step - loss: 0.4617 - accuracy: 0.7760\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4600 - accuracy: 0.7773\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 0s 799us/step - loss: 0.4602 - accuracy: 0.7708\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 0s 797us/step - loss: 0.4620 - accuracy: 0.7669\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4608 - accuracy: 0.7760\n",
      "Epoch 115/500\n",
      "16/16 [==============================] - 0s 732us/step - loss: 0.4757 - accuracy: 0.7786\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4595 - accuracy: 0.7721\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4668 - accuracy: 0.7773\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4650 - accuracy: 0.7747\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4677 - accuracy: 0.7747\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4669 - accuracy: 0.7852\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4684 - accuracy: 0.7656\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4594 - accuracy: 0.7747\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4600 - accuracy: 0.7812\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4613 - accuracy: 0.7721\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4676 - accuracy: 0.7773\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4563 - accuracy: 0.7799\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4573 - accuracy: 0.7682\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4795 - accuracy: 0.7474\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - 0s 799us/step - loss: 0.4716 - accuracy: 0.7578\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4720 - accuracy: 0.7865\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4598 - accuracy: 0.7839\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4609 - accuracy: 0.7747\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4697 - accuracy: 0.7773\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4686 - accuracy: 0.7708\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4581 - accuracy: 0.7812\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 0s 732us/step - loss: 0.4587 - accuracy: 0.7786\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4584 - accuracy: 0.7786\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4583 - accuracy: 0.7826\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4594 - accuracy: 0.7760\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - 0s 732us/step - loss: 0.4704 - accuracy: 0.7643\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4666 - accuracy: 0.7891\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4718 - accuracy: 0.7721\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4621 - accuracy: 0.7656\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4582 - accuracy: 0.7773\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4616 - accuracy: 0.7786\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4635 - accuracy: 0.7760\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4612 - accuracy: 0.7721\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4591 - accuracy: 0.7695\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4769 - accuracy: 0.7656\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4535 - accuracy: 0.7812\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4537 - accuracy: 0.7799\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4634 - accuracy: 0.7721\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4678 - accuracy: 0.7721\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4575 - accuracy: 0.7773\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4542 - accuracy: 0.7760\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4534 - accuracy: 0.7747\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4526 - accuracy: 0.7760\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4681 - accuracy: 0.7721\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4626 - accuracy: 0.7682\n",
      "Epoch 160/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 665us/step - loss: 0.4572 - accuracy: 0.7865\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4514 - accuracy: 0.7826\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4524 - accuracy: 0.7747\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4544 - accuracy: 0.7747\n",
      "Epoch 164/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4513 - accuracy: 0.7786\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4545 - accuracy: 0.7747\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 0s 666us/step - loss: 0.4555 - accuracy: 0.7943\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4682 - accuracy: 0.7721\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4735 - accuracy: 0.7747\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4884 - accuracy: 0.7591\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4765 - accuracy: 0.7630\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4667 - accuracy: 0.7734\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4581 - accuracy: 0.7826\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4584 - accuracy: 0.7747\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4637 - accuracy: 0.7734\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4537 - accuracy: 0.7734\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4569 - accuracy: 0.7708\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4606 - accuracy: 0.7773\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 0s 666us/step - loss: 0.4492 - accuracy: 0.7708\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4845 - accuracy: 0.7526\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4644 - accuracy: 0.7891\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4603 - accuracy: 0.7786\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4524 - accuracy: 0.7721\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 0s 865us/step - loss: 0.4567 - accuracy: 0.7734\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4450 - accuracy: 0.7839\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.4534 - accuracy: 0.7708\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4495 - accuracy: 0.7773\n",
      "Epoch 187/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4530 - accuracy: 0.7878\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4496 - accuracy: 0.7721\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4517 - accuracy: 0.7734\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4486 - accuracy: 0.7799\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4534 - accuracy: 0.7786\n",
      "Epoch 192/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4469 - accuracy: 0.7799\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4515 - accuracy: 0.7826\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4522 - accuracy: 0.7734\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4484 - accuracy: 0.7656\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4462 - accuracy: 0.7852\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4550 - accuracy: 0.7839\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4698 - accuracy: 0.7721\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4450 - accuracy: 0.7839\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4482 - accuracy: 0.7799\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4547 - accuracy: 0.7878\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4545 - accuracy: 0.7734\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 0s 797us/step - loss: 0.4452 - accuracy: 0.7812\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4648 - accuracy: 0.7760\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4548 - accuracy: 0.7786\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4483 - accuracy: 0.7786\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4515 - accuracy: 0.7799\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4612 - accuracy: 0.7721\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4544 - accuracy: 0.7799\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4499 - accuracy: 0.7734\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4496 - accuracy: 0.7747\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4445 - accuracy: 0.7878\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4495 - accuracy: 0.7630\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4555 - accuracy: 0.7734\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4500 - accuracy: 0.7734\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4474 - accuracy: 0.7812\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4467 - accuracy: 0.7773\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4474 - accuracy: 0.7839\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4490 - accuracy: 0.7839\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4556 - accuracy: 0.7734\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4448 - accuracy: 0.7760\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4455 - accuracy: 0.7839\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4480 - accuracy: 0.7826\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4652 - accuracy: 0.7682\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4514 - accuracy: 0.7604\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4435 - accuracy: 0.7904\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4546 - accuracy: 0.7891\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4507 - accuracy: 0.7839\n",
      "Epoch 229/500\n",
      "16/16 [==============================] - 0s 666us/step - loss: 0.4532 - accuracy: 0.7552\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4499 - accuracy: 0.7734\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4410 - accuracy: 0.7878\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4416 - accuracy: 0.7917\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4424 - accuracy: 0.7734\n",
      "Epoch 234/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4438 - accuracy: 0.7656\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4461 - accuracy: 0.7799\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4423 - accuracy: 0.7865\n",
      "Epoch 237/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4456 - accuracy: 0.7786\n",
      "Epoch 238/500\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4456 - accuracy: 0.7773\n",
      "Epoch 239/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 798us/step - loss: 0.4557 - accuracy: 0.7826\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4606 - accuracy: 0.7773\n",
      "Epoch 241/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4555 - accuracy: 0.7799\n",
      "Epoch 242/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4604 - accuracy: 0.7643\n",
      "Epoch 243/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4397 - accuracy: 0.7773\n",
      "Epoch 244/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4446 - accuracy: 0.7839\n",
      "Epoch 245/500\n",
      "16/16 [==============================] - 0s 799us/step - loss: 0.4506 - accuracy: 0.7839\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4630 - accuracy: 0.7656\n",
      "Epoch 247/500\n",
      "16/16 [==============================] - 0s 666us/step - loss: 0.4497 - accuracy: 0.7799\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4440 - accuracy: 0.7656\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4375 - accuracy: 0.7826\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4373 - accuracy: 0.7747\n",
      "Epoch 251/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4402 - accuracy: 0.7865\n",
      "Epoch 252/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4401 - accuracy: 0.7812\n",
      "Epoch 253/500\n",
      "16/16 [==============================] - 0s 664us/step - loss: 0.4374 - accuracy: 0.7839\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4628 - accuracy: 0.7695\n",
      "Epoch 255/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4411 - accuracy: 0.7812\n",
      "Epoch 256/500\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4350 - accuracy: 0.7812\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4499 - accuracy: 0.7682\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4388 - accuracy: 0.7773\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4350 - accuracy: 0.7878\n",
      "Epoch 260/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4398 - accuracy: 0.7773\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4427 - accuracy: 0.7904\n",
      "Epoch 262/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4370 - accuracy: 0.7826\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4367 - accuracy: 0.7878\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4365 - accuracy: 0.7786\n",
      "Epoch 265/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4407 - accuracy: 0.7760\n",
      "Epoch 266/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4391 - accuracy: 0.7812\n",
      "Epoch 267/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4370 - accuracy: 0.7852\n",
      "Epoch 268/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4405 - accuracy: 0.7786\n",
      "Epoch 269/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4448 - accuracy: 0.7773\n",
      "Epoch 270/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4453 - accuracy: 0.7760\n",
      "Epoch 271/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4355 - accuracy: 0.7969\n",
      "Epoch 272/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4480 - accuracy: 0.7917\n",
      "Epoch 273/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4378 - accuracy: 0.7852\n",
      "Epoch 274/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4438 - accuracy: 0.7708\n",
      "Epoch 275/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4436 - accuracy: 0.7878\n",
      "Epoch 276/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4388 - accuracy: 0.7917\n",
      "Epoch 277/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4419 - accuracy: 0.7760\n",
      "Epoch 278/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4597 - accuracy: 0.7591\n",
      "Epoch 279/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4453 - accuracy: 0.7799\n",
      "Epoch 280/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4570 - accuracy: 0.7747\n",
      "Epoch 281/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4485 - accuracy: 0.7643\n",
      "Epoch 282/500\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4519 - accuracy: 0.7839\n",
      "Epoch 283/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4450 - accuracy: 0.7721\n",
      "Epoch 284/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4404 - accuracy: 0.7734\n",
      "Epoch 285/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4369 - accuracy: 0.7839\n",
      "Epoch 286/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4404 - accuracy: 0.7695\n",
      "Epoch 287/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4373 - accuracy: 0.7734\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4461 - accuracy: 0.7839\n",
      "Epoch 289/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4394 - accuracy: 0.7760\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4360 - accuracy: 0.7930\n",
      "Epoch 291/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4405 - accuracy: 0.7839\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4404 - accuracy: 0.7760\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4460 - accuracy: 0.7891\n",
      "Epoch 294/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4514 - accuracy: 0.7734\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4512 - accuracy: 0.7721\n",
      "Epoch 296/500\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4481 - accuracy: 0.7943\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - 0s 797us/step - loss: 0.4459 - accuracy: 0.7799\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4374 - accuracy: 0.7786\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4326 - accuracy: 0.7786\n",
      "Epoch 300/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4368 - accuracy: 0.7865\n",
      "Epoch 301/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3437 - accuracy: 0.88 - 0s 731us/step - loss: 0.4411 - accuracy: 0.7917\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4438 - accuracy: 0.7799\n",
      "Epoch 303/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4424 - accuracy: 0.7682\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4396 - accuracy: 0.7786\n",
      "Epoch 305/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4392 - accuracy: 0.7773\n",
      "Epoch 306/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4491 - accuracy: 0.7943\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4385 - accuracy: 0.7891\n",
      "Epoch 308/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4531 - accuracy: 0.7786\n",
      "Epoch 309/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4385 - accuracy: 0.7799\n",
      "Epoch 310/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4368 - accuracy: 0.7891\n",
      "Epoch 311/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4494 - accuracy: 0.7695\n",
      "Epoch 312/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4435 - accuracy: 0.7734\n",
      "Epoch 313/500\n",
      "16/16 [==============================] - 0s 734us/step - loss: 0.4450 - accuracy: 0.7643\n",
      "Epoch 314/500\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4515 - accuracy: 0.7721\n",
      "Epoch 315/500\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4383 - accuracy: 0.7708\n",
      "Epoch 316/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4388 - accuracy: 0.7852\n",
      "Epoch 317/500\n",
      "16/16 [==============================] - 0s 733us/step - loss: 0.4323 - accuracy: 0.7852\n",
      "Epoch 318/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 666us/step - loss: 0.4335 - accuracy: 0.7839\n",
      "Epoch 319/500\n",
      "16/16 [==============================] - 0s 797us/step - loss: 0.4289 - accuracy: 0.7865\n",
      "Epoch 320/500\n",
      "16/16 [==============================] - 0s 799us/step - loss: 0.4434 - accuracy: 0.7695\n",
      "Epoch 321/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4353 - accuracy: 0.7878\n",
      "Epoch 322/500\n",
      "16/16 [==============================] - 0s 733us/step - loss: 0.4314 - accuracy: 0.7760\n",
      "Epoch 323/500\n",
      "16/16 [==============================] - 0s 796us/step - loss: 0.4386 - accuracy: 0.7839\n",
      "Epoch 324/500\n",
      "16/16 [==============================] - 0s 932us/step - loss: 0.4302 - accuracy: 0.7878\n",
      "Epoch 325/500\n",
      "16/16 [==============================] - 0s 932us/step - loss: 0.4396 - accuracy: 0.7826\n",
      "Epoch 326/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4318 - accuracy: 0.7904\n",
      "Epoch 327/500\n",
      "16/16 [==============================] - 0s 799us/step - loss: 0.4351 - accuracy: 0.7839\n",
      "Epoch 328/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4341 - accuracy: 0.7865\n",
      "Epoch 329/500\n",
      "16/16 [==============================] - 0s 797us/step - loss: 0.4354 - accuracy: 0.7708\n",
      "Epoch 330/500\n",
      "16/16 [==============================] - 0s 732us/step - loss: 0.4324 - accuracy: 0.7812\n",
      "Epoch 331/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4322 - accuracy: 0.7878\n",
      "Epoch 332/500\n",
      "16/16 [==============================] - 0s 799us/step - loss: 0.4293 - accuracy: 0.7799\n",
      "Epoch 333/500\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4546 - accuracy: 0.7708\n",
      "Epoch 334/500\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4388 - accuracy: 0.7852\n",
      "Epoch 335/500\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4274 - accuracy: 0.7852\n",
      "Epoch 336/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4315 - accuracy: 0.7917\n",
      "Epoch 337/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4331 - accuracy: 0.7747\n",
      "Epoch 338/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4344 - accuracy: 0.7799\n",
      "Epoch 339/500\n",
      "16/16 [==============================] - 0s 733us/step - loss: 0.4456 - accuracy: 0.7734\n",
      "Epoch 340/500\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4710 - accuracy: 0.7773\n",
      "Epoch 341/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4368 - accuracy: 0.7878\n",
      "Epoch 342/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4311 - accuracy: 0.7956\n",
      "Epoch 343/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4337 - accuracy: 0.7865\n",
      "Epoch 344/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4352 - accuracy: 0.7826\n",
      "Epoch 345/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4305 - accuracy: 0.7904\n",
      "Epoch 346/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4344 - accuracy: 0.7891\n",
      "Epoch 347/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4316 - accuracy: 0.7799\n",
      "Epoch 348/500\n",
      "16/16 [==============================] - 0s 664us/step - loss: 0.4243 - accuracy: 0.7917\n",
      "Epoch 349/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4373 - accuracy: 0.7826\n",
      "Epoch 350/500\n",
      "16/16 [==============================] - 0s 865us/step - loss: 0.4358 - accuracy: 0.7773\n",
      "Epoch 351/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4349 - accuracy: 0.7969\n",
      "Epoch 352/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4278 - accuracy: 0.7865\n",
      "Epoch 353/500\n",
      "16/16 [==============================] - 0s 797us/step - loss: 0.4265 - accuracy: 0.7865\n",
      "Epoch 354/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4276 - accuracy: 0.7930\n",
      "Epoch 355/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4270 - accuracy: 0.7852\n",
      "Epoch 356/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4289 - accuracy: 0.7839\n",
      "Epoch 357/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4316 - accuracy: 0.7891\n",
      "Epoch 358/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4254 - accuracy: 0.8073\n",
      "Epoch 359/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4445 - accuracy: 0.7826\n",
      "Epoch 360/500\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4355 - accuracy: 0.7799\n",
      "Epoch 361/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4317 - accuracy: 0.7956\n",
      "Epoch 362/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4278 - accuracy: 0.7943\n",
      "Epoch 363/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4429 - accuracy: 0.7799\n",
      "Epoch 364/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4424 - accuracy: 0.7799\n",
      "Epoch 365/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4324 - accuracy: 0.7891\n",
      "Epoch 366/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4397 - accuracy: 0.7839\n",
      "Epoch 367/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4280 - accuracy: 0.7943\n",
      "Epoch 368/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4347 - accuracy: 0.7786\n",
      "Epoch 369/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4319 - accuracy: 0.7982\n",
      "Epoch 370/500\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4331 - accuracy: 0.7865\n",
      "Epoch 371/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4371 - accuracy: 0.7734\n",
      "Epoch 372/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4351 - accuracy: 0.7852\n",
      "Epoch 373/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4417 - accuracy: 0.7839\n",
      "Epoch 374/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4311 - accuracy: 0.7865\n",
      "Epoch 375/500\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4268 - accuracy: 0.7891\n",
      "Epoch 376/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4639 - accuracy: 0.7669\n",
      "Epoch 377/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4310 - accuracy: 0.7852\n",
      "Epoch 378/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4321 - accuracy: 0.7930\n",
      "Epoch 379/500\n",
      "16/16 [==============================] - 0s 797us/step - loss: 0.4306 - accuracy: 0.7773\n",
      "Epoch 380/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4270 - accuracy: 0.7826\n",
      "Epoch 381/500\n",
      "16/16 [==============================] - 0s 664us/step - loss: 0.4270 - accuracy: 0.7956\n",
      "Epoch 382/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4236 - accuracy: 0.7891\n",
      "Epoch 383/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4258 - accuracy: 0.7969\n",
      "Epoch 384/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4255 - accuracy: 0.7865\n",
      "Epoch 385/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4287 - accuracy: 0.7878\n",
      "Epoch 386/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4273 - accuracy: 0.7852\n",
      "Epoch 387/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4249 - accuracy: 0.7891\n",
      "Epoch 388/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4301 - accuracy: 0.7878\n",
      "Epoch 389/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4237 - accuracy: 0.7956\n",
      "Epoch 390/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4224 - accuracy: 0.7865\n",
      "Epoch 391/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4296 - accuracy: 0.7839\n",
      "Epoch 392/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4281 - accuracy: 0.7865\n",
      "Epoch 393/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4288 - accuracy: 0.7904\n",
      "Epoch 394/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4247 - accuracy: 0.7930\n",
      "Epoch 395/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4304 - accuracy: 0.7878\n",
      "Epoch 396/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4229 - accuracy: 0.7982\n",
      "Epoch 397/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 731us/step - loss: 0.4299 - accuracy: 0.7865\n",
      "Epoch 398/500\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4238 - accuracy: 0.7839\n",
      "Epoch 399/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4237 - accuracy: 0.7982\n",
      "Epoch 400/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4383 - accuracy: 0.7865\n",
      "Epoch 401/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4432 - accuracy: 0.7969\n",
      "Epoch 402/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4357 - accuracy: 0.7643\n",
      "Epoch 403/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4263 - accuracy: 0.7917\n",
      "Epoch 404/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4288 - accuracy: 0.7852\n",
      "Epoch 405/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4223 - accuracy: 0.7812\n",
      "Epoch 406/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4224 - accuracy: 0.7943\n",
      "Epoch 407/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4235 - accuracy: 0.7969\n",
      "Epoch 408/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4263 - accuracy: 0.7852\n",
      "Epoch 409/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4256 - accuracy: 0.7839\n",
      "Epoch 410/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4339 - accuracy: 0.7826\n",
      "Epoch 411/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4371 - accuracy: 0.7826\n",
      "Epoch 412/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4266 - accuracy: 0.7930\n",
      "Epoch 413/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4286 - accuracy: 0.7904\n",
      "Epoch 414/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4313 - accuracy: 0.7917\n",
      "Epoch 415/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4222 - accuracy: 0.7839\n",
      "Epoch 416/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4270 - accuracy: 0.7904\n",
      "Epoch 417/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4212 - accuracy: 0.7891\n",
      "Epoch 418/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4316 - accuracy: 0.7969\n",
      "Epoch 419/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4221 - accuracy: 0.7904\n",
      "Epoch 420/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4274 - accuracy: 0.7865\n",
      "Epoch 421/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4226 - accuracy: 0.7969\n",
      "Epoch 422/500\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4327 - accuracy: 0.7812\n",
      "Epoch 423/500\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4289 - accuracy: 0.7799\n",
      "Epoch 424/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4326 - accuracy: 0.7826\n",
      "Epoch 425/500\n",
      "16/16 [==============================] - 0s 732us/step - loss: 0.4248 - accuracy: 0.7839\n",
      "Epoch 426/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4307 - accuracy: 0.7943\n",
      "Epoch 427/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4261 - accuracy: 0.7891\n",
      "Epoch 428/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4219 - accuracy: 0.7917\n",
      "Epoch 429/500\n",
      "16/16 [==============================] - 0s 865us/step - loss: 0.4277 - accuracy: 0.7865\n",
      "Epoch 430/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4236 - accuracy: 0.7956\n",
      "Epoch 431/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4825 - accuracy: 0.78 - 0s 798us/step - loss: 0.4222 - accuracy: 0.7917\n",
      "Epoch 432/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4298 - accuracy: 0.7969\n",
      "Epoch 433/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4364 - accuracy: 0.8008\n",
      "Epoch 434/500\n",
      "16/16 [==============================] - 0s 663us/step - loss: 0.4190 - accuracy: 0.7904\n",
      "Epoch 435/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4230 - accuracy: 0.7917\n",
      "Epoch 436/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4198 - accuracy: 0.7904\n",
      "Epoch 437/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4278 - accuracy: 0.7969\n",
      "Epoch 438/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4209 - accuracy: 0.7917\n",
      "Epoch 439/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4227 - accuracy: 0.7969\n",
      "Epoch 440/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4264 - accuracy: 0.7878\n",
      "Epoch 441/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4228 - accuracy: 0.7943\n",
      "Epoch 442/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4275 - accuracy: 0.7826\n",
      "Epoch 443/500\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4253 - accuracy: 0.7878\n",
      "Epoch 444/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4396 - accuracy: 0.7852\n",
      "Epoch 445/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4364 - accuracy: 0.8034\n",
      "Epoch 446/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4257 - accuracy: 0.7852\n",
      "Epoch 447/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4257 - accuracy: 0.7917\n",
      "Epoch 448/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4249 - accuracy: 0.7943\n",
      "Epoch 449/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4278 - accuracy: 0.7956\n",
      "Epoch 450/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4259 - accuracy: 0.7930\n",
      "Epoch 451/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4216 - accuracy: 0.7865\n",
      "Epoch 452/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4212 - accuracy: 0.7917\n",
      "Epoch 453/500\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4247 - accuracy: 0.7956\n",
      "Epoch 454/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4234 - accuracy: 0.7956\n",
      "Epoch 455/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4221 - accuracy: 0.7943\n",
      "Epoch 456/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4245 - accuracy: 0.7956\n",
      "Epoch 457/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4235 - accuracy: 0.7852\n",
      "Epoch 458/500\n",
      "16/16 [==============================] - 0s 732us/step - loss: 0.4279 - accuracy: 0.7995\n",
      "Epoch 459/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4402 - accuracy: 0.7760\n",
      "Epoch 460/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4323 - accuracy: 0.7839\n",
      "Epoch 461/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4341 - accuracy: 0.7839\n",
      "Epoch 462/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4240 - accuracy: 0.7891\n",
      "Epoch 463/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4192 - accuracy: 0.8021\n",
      "Epoch 464/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4230 - accuracy: 0.7930\n",
      "Epoch 465/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4200 - accuracy: 0.7995\n",
      "Epoch 466/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4218 - accuracy: 0.7995\n",
      "Epoch 467/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4217 - accuracy: 0.7956\n",
      "Epoch 468/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4236 - accuracy: 0.7943\n",
      "Epoch 469/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4244 - accuracy: 0.7891\n",
      "Epoch 470/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4225 - accuracy: 0.7969\n",
      "Epoch 471/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4171 - accuracy: 0.7930\n",
      "Epoch 472/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4211 - accuracy: 0.78 - 0s 731us/step - loss: 0.4233 - accuracy: 0.7995\n",
      "Epoch 473/500\n",
      "16/16 [==============================] - 0s 732us/step - loss: 0.4267 - accuracy: 0.7969\n",
      "Epoch 474/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4255 - accuracy: 0.7943\n",
      "Epoch 475/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 731us/step - loss: 0.4199 - accuracy: 0.7969\n",
      "Epoch 476/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4212 - accuracy: 0.7878\n",
      "Epoch 477/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4359 - accuracy: 0.7930\n",
      "Epoch 478/500\n",
      "16/16 [==============================] - 0s 865us/step - loss: 0.4229 - accuracy: 0.7878\n",
      "Epoch 479/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4188 - accuracy: 0.7917\n",
      "Epoch 480/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4236 - accuracy: 0.7943\n",
      "Epoch 481/500\n",
      "16/16 [==============================] - 0s 599us/step - loss: 0.4172 - accuracy: 0.7969\n",
      "Epoch 482/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4263 - accuracy: 0.7995\n",
      "Epoch 483/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4310 - accuracy: 0.7852\n",
      "Epoch 484/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4168 - accuracy: 0.7995\n",
      "Epoch 485/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4161 - accuracy: 0.8047\n",
      "Epoch 486/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4149 - accuracy: 0.7969\n",
      "Epoch 487/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4199 - accuracy: 0.7956\n",
      "Epoch 488/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4191 - accuracy: 0.7982\n",
      "Epoch 489/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4183 - accuracy: 0.7969\n",
      "Epoch 490/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4211 - accuracy: 0.8008\n",
      "Epoch 491/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4286 - accuracy: 0.7904\n",
      "Epoch 492/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4187 - accuracy: 0.7956\n",
      "Epoch 493/500\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4218 - accuracy: 0.7839\n",
      "Epoch 494/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4279 - accuracy: 0.7956\n",
      "Epoch 495/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4364 - accuracy: 0.8034\n",
      "Epoch 496/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4318 - accuracy: 0.7878\n",
      "Epoch 497/500\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4180 - accuracy: 0.8008\n",
      "Epoch 498/500\n",
      "16/16 [==============================] - 0s 797us/step - loss: 0.4223 - accuracy: 0.7956\n",
      "Epoch 499/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4557 - accuracy: 0.7891\n",
      "Epoch 500/500\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4159 - accuracy: 0.7956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e5eb1dd820>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=500, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "312cbbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      " 1/16 [>.............................] - ETA: 3s - loss: 4.3358 - accuracy: 0.7600\n",
      "Epoch 00001: loss improved from inf to 8.43255, saving model to ./pima_model\\01-8.4325.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.4325 - accuracy: 0.6068\n",
      "Epoch 2/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 7.4859 - accuracy: 0.6000\n",
      "Epoch 00002: loss improved from 8.43255 to 5.13224, saving model to ./pima_model\\02-5.1322.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.1322 - accuracy: 0.5690\n",
      "Epoch 3/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 4.0559 - accuracy: 0.4800\n",
      "Epoch 00003: loss improved from 5.13224 to 3.16254, saving model to ./pima_model\\03-3.1625.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.1625 - accuracy: 0.5299\n",
      "Epoch 4/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 2.8945 - accuracy: 0.3400\n",
      "Epoch 00004: loss improved from 3.16254 to 2.15491, saving model to ./pima_model\\04-2.1549.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.1549 - accuracy: 0.4948\n",
      "Epoch 5/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 1.8245 - accuracy: 0.5400\n",
      "Epoch 00005: loss improved from 2.15491 to 1.69604, saving model to ./pima_model\\05-1.6960.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6960 - accuracy: 0.5482\n",
      "Epoch 6/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 2.2727 - accuracy: 0.4800\n",
      "Epoch 00006: loss improved from 1.69604 to 1.42739, saving model to ./pima_model\\06-1.4274.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4274 - accuracy: 0.5534\n",
      "Epoch 7/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 1.2446 - accuracy: 0.6000\n",
      "Epoch 00007: loss improved from 1.42739 to 1.23221, saving model to ./pima_model\\07-1.2322.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2322 - accuracy: 0.5846\n",
      "Epoch 8/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 1.1161 - accuracy: 0.6200\n",
      "Epoch 00008: loss improved from 1.23221 to 1.11213, saving model to ./pima_model\\08-1.1121.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1121 - accuracy: 0.6016\n",
      "Epoch 9/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.7563 - accuracy: 0.6800\n",
      "Epoch 00009: loss improved from 1.11213 to 1.03779, saving model to ./pima_model\\09-1.0378.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0378 - accuracy: 0.6211\n",
      "Epoch 10/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.7787 - accuracy: 0.6600\n",
      "Epoch 00010: loss improved from 1.03779 to 0.96260, saving model to ./pima_model\\10-0.9626.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.9626 - accuracy: 0.6315\n",
      "Epoch 11/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 1.2680 - accuracy: 0.5400\n",
      "Epoch 00011: loss improved from 0.96260 to 0.92348, saving model to ./pima_model\\11-0.9235.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.9235 - accuracy: 0.6276\n",
      "Epoch 12/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5593 - accuracy: 0.7200\n",
      "Epoch 00012: loss improved from 0.92348 to 0.89376, saving model to ./pima_model\\12-0.8938.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8938 - accuracy: 0.6315\n",
      "Epoch 13/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.8297 - accuracy: 0.7000\n",
      "Epoch 00013: loss improved from 0.89376 to 0.86069, saving model to ./pima_model\\13-0.8607.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8607 - accuracy: 0.6445\n",
      "Epoch 14/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.8127 - accuracy: 0.6000\n",
      "Epoch 00014: loss improved from 0.86069 to 0.83948, saving model to ./pima_model\\14-0.8395.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8395 - accuracy: 0.6602\n",
      "Epoch 15/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6260 - accuracy: 0.6800\n",
      "Epoch 00015: loss improved from 0.83948 to 0.81627, saving model to ./pima_model\\15-0.8163.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8163 - accuracy: 0.6471\n",
      "Epoch 16/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.7121 - accuracy: 0.7200\n",
      "Epoch 00016: loss improved from 0.81627 to 0.80527, saving model to ./pima_model\\16-0.8053.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8053 - accuracy: 0.6706\n",
      "Epoch 17/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.9193 - accuracy: 0.6800\n",
      "Epoch 00017: loss improved from 0.80527 to 0.79363, saving model to ./pima_model\\17-0.7936.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7936 - accuracy: 0.6641\n",
      "Epoch 18/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.7817 - accuracy: 0.5400\n",
      "Epoch 00018: loss did not improve from 0.79363\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.7952 - accuracy: 0.6458\n",
      "Epoch 19/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.8077 - accuracy: 0.6400\n",
      "Epoch 00019: loss improved from 0.79363 to 0.77845, saving model to ./pima_model\\19-0.7784.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7784 - accuracy: 0.6549\n",
      "Epoch 20/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6018 - accuracy: 0.7200\n",
      "Epoch 00020: loss improved from 0.77845 to 0.76504, saving model to ./pima_model\\20-0.7650.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7650 - accuracy: 0.6380\n",
      "Epoch 21/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.7911 - accuracy: 0.7200\n",
      "Epoch 00021: loss improved from 0.76504 to 0.75085, saving model to ./pima_model\\21-0.7508.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7508 - accuracy: 0.6458\n",
      "Epoch 22/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.7091 - accuracy: 0.6800\n",
      "Epoch 00022: loss did not improve from 0.75085\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.7753 - accuracy: 0.6133\n",
      "Epoch 23/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6680 - accuracy: 0.6200\n",
      "Epoch 00023: loss improved from 0.75085 to 0.74873, saving model to ./pima_model\\23-0.7487.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7487 - accuracy: 0.6589\n",
      "Epoch 24/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5643 - accuracy: 0.7800\n",
      "Epoch 00024: loss improved from 0.74873 to 0.72565, saving model to ./pima_model\\24-0.7257.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7257 - accuracy: 0.6654\n",
      "Epoch 25/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6375 - accuracy: 0.6600\n",
      "Epoch 00025: loss improved from 0.72565 to 0.71204, saving model to ./pima_model\\25-0.7120.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.7120 - accuracy: 0.6484\n",
      "Epoch 26/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.8220 - accuracy: 0.6000\n",
      "Epoch 00026: loss improved from 0.71204 to 0.70266, saving model to ./pima_model\\26-0.7027.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7027 - accuracy: 0.6602\n",
      "Epoch 27/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6114 - accuracy: 0.7000\n",
      "Epoch 00027: loss did not improve from 0.70266\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.7068 - accuracy: 0.6497\n",
      "Epoch 28/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5869 - accuracy: 0.7000\n",
      "Epoch 00028: loss improved from 0.70266 to 0.70083, saving model to ./pima_model\\28-0.7008.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7008 - accuracy: 0.6667\n",
      "Epoch 29/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5589 - accuracy: 0.7800\n",
      "Epoch 00029: loss improved from 0.70083 to 0.69568, saving model to ./pima_model\\29-0.6957.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6957 - accuracy: 0.6602\n",
      "Epoch 30/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5191 - accuracy: 0.6600\n",
      "Epoch 00030: loss improved from 0.69568 to 0.67027, saving model to ./pima_model\\30-0.6703.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6703 - accuracy: 0.6549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6833 - accuracy: 0.6200\n",
      "Epoch 00031: loss did not improve from 0.67027\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.6724 - accuracy: 0.6680\n",
      "Epoch 32/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.7545 - accuracy: 0.6200\n",
      "Epoch 00032: loss improved from 0.67027 to 0.66741, saving model to ./pima_model\\32-0.6674.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6674 - accuracy: 0.6523\n",
      "Epoch 33/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6157 - accuracy: 0.7200\n",
      "Epoch 00033: loss improved from 0.66741 to 0.65749, saving model to ./pima_model\\33-0.6575.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6575 - accuracy: 0.6641\n",
      "Epoch 34/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.7424 - accuracy: 0.6200\n",
      "Epoch 00034: loss improved from 0.65749 to 0.65475, saving model to ./pima_model\\34-0.6548.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6548 - accuracy: 0.6628\n",
      "Epoch 35/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.8109 - accuracy: 0.5800\n",
      "Epoch 00035: loss did not improve from 0.65475\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.6668 - accuracy: 0.6602\n",
      "Epoch 36/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5424 - accuracy: 0.7600\n",
      "Epoch 00036: loss improved from 0.65475 to 0.64308, saving model to ./pima_model\\36-0.6431.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6431 - accuracy: 0.6680\n",
      "Epoch 37/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5577 - accuracy: 0.7200\n",
      "Epoch 00037: loss did not improve from 0.64308\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.6823 - accuracy: 0.6615\n",
      "Epoch 38/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5802 - accuracy: 0.7200\n",
      "Epoch 00038: loss did not improve from 0.64308\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.6434 - accuracy: 0.6667\n",
      "Epoch 39/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.7937 - accuracy: 0.5000\n",
      "Epoch 00039: loss improved from 0.64308 to 0.63411, saving model to ./pima_model\\39-0.6341.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6341 - accuracy: 0.6667\n",
      "Epoch 40/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6064 - accuracy: 0.7200\n",
      "Epoch 00040: loss did not improve from 0.63411\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.6353 - accuracy: 0.6771\n",
      "Epoch 41/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.7277 - accuracy: 0.5200\n",
      "Epoch 00041: loss improved from 0.63411 to 0.62815, saving model to ./pima_model\\41-0.6281.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6281 - accuracy: 0.6680\n",
      "Epoch 42/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5547 - accuracy: 0.7000\n",
      "Epoch 00042: loss improved from 0.62815 to 0.62394, saving model to ./pima_model\\42-0.6239.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6239 - accuracy: 0.6771\n",
      "Epoch 43/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6597 - accuracy: 0.6600\n",
      "Epoch 00043: loss improved from 0.62394 to 0.62383, saving model to ./pima_model\\43-0.6238.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6238 - accuracy: 0.6706\n",
      "Epoch 44/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6290 - accuracy: 0.7600\n",
      "Epoch 00044: loss improved from 0.62383 to 0.62155, saving model to ./pima_model\\44-0.6215.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6215 - accuracy: 0.6667\n",
      "Epoch 45/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6194 - accuracy: 0.6400\n",
      "Epoch 00045: loss improved from 0.62155 to 0.62057, saving model to ./pima_model\\45-0.6206.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6206 - accuracy: 0.6602\n",
      "Epoch 46/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4788 - accuracy: 0.8400\n",
      "Epoch 00046: loss improved from 0.62057 to 0.61590, saving model to ./pima_model\\46-0.6159.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6159 - accuracy: 0.6732\n",
      "Epoch 47/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5965 - accuracy: 0.6800\n",
      "Epoch 00047: loss did not improve from 0.61590\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6209 - accuracy: 0.6693\n",
      "Epoch 48/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5471 - accuracy: 0.7000\n",
      "Epoch 00048: loss did not improve from 0.61590\n",
      "16/16 [==============================] - 0s 689us/step - loss: 0.6290 - accuracy: 0.6549\n",
      "Epoch 49/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.7183 - accuracy: 0.5600\n",
      "Epoch 00049: loss improved from 0.61590 to 0.60962, saving model to ./pima_model\\49-0.6096.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6096 - accuracy: 0.6602\n",
      "Epoch 50/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6482 - accuracy: 0.6800\n",
      "Epoch 00050: loss improved from 0.60962 to 0.60210, saving model to ./pima_model\\50-0.6021.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6021 - accuracy: 0.6836\n",
      "Epoch 51/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6855 - accuracy: 0.6800\n",
      "Epoch 00051: loss did not improve from 0.60210\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.6056 - accuracy: 0.6745\n",
      "Epoch 52/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6153 - accuracy: 0.6800\n",
      "Epoch 00052: loss did not improve from 0.60210\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.6118 - accuracy: 0.6758\n",
      "Epoch 53/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6070 - accuracy: 0.6600\n",
      "Epoch 00053: loss did not improve from 0.60210\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6191 - accuracy: 0.6628\n",
      "Epoch 54/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5192 - accuracy: 0.8000\n",
      "Epoch 00054: loss did not improve from 0.60210\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.6133 - accuracy: 0.6849\n",
      "Epoch 55/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5205 - accuracy: 0.7800\n",
      "Epoch 00055: loss did not improve from 0.60210\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.6182 - accuracy: 0.6771\n",
      "Epoch 56/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6064 - accuracy: 0.6800\n",
      "Epoch 00056: loss did not improve from 0.60210\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.6032 - accuracy: 0.6992\n",
      "Epoch 57/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5231 - accuracy: 0.7000\n",
      "Epoch 00057: loss did not improve from 0.60210\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.6070 - accuracy: 0.6901\n",
      "Epoch 58/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5319 - accuracy: 0.7400\n",
      "Epoch 00058: loss improved from 0.60210 to 0.58732, saving model to ./pima_model\\58-0.5873.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5873 - accuracy: 0.6914\n",
      "Epoch 59/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6240 - accuracy: 0.6600\n",
      "Epoch 00059: loss did not improve from 0.58732\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5960 - accuracy: 0.6836\n",
      "Epoch 60/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5584 - accuracy: 0.7600\n",
      "Epoch 00060: loss improved from 0.58732 to 0.58093, saving model to ./pima_model\\60-0.5809.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5809 - accuracy: 0.6719\n",
      "Epoch 61/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5839 - accuracy: 0.7400\n",
      "Epoch 00061: loss improved from 0.58093 to 0.57538, saving model to ./pima_model\\61-0.5754.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5754 - accuracy: 0.7031\n",
      "Epoch 62/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4751 - accuracy: 0.7400\n",
      "Epoch 00062: loss did not improve from 0.57538\n",
      "16/16 [==============================] - 0s 733us/step - loss: 0.5775 - accuracy: 0.6927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6227 - accuracy: 0.7200\n",
      "Epoch 00063: loss did not improve from 0.57538\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5866 - accuracy: 0.6875\n",
      "Epoch 64/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4752 - accuracy: 0.7600\n",
      "Epoch 00064: loss improved from 0.57538 to 0.57331, saving model to ./pima_model\\64-0.5733.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5733 - accuracy: 0.7018\n",
      "Epoch 65/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5806 - accuracy: 0.7000\n",
      "Epoch 00065: loss improved from 0.57331 to 0.57090, saving model to ./pima_model\\65-0.5709.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5709 - accuracy: 0.6914\n",
      "Epoch 66/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5287 - accuracy: 0.6800\n",
      "Epoch 00066: loss improved from 0.57090 to 0.56639, saving model to ./pima_model\\66-0.5664.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5664 - accuracy: 0.7057\n",
      "Epoch 67/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4945 - accuracy: 0.8000\n",
      "Epoch 00067: loss did not improve from 0.56639\n",
      "16/16 [==============================] - 0s 730us/step - loss: 0.5832 - accuracy: 0.6823\n",
      "Epoch 68/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5518 - accuracy: 0.7000\n",
      "Epoch 00068: loss did not improve from 0.56639\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5681 - accuracy: 0.7005\n",
      "Epoch 69/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5421 - accuracy: 0.7400\n",
      "Epoch 00069: loss did not improve from 0.56639\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5672 - accuracy: 0.7005\n",
      "Epoch 70/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5787 - accuracy: 0.6800\n",
      "Epoch 00070: loss did not improve from 0.56639\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5768 - accuracy: 0.6862\n",
      "Epoch 71/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4984 - accuracy: 0.7600\n",
      "Epoch 00071: loss did not improve from 0.56639\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.5668 - accuracy: 0.6953\n",
      "Epoch 72/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5380 - accuracy: 0.7400\n",
      "Epoch 00072: loss did not improve from 0.56639\n",
      "16/16 [==============================] - 0s 999us/step - loss: 0.5698 - accuracy: 0.7005\n",
      "Epoch 73/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5404 - accuracy: 0.7200\n",
      "Epoch 00073: loss improved from 0.56639 to 0.56057, saving model to ./pima_model\\73-0.5606.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5606 - accuracy: 0.6966\n",
      "Epoch 74/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5652 - accuracy: 0.6800\n",
      "Epoch 00074: loss improved from 0.56057 to 0.55783, saving model to ./pima_model\\74-0.5578.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5578 - accuracy: 0.7148\n",
      "Epoch 75/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6280 - accuracy: 0.7000\n",
      "Epoch 00075: loss improved from 0.55783 to 0.55482, saving model to ./pima_model\\75-0.5548.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5548 - accuracy: 0.7018\n",
      "Epoch 76/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5817 - accuracy: 0.7200\n",
      "Epoch 00076: loss did not improve from 0.55482\n",
      "16/16 [==============================] - 0s 733us/step - loss: 0.5593 - accuracy: 0.6992\n",
      "Epoch 77/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5315 - accuracy: 0.6800\n",
      "Epoch 00077: loss did not improve from 0.55482\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5560 - accuracy: 0.7083\n",
      "Epoch 78/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4869 - accuracy: 0.7600\n",
      "Epoch 00078: loss improved from 0.55482 to 0.54742, saving model to ./pima_model\\78-0.5474.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5474 - accuracy: 0.7135\n",
      "Epoch 79/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5654 - accuracy: 0.8000\n",
      "Epoch 00079: loss did not improve from 0.54742\n",
      "16/16 [==============================] - 0s 797us/step - loss: 0.5531 - accuracy: 0.7266\n",
      "Epoch 80/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5371 - accuracy: 0.7400\n",
      "Epoch 00080: loss did not improve from 0.54742\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5655 - accuracy: 0.6953\n",
      "Epoch 81/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5729 - accuracy: 0.7000\n",
      "Epoch 00081: loss did not improve from 0.54742\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.5511 - accuracy: 0.7083\n",
      "Epoch 82/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5650 - accuracy: 0.6600\n",
      "Epoch 00082: loss improved from 0.54742 to 0.54321, saving model to ./pima_model\\82-0.5432.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5432 - accuracy: 0.7135\n",
      "Epoch 83/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5088 - accuracy: 0.8400\n",
      "Epoch 00083: loss did not improve from 0.54321\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.5471 - accuracy: 0.7174\n",
      "Epoch 84/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5128 - accuracy: 0.7000\n",
      "Epoch 00084: loss did not improve from 0.54321\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5439 - accuracy: 0.7161\n",
      "Epoch 85/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5395 - accuracy: 0.7200\n",
      "Epoch 00085: loss did not improve from 0.54321\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5447 - accuracy: 0.7318\n",
      "Epoch 86/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5708 - accuracy: 0.6400\n",
      "Epoch 00086: loss did not improve from 0.54321\n",
      "16/16 [==============================] - 0s 801us/step - loss: 0.5524 - accuracy: 0.7135\n",
      "Epoch 87/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5039 - accuracy: 0.7200\n",
      "Epoch 00087: loss improved from 0.54321 to 0.53646, saving model to ./pima_model\\87-0.5365.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5365 - accuracy: 0.7266\n",
      "Epoch 88/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5202 - accuracy: 0.8000\n",
      "Epoch 00088: loss did not improve from 0.53646\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.5377 - accuracy: 0.7318\n",
      "Epoch 89/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5502 - accuracy: 0.7200\n",
      "Epoch 00089: loss did not improve from 0.53646\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5513 - accuracy: 0.7083\n",
      "Epoch 90/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.7323 - accuracy: 0.6000\n",
      "Epoch 00090: loss did not improve from 0.53646\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.5374 - accuracy: 0.7292\n",
      "Epoch 91/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5295 - accuracy: 0.7600\n",
      "Epoch 00091: loss did not improve from 0.53646\n",
      "16/16 [==============================] - 0s 733us/step - loss: 0.5431 - accuracy: 0.7266\n",
      "Epoch 92/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6100 - accuracy: 0.7000\n",
      "Epoch 00092: loss improved from 0.53646 to 0.53509, saving model to ./pima_model\\92-0.5351.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5351 - accuracy: 0.7240\n",
      "Epoch 93/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5599 - accuracy: 0.7200\n",
      "Epoch 00093: loss did not improve from 0.53509\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5494 - accuracy: 0.7083\n",
      "Epoch 94/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4833 - accuracy: 0.8400\n",
      "Epoch 00094: loss did not improve from 0.53509\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5398 - accuracy: 0.7279\n",
      "Epoch 95/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6107 - accuracy: 0.7000\n",
      "Epoch 00095: loss did not improve from 0.53509\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.5459 - accuracy: 0.7057\n",
      "Epoch 96/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5375 - accuracy: 0.7800\n",
      "Epoch 00096: loss did not improve from 0.53509\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.5405 - accuracy: 0.7109\n",
      "Epoch 97/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5300 - accuracy: 0.7400\n",
      "Epoch 00097: loss did not improve from 0.53509\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5392 - accuracy: 0.7188\n",
      "Epoch 98/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5372 - accuracy: 0.7400\n",
      "Epoch 00098: loss improved from 0.53509 to 0.53279, saving model to ./pima_model\\98-0.5328.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7161\n",
      "Epoch 99/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5766 - accuracy: 0.7800\n",
      "Epoch 00099: loss improved from 0.53279 to 0.52853, saving model to ./pima_model\\99-0.5285.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5285 - accuracy: 0.7409\n",
      "Epoch 100/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6772 - accuracy: 0.6000\n",
      "Epoch 00100: loss did not improve from 0.52853\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5286 - accuracy: 0.7227\n",
      "Epoch 101/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4869 - accuracy: 0.8200\n",
      "Epoch 00101: loss did not improve from 0.52853\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5289 - accuracy: 0.7279\n",
      "Epoch 102/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5152 - accuracy: 0.7200\n",
      "Epoch 00102: loss did not improve from 0.52853\n",
      "16/16 [==============================] - 0s 732us/step - loss: 0.5543 - accuracy: 0.7109\n",
      "Epoch 103/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5657 - accuracy: 0.6600\n",
      "Epoch 00103: loss did not improve from 0.52853\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.5313 - accuracy: 0.7266\n",
      "Epoch 104/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5873 - accuracy: 0.6800\n",
      "Epoch 00104: loss did not improve from 0.52853\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.5386 - accuracy: 0.7201\n",
      "Epoch 105/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6144 - accuracy: 0.6600\n",
      "Epoch 00105: loss did not improve from 0.52853\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.5293 - accuracy: 0.7253\n",
      "Epoch 106/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5640 - accuracy: 0.7200\n",
      "Epoch 00106: loss did not improve from 0.52853\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.5473 - accuracy: 0.7148\n",
      "Epoch 107/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4831 - accuracy: 0.7800\n",
      "Epoch 00107: loss improved from 0.52853 to 0.52439, saving model to ./pima_model\\107-0.5244.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5244 - accuracy: 0.7357\n",
      "Epoch 108/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5240 - accuracy: 0.7400\n",
      "Epoch 00108: loss did not improve from 0.52439\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5352 - accuracy: 0.7096\n",
      "Epoch 109/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5301 - accuracy: 0.7200\n",
      "Epoch 00109: loss did not improve from 0.52439\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5322 - accuracy: 0.7422\n",
      "Epoch 110/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5354 - accuracy: 0.7400\n",
      "Epoch 00110: loss did not improve from 0.52439\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5295 - accuracy: 0.7396\n",
      "Epoch 111/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5628 - accuracy: 0.6800\n",
      "Epoch 00111: loss did not improve from 0.52439\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5360 - accuracy: 0.7214\n",
      "Epoch 112/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4712 - accuracy: 0.7400\n",
      "Epoch 00112: loss did not improve from 0.52439\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5408 - accuracy: 0.7227\n",
      "Epoch 113/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5301 - accuracy: 0.7400\n",
      "Epoch 00113: loss did not improve from 0.52439\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5324 - accuracy: 0.7318\n",
      "Epoch 114/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4804 - accuracy: 0.7600\n",
      "Epoch 00114: loss improved from 0.52439 to 0.51536, saving model to ./pima_model\\114-0.5154.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5154 - accuracy: 0.7370\n",
      "Epoch 115/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5220 - accuracy: 0.7200\n",
      "Epoch 00115: loss did not improve from 0.51536\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5313 - accuracy: 0.7435\n",
      "Epoch 116/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4359 - accuracy: 0.8600\n",
      "Epoch 00116: loss did not improve from 0.51536\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.5182 - accuracy: 0.7461\n",
      "Epoch 117/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5053 - accuracy: 0.7400\n",
      "Epoch 00117: loss did not improve from 0.51536\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.5381 - accuracy: 0.7096\n",
      "Epoch 118/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3702 - accuracy: 0.8200\n",
      "Epoch 00118: loss did not improve from 0.51536\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.5341 - accuracy: 0.7318\n",
      "Epoch 119/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5880 - accuracy: 0.6800\n",
      "Epoch 00119: loss did not improve from 0.51536\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.5234 - accuracy: 0.7357\n",
      "Epoch 120/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4776 - accuracy: 0.8600\n",
      "Epoch 00120: loss did not improve from 0.51536\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.5228 - accuracy: 0.7344\n",
      "Epoch 121/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5547 - accuracy: 0.7000\n",
      "Epoch 00121: loss did not improve from 0.51536\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.5248 - accuracy: 0.7422\n",
      "Epoch 122/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5639 - accuracy: 0.6800\n",
      "Epoch 00122: loss did not improve from 0.51536\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5389 - accuracy: 0.7279\n",
      "Epoch 123/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5489 - accuracy: 0.7400\n",
      "Epoch 00123: loss improved from 0.51536 to 0.51520, saving model to ./pima_model\\123-0.5152.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5152 - accuracy: 0.7422\n",
      "Epoch 124/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4935 - accuracy: 0.7400\n",
      "Epoch 00124: loss did not improve from 0.51520\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5231 - accuracy: 0.7279\n",
      "Epoch 125/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5481 - accuracy: 0.7800\n",
      "Epoch 00125: loss did not improve from 0.51520\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5287 - accuracy: 0.7370\n",
      "Epoch 126/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4415 - accuracy: 0.7800\n",
      "Epoch 00126: loss did not improve from 0.51520\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5156 - accuracy: 0.7383\n",
      "Epoch 127/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5124 - accuracy: 0.7400\n",
      "Epoch 00127: loss improved from 0.51520 to 0.51049, saving model to ./pima_model\\127-0.5105.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.7500\n",
      "Epoch 128/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5209 - accuracy: 0.7200\n",
      "Epoch 00128: loss did not improve from 0.51049\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.5239 - accuracy: 0.7305\n",
      "Epoch 129/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5900 - accuracy: 0.6000\n",
      "Epoch 00129: loss did not improve from 0.51049\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5257 - accuracy: 0.7396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4534 - accuracy: 0.8200\n",
      "Epoch 00130: loss did not improve from 0.51049\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.5166 - accuracy: 0.7357\n",
      "Epoch 131/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5521 - accuracy: 0.7200\n",
      "Epoch 00131: loss did not improve from 0.51049\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5106 - accuracy: 0.7539\n",
      "Epoch 132/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4819 - accuracy: 0.7600\n",
      "Epoch 00132: loss did not improve from 0.51049\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5202 - accuracy: 0.7383\n",
      "Epoch 133/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6347 - accuracy: 0.6400\n",
      "Epoch 00133: loss did not improve from 0.51049\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5167 - accuracy: 0.7409\n",
      "Epoch 134/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4913 - accuracy: 0.8000\n",
      "Epoch 00134: loss did not improve from 0.51049\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5141 - accuracy: 0.7396\n",
      "Epoch 135/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6127 - accuracy: 0.6400\n",
      "Epoch 00135: loss did not improve from 0.51049\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.5199 - accuracy: 0.7279\n",
      "Epoch 136/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4483 - accuracy: 0.7600\n",
      "Epoch 00136: loss improved from 0.51049 to 0.50955, saving model to ./pima_model\\136-0.5095.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5095 - accuracy: 0.7370\n",
      "Epoch 137/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5350 - accuracy: 0.7400\n",
      "Epoch 00137: loss did not improve from 0.50955\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.5177 - accuracy: 0.7435\n",
      "Epoch 138/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4831 - accuracy: 0.7600\n",
      "Epoch 00138: loss did not improve from 0.50955\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.5101 - accuracy: 0.7656\n",
      "Epoch 139/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5193 - accuracy: 0.6600\n",
      "Epoch 00139: loss did not improve from 0.50955\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5249 - accuracy: 0.7279\n",
      "Epoch 140/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4986 - accuracy: 0.7800\n",
      "Epoch 00140: loss improved from 0.50955 to 0.50743, saving model to ./pima_model\\140-0.5074.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5074 - accuracy: 0.7591\n",
      "Epoch 141/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5054 - accuracy: 0.7600\n",
      "Epoch 00141: loss did not improve from 0.50743\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.5082 - accuracy: 0.7500\n",
      "Epoch 142/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4821 - accuracy: 0.7800\n",
      "Epoch 00142: loss did not improve from 0.50743\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5204 - accuracy: 0.7266\n",
      "Epoch 143/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4466 - accuracy: 0.8000\n",
      "Epoch 00143: loss did not improve from 0.50743\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5124 - accuracy: 0.7487\n",
      "Epoch 144/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4732 - accuracy: 0.7600\n",
      "Epoch 00144: loss improved from 0.50743 to 0.50601, saving model to ./pima_model\\144-0.5060.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5060 - accuracy: 0.7409\n",
      "Epoch 145/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6011 - accuracy: 0.6800\n",
      "Epoch 00145: loss improved from 0.50601 to 0.50316, saving model to ./pima_model\\145-0.5032.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5032 - accuracy: 0.7565\n",
      "Epoch 146/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4930 - accuracy: 0.7800\n",
      "Epoch 00146: loss improved from 0.50316 to 0.50084, saving model to ./pima_model\\146-0.5008.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5008 - accuracy: 0.7526\n",
      "Epoch 147/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5461 - accuracy: 0.7600\n",
      "Epoch 00147: loss did not improve from 0.50084\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.5101 - accuracy: 0.7513\n",
      "Epoch 148/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4410 - accuracy: 0.7800\n",
      "Epoch 00148: loss improved from 0.50084 to 0.49926, saving model to ./pima_model\\148-0.4993.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4993 - accuracy: 0.7669\n",
      "Epoch 149/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5169 - accuracy: 0.7200\n",
      "Epoch 00149: loss did not improve from 0.49926\n",
      "16/16 [==============================] - 0s 866us/step - loss: 0.5111 - accuracy: 0.7591\n",
      "Epoch 150/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5044 - accuracy: 0.7400\n",
      "Epoch 00150: loss did not improve from 0.49926\n",
      "16/16 [==============================] - 0s 866us/step - loss: 0.5108 - accuracy: 0.7396\n",
      "Epoch 151/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6516 - accuracy: 0.6400\n",
      "Epoch 00151: loss did not improve from 0.49926\n",
      "16/16 [==============================] - 0s 866us/step - loss: 0.5197 - accuracy: 0.7370\n",
      "Epoch 152/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4711 - accuracy: 0.7800\n",
      "Epoch 00152: loss did not improve from 0.49926\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.5150 - accuracy: 0.7422\n",
      "Epoch 153/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5427 - accuracy: 0.6400\n",
      "Epoch 00153: loss did not improve from 0.49926\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.5045 - accuracy: 0.7526\n",
      "Epoch 154/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5216 - accuracy: 0.7600\n",
      "Epoch 00154: loss did not improve from 0.49926\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.5212 - accuracy: 0.7448\n",
      "Epoch 155/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4038 - accuracy: 0.8800\n",
      "Epoch 00155: loss did not improve from 0.49926\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.5068 - accuracy: 0.7487\n",
      "Epoch 156/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5148 - accuracy: 0.7400\n",
      "Epoch 00156: loss did not improve from 0.49926\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.5201 - accuracy: 0.7526\n",
      "Epoch 157/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4133 - accuracy: 0.8200\n",
      "Epoch 00157: loss did not improve from 0.49926\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5112 - accuracy: 0.7448\n",
      "Epoch 158/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5636 - accuracy: 0.6800\n",
      "Epoch 00158: loss did not improve from 0.49926\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.5234 - accuracy: 0.7539\n",
      "Epoch 159/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4840 - accuracy: 0.7400\n",
      "Epoch 00159: loss did not improve from 0.49926\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.5115 - accuracy: 0.7318\n",
      "Epoch 160/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6580 - accuracy: 0.6400\n",
      "Epoch 00160: loss did not improve from 0.49926\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5000 - accuracy: 0.7591\n",
      "Epoch 161/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3709 - accuracy: 0.8200\n",
      "Epoch 00161: loss did not improve from 0.49926\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5068 - accuracy: 0.7526\n",
      "Epoch 162/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5201 - accuracy: 0.7400\n",
      "Epoch 00162: loss did not improve from 0.49926\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5038 - accuracy: 0.7578\n",
      "Epoch 163/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4973 - accuracy: 0.7600\n",
      "Epoch 00163: loss did not improve from 0.49926\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5039 - accuracy: 0.7526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5373 - accuracy: 0.7200\n",
      "Epoch 00164: loss did not improve from 0.49926\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5003 - accuracy: 0.7591\n",
      "Epoch 165/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4958 - accuracy: 0.7800\n",
      "Epoch 00165: loss did not improve from 0.49926\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.5075 - accuracy: 0.7487\n",
      "Epoch 166/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5546 - accuracy: 0.7000\n",
      "Epoch 00166: loss improved from 0.49926 to 0.49909, saving model to ./pima_model\\166-0.4991.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4991 - accuracy: 0.7630\n",
      "Epoch 167/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4275 - accuracy: 0.8000\n",
      "Epoch 00167: loss did not improve from 0.49909\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.5053 - accuracy: 0.7513\n",
      "Epoch 168/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4594 - accuracy: 0.7600\n",
      "Epoch 00168: loss did not improve from 0.49909\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.5055 - accuracy: 0.7474\n",
      "Epoch 169/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5450 - accuracy: 0.6800\n",
      "Epoch 00169: loss improved from 0.49909 to 0.49703, saving model to ./pima_model\\169-0.4970.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4970 - accuracy: 0.7669\n",
      "Epoch 170/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4690 - accuracy: 0.7400\n",
      "Epoch 00170: loss did not improve from 0.49703\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5059 - accuracy: 0.7474\n",
      "Epoch 171/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6081 - accuracy: 0.7000\n",
      "Epoch 00171: loss did not improve from 0.49703\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.5037 - accuracy: 0.7565\n",
      "Epoch 172/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3691 - accuracy: 0.8600\n",
      "Epoch 00172: loss improved from 0.49703 to 0.49691, saving model to ./pima_model\\172-0.4969.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4969 - accuracy: 0.7539\n",
      "Epoch 173/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4137 - accuracy: 0.8000\n",
      "Epoch 00173: loss did not improve from 0.49691\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5083 - accuracy: 0.7435\n",
      "Epoch 174/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5360 - accuracy: 0.7000\n",
      "Epoch 00174: loss did not improve from 0.49691\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4971 - accuracy: 0.7630\n",
      "Epoch 175/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5367 - accuracy: 0.7200\n",
      "Epoch 00175: loss did not improve from 0.49691\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.5221 - accuracy: 0.7396\n",
      "Epoch 176/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4446 - accuracy: 0.8200\n",
      "Epoch 00176: loss did not improve from 0.49691\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.5083 - accuracy: 0.7552\n",
      "Epoch 177/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5146 - accuracy: 0.7400\n",
      "Epoch 00177: loss improved from 0.49691 to 0.48911, saving model to ./pima_model\\177-0.4891.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.7695\n",
      "Epoch 178/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5757 - accuracy: 0.7200\n",
      "Epoch 00178: loss improved from 0.48911 to 0.48892, saving model to ./pima_model\\178-0.4889.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.7552\n",
      "Epoch 179/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4650 - accuracy: 0.8400\n",
      "Epoch 00179: loss did not improve from 0.48892\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4977 - accuracy: 0.7682\n",
      "Epoch 180/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5977 - accuracy: 0.7000\n",
      "Epoch 00180: loss did not improve from 0.48892\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4994 - accuracy: 0.7526\n",
      "Epoch 181/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5535 - accuracy: 0.7400\n",
      "Epoch 00181: loss did not improve from 0.48892\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.5048 - accuracy: 0.7474\n",
      "Epoch 182/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5257 - accuracy: 0.7600\n",
      "Epoch 00182: loss did not improve from 0.48892\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4934 - accuracy: 0.7604\n",
      "Epoch 183/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4350 - accuracy: 0.7800\n",
      "Epoch 00183: loss improved from 0.48892 to 0.48800, saving model to ./pima_model\\183-0.4880.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.7656\n",
      "Epoch 184/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6003 - accuracy: 0.6800\n",
      "Epoch 00184: loss did not improve from 0.48800\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4929 - accuracy: 0.7617\n",
      "Epoch 185/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5228 - accuracy: 0.7600\n",
      "Epoch 00185: loss did not improve from 0.48800\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4949 - accuracy: 0.7747\n",
      "Epoch 186/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4891 - accuracy: 0.7200\n",
      "Epoch 00186: loss did not improve from 0.48800\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5127 - accuracy: 0.7383\n",
      "Epoch 187/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4847 - accuracy: 0.8200\n",
      "Epoch 00187: loss did not improve from 0.48800\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4915 - accuracy: 0.7695\n",
      "Epoch 188/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5448 - accuracy: 0.7600\n",
      "Epoch 00188: loss did not improve from 0.48800\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4943 - accuracy: 0.7721\n",
      "Epoch 189/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3847 - accuracy: 0.8600\n",
      "Epoch 00189: loss did not improve from 0.48800\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.5007 - accuracy: 0.7578\n",
      "Epoch 190/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5858 - accuracy: 0.7000\n",
      "Epoch 00190: loss improved from 0.48800 to 0.48690, saving model to ./pima_model\\190-0.4869.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4869 - accuracy: 0.7721\n",
      "Epoch 191/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4902 - accuracy: 0.7600\n",
      "Epoch 00191: loss did not improve from 0.48690\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4946 - accuracy: 0.7526\n",
      "Epoch 192/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5023 - accuracy: 0.7200\n",
      "Epoch 00192: loss improved from 0.48690 to 0.48393, saving model to ./pima_model\\192-0.4839.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4839 - accuracy: 0.7734\n",
      "Epoch 193/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4532 - accuracy: 0.8400\n",
      "Epoch 00193: loss did not improve from 0.48393\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4992 - accuracy: 0.7578\n",
      "Epoch 194/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5366 - accuracy: 0.7400\n",
      "Epoch 00194: loss did not improve from 0.48393\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4955 - accuracy: 0.7578\n",
      "Epoch 195/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4701 - accuracy: 0.7400\n",
      "Epoch 00195: loss did not improve from 0.48393\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4919 - accuracy: 0.7500\n",
      "Epoch 196/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4627 - accuracy: 0.7600\n",
      "Epoch 00196: loss did not improve from 0.48393\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4915 - accuracy: 0.7617\n",
      "Epoch 197/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4155 - accuracy: 0.8000\n",
      "Epoch 00197: loss did not improve from 0.48393\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4873 - accuracy: 0.7695\n",
      "Epoch 198/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5812 - accuracy: 0.7400\n",
      "Epoch 00198: loss improved from 0.48393 to 0.48338, saving model to ./pima_model\\198-0.4834.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4834 - accuracy: 0.7695\n",
      "Epoch 199/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4461 - accuracy: 0.7400\n",
      "Epoch 00199: loss did not improve from 0.48338\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4880 - accuracy: 0.7734\n",
      "Epoch 200/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5503 - accuracy: 0.7000\n",
      "Epoch 00200: loss did not improve from 0.48338\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4886 - accuracy: 0.7721\n",
      "Epoch 201/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4558 - accuracy: 0.8000\n",
      "Epoch 00201: loss improved from 0.48338 to 0.48132, saving model to ./pima_model\\201-0.4813.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4813 - accuracy: 0.7799\n",
      "Epoch 202/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5419 - accuracy: 0.7200\n",
      "Epoch 00202: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4937 - accuracy: 0.7604\n",
      "Epoch 203/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5310 - accuracy: 0.7400\n",
      "Epoch 00203: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4903 - accuracy: 0.7656\n",
      "Epoch 204/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5124 - accuracy: 0.7200\n",
      "Epoch 00204: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4867 - accuracy: 0.7604\n",
      "Epoch 205/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4847 - accuracy: 0.7800\n",
      "Epoch 00205: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4872 - accuracy: 0.7760\n",
      "Epoch 206/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5160 - accuracy: 0.7400\n",
      "Epoch 00206: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.5167 - accuracy: 0.7292\n",
      "Epoch 207/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6047 - accuracy: 0.6400\n",
      "Epoch 00207: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5060 - accuracy: 0.7500\n",
      "Epoch 208/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4511 - accuracy: 0.7800\n",
      "Epoch 00208: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.5096 - accuracy: 0.7526\n",
      "Epoch 209/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5161 - accuracy: 0.7200\n",
      "Epoch 00209: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4859 - accuracy: 0.7656\n",
      "Epoch 210/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4267 - accuracy: 0.8400\n",
      "Epoch 00210: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4922 - accuracy: 0.7669\n",
      "Epoch 211/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4766 - accuracy: 0.7800\n",
      "Epoch 00211: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4895 - accuracy: 0.7578\n",
      "Epoch 212/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5771 - accuracy: 0.7600\n",
      "Epoch 00212: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4847 - accuracy: 0.7786\n",
      "Epoch 213/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4271 - accuracy: 0.8400\n",
      "Epoch 00213: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4832 - accuracy: 0.7747\n",
      "Epoch 214/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5407 - accuracy: 0.7600\n",
      "Epoch 00214: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4852 - accuracy: 0.7773\n",
      "Epoch 215/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4181 - accuracy: 0.8600\n",
      "Epoch 00215: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4924 - accuracy: 0.7682\n",
      "Epoch 216/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4850 - accuracy: 0.8200\n",
      "Epoch 00216: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4858 - accuracy: 0.7695\n",
      "Epoch 217/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4580 - accuracy: 0.7800\n",
      "Epoch 00217: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 732us/step - loss: 0.4916 - accuracy: 0.7591\n",
      "Epoch 218/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5494 - accuracy: 0.7400\n",
      "Epoch 00218: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4882 - accuracy: 0.7708\n",
      "Epoch 219/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5099 - accuracy: 0.7600\n",
      "Epoch 00219: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4813 - accuracy: 0.7760\n",
      "Epoch 220/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4989 - accuracy: 0.7000\n",
      "Epoch 00220: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4819 - accuracy: 0.7656\n",
      "Epoch 221/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4962 - accuracy: 0.7000\n",
      "Epoch 00221: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4999 - accuracy: 0.7604\n",
      "Epoch 222/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5375 - accuracy: 0.6200\n",
      "Epoch 00222: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5080 - accuracy: 0.7591\n",
      "Epoch 223/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5762 - accuracy: 0.7200\n",
      "Epoch 00223: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4859 - accuracy: 0.7578\n",
      "Epoch 224/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4583 - accuracy: 0.8000\n",
      "Epoch 00224: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.5106 - accuracy: 0.7630\n",
      "Epoch 225/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4908 - accuracy: 0.7800\n",
      "Epoch 00225: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 664us/step - loss: 0.4925 - accuracy: 0.7591\n",
      "Epoch 226/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3862 - accuracy: 0.8200\n",
      "Epoch 00226: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4910 - accuracy: 0.7695\n",
      "Epoch 227/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4810 - accuracy: 0.7800\n",
      "Epoch 00227: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4931 - accuracy: 0.7682\n",
      "Epoch 228/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4963 - accuracy: 0.7600\n",
      "Epoch 00228: loss did not improve from 0.48132\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4915 - accuracy: 0.7565\n",
      "Epoch 229/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4687 - accuracy: 0.7400\n",
      "Epoch 00229: loss improved from 0.48132 to 0.47629, saving model to ./pima_model\\229-0.4763.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4763 - accuracy: 0.7734\n",
      "Epoch 230/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4634 - accuracy: 0.8000\n",
      "Epoch 00230: loss did not improve from 0.47629\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4787 - accuracy: 0.7826\n",
      "Epoch 231/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6356 - accuracy: 0.6800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00231: loss did not improve from 0.47629\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4765 - accuracy: 0.7786\n",
      "Epoch 232/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4851 - accuracy: 0.7000\n",
      "Epoch 00232: loss did not improve from 0.47629\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4928 - accuracy: 0.7526\n",
      "Epoch 233/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4824 - accuracy: 0.7600\n",
      "Epoch 00233: loss did not improve from 0.47629\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4845 - accuracy: 0.7786\n",
      "Epoch 234/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5133 - accuracy: 0.7200\n",
      "Epoch 00234: loss did not improve from 0.47629\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4791 - accuracy: 0.7552\n",
      "Epoch 235/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4646 - accuracy: 0.7600\n",
      "Epoch 00235: loss did not improve from 0.47629\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4855 - accuracy: 0.7708\n",
      "Epoch 236/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5093 - accuracy: 0.7200\n",
      "Epoch 00236: loss did not improve from 0.47629\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4869 - accuracy: 0.7604\n",
      "Epoch 237/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5418 - accuracy: 0.7200\n",
      "Epoch 00237: loss did not improve from 0.47629\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4973 - accuracy: 0.7474\n",
      "Epoch 238/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4590 - accuracy: 0.7800\n",
      "Epoch 00238: loss did not improve from 0.47629\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4876 - accuracy: 0.7591\n",
      "Epoch 239/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5057 - accuracy: 0.7000\n",
      "Epoch 00239: loss did not improve from 0.47629\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4814 - accuracy: 0.7682\n",
      "Epoch 240/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4988 - accuracy: 0.7200\n",
      "Epoch 00240: loss did not improve from 0.47629\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4881 - accuracy: 0.7435\n",
      "Epoch 241/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5424 - accuracy: 0.7400\n",
      "Epoch 00241: loss did not improve from 0.47629\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.5009 - accuracy: 0.7448\n",
      "Epoch 242/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4205 - accuracy: 0.7600\n",
      "Epoch 00242: loss did not improve from 0.47629\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4808 - accuracy: 0.7799\n",
      "Epoch 243/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4889 - accuracy: 0.8000\n",
      "Epoch 00243: loss improved from 0.47629 to 0.47260, saving model to ./pima_model\\243-0.4726.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4726 - accuracy: 0.7773\n",
      "Epoch 244/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4706 - accuracy: 0.7400\n",
      "Epoch 00244: loss did not improve from 0.47260\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4835 - accuracy: 0.7708\n",
      "Epoch 245/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4069 - accuracy: 0.8200\n",
      "Epoch 00245: loss did not improve from 0.47260\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4799 - accuracy: 0.7734\n",
      "Epoch 246/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5098 - accuracy: 0.8000\n",
      "Epoch 00246: loss did not improve from 0.47260\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4741 - accuracy: 0.7708\n",
      "Epoch 247/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5044 - accuracy: 0.7600\n",
      "Epoch 00247: loss did not improve from 0.47260\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4890 - accuracy: 0.7682\n",
      "Epoch 248/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4643 - accuracy: 0.8400\n",
      "Epoch 00248: loss did not improve from 0.47260\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4879 - accuracy: 0.7695\n",
      "Epoch 249/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4280 - accuracy: 0.7800\n",
      "Epoch 00249: loss did not improve from 0.47260\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4763 - accuracy: 0.7695\n",
      "Epoch 250/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5413 - accuracy: 0.7200\n",
      "Epoch 00250: loss did not improve from 0.47260\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4854 - accuracy: 0.7643\n",
      "Epoch 251/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5731 - accuracy: 0.7400\n",
      "Epoch 00251: loss did not improve from 0.47260\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.4739 - accuracy: 0.7773\n",
      "Epoch 252/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4721 - accuracy: 0.7800\n",
      "Epoch 00252: loss did not improve from 0.47260\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4983 - accuracy: 0.7617\n",
      "Epoch 253/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5182 - accuracy: 0.7600\n",
      "Epoch 00253: loss did not improve from 0.47260\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4809 - accuracy: 0.7721\n",
      "Epoch 254/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5240 - accuracy: 0.7000\n",
      "Epoch 00254: loss did not improve from 0.47260\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4754 - accuracy: 0.7669\n",
      "Epoch 255/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3996 - accuracy: 0.8400\n",
      "Epoch 00255: loss did not improve from 0.47260\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4765 - accuracy: 0.7604\n",
      "Epoch 256/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3984 - accuracy: 0.8200\n",
      "Epoch 00256: loss did not improve from 0.47260\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4788 - accuracy: 0.7591\n",
      "Epoch 257/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4804 - accuracy: 0.7600\n",
      "Epoch 00257: loss did not improve from 0.47260\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4731 - accuracy: 0.7760\n",
      "Epoch 258/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5807 - accuracy: 0.7600\n",
      "Epoch 00258: loss improved from 0.47260 to 0.47121, saving model to ./pima_model\\258-0.4712.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4712 - accuracy: 0.7852\n",
      "Epoch 259/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5075 - accuracy: 0.7400\n",
      "Epoch 00259: loss did not improve from 0.47121\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4771 - accuracy: 0.7643\n",
      "Epoch 260/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4975 - accuracy: 0.7400\n",
      "Epoch 00260: loss did not improve from 0.47121\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4808 - accuracy: 0.7643\n",
      "Epoch 261/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5633 - accuracy: 0.6600\n",
      "Epoch 00261: loss improved from 0.47121 to 0.46703, saving model to ./pima_model\\261-0.4670.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4670 - accuracy: 0.7826\n",
      "Epoch 262/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5153 - accuracy: 0.6800\n",
      "Epoch 00262: loss did not improve from 0.46703\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4820 - accuracy: 0.7708\n",
      "Epoch 263/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5026 - accuracy: 0.7400\n",
      "Epoch 00263: loss did not improve from 0.46703\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4760 - accuracy: 0.7643\n",
      "Epoch 264/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4491 - accuracy: 0.8000\n",
      "Epoch 00264: loss improved from 0.46703 to 0.46677, saving model to ./pima_model\\264-0.4668.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4668 - accuracy: 0.7891\n",
      "Epoch 265/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3980 - accuracy: 0.8200\n",
      "Epoch 00265: loss did not improve from 0.46677\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4729 - accuracy: 0.7852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 266/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5049 - accuracy: 0.7400\n",
      "Epoch 00266: loss did not improve from 0.46677\n",
      "16/16 [==============================] - 0s 866us/step - loss: 0.4692 - accuracy: 0.7799\n",
      "Epoch 267/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4282 - accuracy: 0.7600\n",
      "Epoch 00267: loss did not improve from 0.46677\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4883 - accuracy: 0.7708\n",
      "Epoch 268/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4736 - accuracy: 0.7800\n",
      "Epoch 00268: loss did not improve from 0.46677\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4727 - accuracy: 0.7891\n",
      "Epoch 269/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3874 - accuracy: 0.8000\n",
      "Epoch 00269: loss did not improve from 0.46677\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4688 - accuracy: 0.7852\n",
      "Epoch 270/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4490 - accuracy: 0.8200\n",
      "Epoch 00270: loss improved from 0.46677 to 0.46656, saving model to ./pima_model\\270-0.4666.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4666 - accuracy: 0.7852\n",
      "Epoch 271/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4550 - accuracy: 0.8200\n",
      "Epoch 00271: loss improved from 0.46656 to 0.46383, saving model to ./pima_model\\271-0.4638.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4638 - accuracy: 0.7943\n",
      "Epoch 272/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4837 - accuracy: 0.7400\n",
      "Epoch 00272: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4773 - accuracy: 0.7721\n",
      "Epoch 273/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4859 - accuracy: 0.7800\n",
      "Epoch 00273: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4707 - accuracy: 0.7695\n",
      "Epoch 274/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4697 - accuracy: 0.7800\n",
      "Epoch 00274: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4708 - accuracy: 0.7695\n",
      "Epoch 275/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4189 - accuracy: 0.8400\n",
      "Epoch 00275: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4690 - accuracy: 0.7799\n",
      "Epoch 276/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4588 - accuracy: 0.8000\n",
      "Epoch 00276: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4672 - accuracy: 0.7865\n",
      "Epoch 277/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4317 - accuracy: 0.8200\n",
      "Epoch 00277: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.4690 - accuracy: 0.7708\n",
      "Epoch 278/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4287 - accuracy: 0.8600\n",
      "Epoch 00278: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4703 - accuracy: 0.7734\n",
      "Epoch 279/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4407 - accuracy: 0.8000\n",
      "Epoch 00279: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4763 - accuracy: 0.7656\n",
      "Epoch 280/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4390 - accuracy: 0.8000\n",
      "Epoch 00280: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4712 - accuracy: 0.7773\n",
      "Epoch 281/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4954 - accuracy: 0.7000\n",
      "Epoch 00281: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4712 - accuracy: 0.7878\n",
      "Epoch 282/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4009 - accuracy: 0.8600\n",
      "Epoch 00282: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4697 - accuracy: 0.7708\n",
      "Epoch 283/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4288 - accuracy: 0.8000\n",
      "Epoch 00283: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4712 - accuracy: 0.7734\n",
      "Epoch 284/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4095 - accuracy: 0.8600\n",
      "Epoch 00284: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4657 - accuracy: 0.7786\n",
      "Epoch 285/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5358 - accuracy: 0.7600\n",
      "Epoch 00285: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4671 - accuracy: 0.7930\n",
      "Epoch 286/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5233 - accuracy: 0.7800\n",
      "Epoch 00286: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4697 - accuracy: 0.7760\n",
      "Epoch 287/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4879 - accuracy: 0.7600\n",
      "Epoch 00287: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4747 - accuracy: 0.7747\n",
      "Epoch 288/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4723 - accuracy: 0.8200\n",
      "Epoch 00288: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4645 - accuracy: 0.7878\n",
      "Epoch 289/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4161 - accuracy: 0.8400\n",
      "Epoch 00289: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4761 - accuracy: 0.7617\n",
      "Epoch 290/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2906 - accuracy: 0.9200\n",
      "Epoch 00290: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 664us/step - loss: 0.4744 - accuracy: 0.7786\n",
      "Epoch 291/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4160 - accuracy: 0.8000\n",
      "Epoch 00291: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4658 - accuracy: 0.7852\n",
      "Epoch 292/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3838 - accuracy: 0.8200\n",
      "Epoch 00292: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4650 - accuracy: 0.7826\n",
      "Epoch 293/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4365 - accuracy: 0.7600\n",
      "Epoch 00293: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4674 - accuracy: 0.7969\n",
      "Epoch 294/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4974 - accuracy: 0.8200\n",
      "Epoch 00294: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4665 - accuracy: 0.7826\n",
      "Epoch 295/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4465 - accuracy: 0.8000\n",
      "Epoch 00295: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4760 - accuracy: 0.7669\n",
      "Epoch 296/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5243 - accuracy: 0.7600\n",
      "Epoch 00296: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 799us/step - loss: 0.4706 - accuracy: 0.7760\n",
      "Epoch 297/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6310 - accuracy: 0.7000\n",
      "Epoch 00297: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4688 - accuracy: 0.7773\n",
      "Epoch 298/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5228 - accuracy: 0.7200\n",
      "Epoch 00298: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.4646 - accuracy: 0.7826\n",
      "Epoch 299/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4248 - accuracy: 0.8200\n",
      "Epoch 00299: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4718 - accuracy: 0.7695\n",
      "Epoch 300/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3997 - accuracy: 0.8400\n",
      "Epoch 00300: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4650 - accuracy: 0.7786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5974 - accuracy: 0.6800\n",
      "Epoch 00301: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4667 - accuracy: 0.7812\n",
      "Epoch 302/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4243 - accuracy: 0.8200\n",
      "Epoch 00302: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4651 - accuracy: 0.7891\n",
      "Epoch 303/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4008 - accuracy: 0.8600\n",
      "Epoch 00303: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4639 - accuracy: 0.7839\n",
      "Epoch 304/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3657 - accuracy: 0.9000\n",
      "Epoch 00304: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4653 - accuracy: 0.7891\n",
      "Epoch 305/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4239 - accuracy: 0.8400\n",
      "Epoch 00305: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4697 - accuracy: 0.7773\n",
      "Epoch 306/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4257 - accuracy: 0.8000\n",
      "Epoch 00306: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4845 - accuracy: 0.7695\n",
      "Epoch 307/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4059 - accuracy: 0.8200\n",
      "Epoch 00307: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4657 - accuracy: 0.7812\n",
      "Epoch 308/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4718 - accuracy: 0.7800\n",
      "Epoch 00308: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4723 - accuracy: 0.7656\n",
      "Epoch 309/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4467 - accuracy: 0.8200\n",
      "Epoch 00309: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4685 - accuracy: 0.7904\n",
      "Epoch 310/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3563 - accuracy: 0.8800\n",
      "Epoch 00310: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.4698 - accuracy: 0.7643\n",
      "Epoch 311/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5755 - accuracy: 0.7000\n",
      "Epoch 00311: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.4713 - accuracy: 0.7852\n",
      "Epoch 312/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3483 - accuracy: 0.8600\n",
      "Epoch 00312: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4810 - accuracy: 0.7552\n",
      "Epoch 313/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3574 - accuracy: 0.8200\n",
      "Epoch 00313: loss did not improve from 0.46383\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4680 - accuracy: 0.7812\n",
      "Epoch 314/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4373 - accuracy: 0.7800\n",
      "Epoch 00314: loss improved from 0.46383 to 0.46369, saving model to ./pima_model\\314-0.4637.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4637 - accuracy: 0.7812\n",
      "Epoch 315/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4485 - accuracy: 0.8200\n",
      "Epoch 00315: loss did not improve from 0.46369\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4640 - accuracy: 0.7930\n",
      "Epoch 316/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5077 - accuracy: 0.7000\n",
      "Epoch 00316: loss did not improve from 0.46369\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4684 - accuracy: 0.7669\n",
      "Epoch 317/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4713 - accuracy: 0.7600\n",
      "Epoch 00317: loss did not improve from 0.46369\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4659 - accuracy: 0.7839\n",
      "Epoch 318/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4194 - accuracy: 0.8000\n",
      "Epoch 00318: loss did not improve from 0.46369\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4874 - accuracy: 0.7695\n",
      "Epoch 319/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4912 - accuracy: 0.8000\n",
      "Epoch 00319: loss did not improve from 0.46369\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4992 - accuracy: 0.7565\n",
      "Epoch 320/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5057 - accuracy: 0.7800\n",
      "Epoch 00320: loss improved from 0.46369 to 0.46077, saving model to ./pima_model\\320-0.4608.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4608 - accuracy: 0.7786\n",
      "Epoch 321/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4862 - accuracy: 0.7800\n",
      "Epoch 00321: loss improved from 0.46077 to 0.45779, saving model to ./pima_model\\321-0.4578.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4578 - accuracy: 0.7852\n",
      "Epoch 322/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3719 - accuracy: 0.8400\n",
      "Epoch 00322: loss improved from 0.45779 to 0.45777, saving model to ./pima_model\\322-0.4578.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4578 - accuracy: 0.7799\n",
      "Epoch 323/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4568 - accuracy: 0.7800\n",
      "Epoch 00323: loss did not improve from 0.45777\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4624 - accuracy: 0.7826\n",
      "Epoch 324/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5657 - accuracy: 0.7400\n",
      "Epoch 00324: loss did not improve from 0.45777\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4684 - accuracy: 0.7826\n",
      "Epoch 325/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3487 - accuracy: 0.8400\n",
      "Epoch 00325: loss did not improve from 0.45777\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4904 - accuracy: 0.7643\n",
      "Epoch 326/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5112 - accuracy: 0.7600\n",
      "Epoch 00326: loss did not improve from 0.45777\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4707 - accuracy: 0.7747\n",
      "Epoch 327/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4538 - accuracy: 0.8000\n",
      "Epoch 00327: loss did not improve from 0.45777\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4640 - accuracy: 0.7826\n",
      "Epoch 328/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6567 - accuracy: 0.6800\n",
      "Epoch 00328: loss did not improve from 0.45777\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4656 - accuracy: 0.7747\n",
      "Epoch 329/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4473 - accuracy: 0.7800\n",
      "Epoch 00329: loss did not improve from 0.45777\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4876 - accuracy: 0.7656\n",
      "Epoch 330/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4923 - accuracy: 0.7400\n",
      "Epoch 00330: loss improved from 0.45777 to 0.45745, saving model to ./pima_model\\330-0.4575.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4575 - accuracy: 0.7865\n",
      "Epoch 331/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4512 - accuracy: 0.7600\n",
      "Epoch 00331: loss did not improve from 0.45745\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4594 - accuracy: 0.7852\n",
      "Epoch 332/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3796 - accuracy: 0.8600\n",
      "Epoch 00332: loss did not improve from 0.45745\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4593 - accuracy: 0.7812\n",
      "Epoch 333/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5279 - accuracy: 0.7600\n",
      "Epoch 00333: loss did not improve from 0.45745\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4737 - accuracy: 0.7695\n",
      "Epoch 334/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3901 - accuracy: 0.8400\n",
      "Epoch 00334: loss did not improve from 0.45745\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4645 - accuracy: 0.7799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 335/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4393 - accuracy: 0.8200\n",
      "Epoch 00335: loss did not improve from 0.45745\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4661 - accuracy: 0.7826\n",
      "Epoch 336/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5311 - accuracy: 0.7600\n",
      "Epoch 00336: loss did not improve from 0.45745\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4756 - accuracy: 0.7747\n",
      "Epoch 337/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5241 - accuracy: 0.7000\n",
      "Epoch 00337: loss did not improve from 0.45745\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4688 - accuracy: 0.7643\n",
      "Epoch 338/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5313 - accuracy: 0.7200\n",
      "Epoch 00338: loss did not improve from 0.45745\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4651 - accuracy: 0.7669\n",
      "Epoch 339/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5375 - accuracy: 0.6800\n",
      "Epoch 00339: loss did not improve from 0.45745\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4632 - accuracy: 0.7891\n",
      "Epoch 340/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4896 - accuracy: 0.7800\n",
      "Epoch 00340: loss did not improve from 0.45745\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4639 - accuracy: 0.7773\n",
      "Epoch 341/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4428 - accuracy: 0.8200\n",
      "Epoch 00341: loss did not improve from 0.45745\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4586 - accuracy: 0.7839\n",
      "Epoch 342/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4834 - accuracy: 0.7600\n",
      "Epoch 00342: loss did not improve from 0.45745\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4591 - accuracy: 0.7839\n",
      "Epoch 343/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5672 - accuracy: 0.6800\n",
      "Epoch 00343: loss did not improve from 0.45745\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4665 - accuracy: 0.7865\n",
      "Epoch 344/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4557 - accuracy: 0.8200\n",
      "Epoch 00344: loss did not improve from 0.45745\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4685 - accuracy: 0.7799\n",
      "Epoch 345/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4275 - accuracy: 0.8000\n",
      "Epoch 00345: loss did not improve from 0.45745\n",
      "16/16 [==============================] - 0s 740us/step - loss: 0.4600 - accuracy: 0.7852\n",
      "Epoch 346/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3651 - accuracy: 0.8400\n",
      "Epoch 00346: loss improved from 0.45745 to 0.45616, saving model to ./pima_model\\346-0.4562.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4562 - accuracy: 0.7878\n",
      "Epoch 347/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4159 - accuracy: 0.8200\n",
      "Epoch 00347: loss did not improve from 0.45616\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4584 - accuracy: 0.7812\n",
      "Epoch 348/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4069 - accuracy: 0.8600\n",
      "Epoch 00348: loss improved from 0.45616 to 0.45515, saving model to ./pima_model\\348-0.4551.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4551 - accuracy: 0.7826\n",
      "Epoch 349/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4326 - accuracy: 0.8400\n",
      "Epoch 00349: loss did not improve from 0.45515\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4570 - accuracy: 0.7891\n",
      "Epoch 350/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4061 - accuracy: 0.8800\n",
      "Epoch 00350: loss did not improve from 0.45515\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4710 - accuracy: 0.7708\n",
      "Epoch 351/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3874 - accuracy: 0.8400\n",
      "Epoch 00351: loss did not improve from 0.45515\n",
      "16/16 [==============================] - 0s 732us/step - loss: 0.4751 - accuracy: 0.7643\n",
      "Epoch 352/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5404 - accuracy: 0.6600\n",
      "Epoch 00352: loss did not improve from 0.45515\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4750 - accuracy: 0.7695\n",
      "Epoch 353/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5679 - accuracy: 0.6800\n",
      "Epoch 00353: loss did not improve from 0.45515\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4701 - accuracy: 0.7695\n",
      "Epoch 354/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4429 - accuracy: 0.8000\n",
      "Epoch 00354: loss improved from 0.45515 to 0.44986, saving model to ./pima_model\\354-0.4499.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4499 - accuracy: 0.7812\n",
      "Epoch 355/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4047 - accuracy: 0.8800\n",
      "Epoch 00355: loss did not improve from 0.44986\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4578 - accuracy: 0.7865\n",
      "Epoch 356/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4549 - accuracy: 0.8000\n",
      "Epoch 00356: loss did not improve from 0.44986\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4842 - accuracy: 0.7578\n",
      "Epoch 357/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5185 - accuracy: 0.7600\n",
      "Epoch 00357: loss did not improve from 0.44986\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4621 - accuracy: 0.7878\n",
      "Epoch 358/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4361 - accuracy: 0.8400\n",
      "Epoch 00358: loss did not improve from 0.44986\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4600 - accuracy: 0.7904\n",
      "Epoch 359/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4662 - accuracy: 0.7600\n",
      "Epoch 00359: loss did not improve from 0.44986\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4525 - accuracy: 0.7982\n",
      "Epoch 360/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3960 - accuracy: 0.8400\n",
      "Epoch 00360: loss did not improve from 0.44986\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4536 - accuracy: 0.7891\n",
      "Epoch 361/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5589 - accuracy: 0.6000\n",
      "Epoch 00361: loss did not improve from 0.44986\n",
      "16/16 [==============================] - 0s 599us/step - loss: 0.4595 - accuracy: 0.7669\n",
      "Epoch 362/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4578 - accuracy: 0.7800\n",
      "Epoch 00362: loss did not improve from 0.44986\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4539 - accuracy: 0.7930\n",
      "Epoch 363/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5234 - accuracy: 0.7400\n",
      "Epoch 00363: loss did not improve from 0.44986\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4540 - accuracy: 0.7904\n",
      "Epoch 364/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6000 - accuracy: 0.6600\n",
      "Epoch 00364: loss did not improve from 0.44986\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4737 - accuracy: 0.7773\n",
      "Epoch 365/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5457 - accuracy: 0.7200\n",
      "Epoch 00365: loss did not improve from 0.44986\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4710 - accuracy: 0.7721\n",
      "Epoch 366/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3979 - accuracy: 0.8400\n",
      "Epoch 00366: loss did not improve from 0.44986\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4653 - accuracy: 0.7826\n",
      "Epoch 367/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4300 - accuracy: 0.7800\n",
      "Epoch 00367: loss did not improve from 0.44986\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4634 - accuracy: 0.7839\n",
      "Epoch 368/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4896 - accuracy: 0.8200\n",
      "Epoch 00368: loss did not improve from 0.44986\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4933 - accuracy: 0.7786\n",
      "Epoch 369/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4426 - accuracy: 0.7400\n",
      "Epoch 00369: loss did not improve from 0.44986\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4572 - accuracy: 0.7721\n",
      "Epoch 370/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4186 - accuracy: 0.7200\n",
      "Epoch 00370: loss improved from 0.44986 to 0.44775, saving model to ./pima_model\\370-0.4477.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4477 - accuracy: 0.7852\n",
      "Epoch 371/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3449 - accuracy: 0.8800\n",
      "Epoch 00371: loss did not improve from 0.44775\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4564 - accuracy: 0.7826\n",
      "Epoch 372/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5417 - accuracy: 0.7400\n",
      "Epoch 00372: loss did not improve from 0.44775\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4608 - accuracy: 0.7747\n",
      "Epoch 373/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5120 - accuracy: 0.7200\n",
      "Epoch 00373: loss did not improve from 0.44775\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4645 - accuracy: 0.7786\n",
      "Epoch 374/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3832 - accuracy: 0.8200\n",
      "Epoch 00374: loss did not improve from 0.44775\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4626 - accuracy: 0.7799\n",
      "Epoch 375/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5219 - accuracy: 0.7000\n",
      "Epoch 00375: loss did not improve from 0.44775\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4561 - accuracy: 0.7812\n",
      "Epoch 376/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3886 - accuracy: 0.8400\n",
      "Epoch 00376: loss did not improve from 0.44775\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4480 - accuracy: 0.7930\n",
      "Epoch 377/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4176 - accuracy: 0.8200\n",
      "Epoch 00377: loss did not improve from 0.44775\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4650 - accuracy: 0.7773\n",
      "Epoch 378/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4387 - accuracy: 0.7800\n",
      "Epoch 00378: loss did not improve from 0.44775\n",
      "16/16 [==============================] - 0s 666us/step - loss: 0.4617 - accuracy: 0.7734\n",
      "Epoch 379/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4506 - accuracy: 0.7800\n",
      "Epoch 00379: loss did not improve from 0.44775\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4558 - accuracy: 0.7839\n",
      "Epoch 380/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5215 - accuracy: 0.7600\n",
      "Epoch 00380: loss did not improve from 0.44775\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4526 - accuracy: 0.7865\n",
      "Epoch 381/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5799 - accuracy: 0.6600\n",
      "Epoch 00381: loss did not improve from 0.44775\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4573 - accuracy: 0.7760\n",
      "Epoch 382/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5473 - accuracy: 0.7600\n",
      "Epoch 00382: loss did not improve from 0.44775\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4625 - accuracy: 0.7839\n",
      "Epoch 383/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5444 - accuracy: 0.7600\n",
      "Epoch 00383: loss did not improve from 0.44775\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4513 - accuracy: 0.7812\n",
      "Epoch 384/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3966 - accuracy: 0.8400\n",
      "Epoch 00384: loss did not improve from 0.44775\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4486 - accuracy: 0.7865\n",
      "Epoch 385/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4867 - accuracy: 0.7800\n",
      "Epoch 00385: loss did not improve from 0.44775\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4503 - accuracy: 0.7904\n",
      "Epoch 386/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5703 - accuracy: 0.7400\n",
      "Epoch 00386: loss improved from 0.44775 to 0.44574, saving model to ./pima_model\\386-0.4457.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4457 - accuracy: 0.8021\n",
      "Epoch 387/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4398 - accuracy: 0.8200\n",
      "Epoch 00387: loss did not improve from 0.44574\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4532 - accuracy: 0.7865\n",
      "Epoch 388/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4635 - accuracy: 0.8000\n",
      "Epoch 00388: loss did not improve from 0.44574\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4597 - accuracy: 0.7786\n",
      "Epoch 389/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3689 - accuracy: 0.9000\n",
      "Epoch 00389: loss did not improve from 0.44574\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4697 - accuracy: 0.7721\n",
      "Epoch 390/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4578 - accuracy: 0.8000\n",
      "Epoch 00390: loss did not improve from 0.44574\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4647 - accuracy: 0.7799\n",
      "Epoch 391/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5203 - accuracy: 0.8000\n",
      "Epoch 00391: loss did not improve from 0.44574\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4543 - accuracy: 0.7826\n",
      "Epoch 392/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3729 - accuracy: 0.8400\n",
      "Epoch 00392: loss did not improve from 0.44574\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4564 - accuracy: 0.7773\n",
      "Epoch 393/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4655 - accuracy: 0.7400\n",
      "Epoch 00393: loss did not improve from 0.44574\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4612 - accuracy: 0.7878\n",
      "Epoch 394/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4590 - accuracy: 0.7600\n",
      "Epoch 00394: loss did not improve from 0.44574\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4649 - accuracy: 0.7760\n",
      "Epoch 395/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4413 - accuracy: 0.7600\n",
      "Epoch 00395: loss did not improve from 0.44574\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4531 - accuracy: 0.8021\n",
      "Epoch 396/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5271 - accuracy: 0.6200\n",
      "Epoch 00396: loss did not improve from 0.44574\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4586 - accuracy: 0.7786\n",
      "Epoch 397/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4198 - accuracy: 0.8000\n",
      "Epoch 00397: loss did not improve from 0.44574\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4468 - accuracy: 0.7969\n",
      "Epoch 398/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5320 - accuracy: 0.7600\n",
      "Epoch 00398: loss improved from 0.44574 to 0.44466, saving model to ./pima_model\\398-0.4447.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4447 - accuracy: 0.8021\n",
      "Epoch 399/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4594 - accuracy: 0.8200\n",
      "Epoch 00399: loss did not improve from 0.44466\n",
      "16/16 [==============================] - 0s 664us/step - loss: 0.4479 - accuracy: 0.8008\n",
      "Epoch 400/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3845 - accuracy: 0.8200\n",
      "Epoch 00400: loss did not improve from 0.44466\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4449 - accuracy: 0.7930\n",
      "Epoch 401/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3981 - accuracy: 0.8800\n",
      "Epoch 00401: loss did not improve from 0.44466\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4470 - accuracy: 0.7930\n",
      "Epoch 402/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3947 - accuracy: 0.8400\n",
      "Epoch 00402: loss improved from 0.44466 to 0.44356, saving model to ./pima_model\\402-0.4436.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4436 - accuracy: 0.7930\n",
      "Epoch 403/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4935 - accuracy: 0.7600\n",
      "Epoch 00403: loss did not improve from 0.44356\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4547 - accuracy: 0.7695\n",
      "Epoch 404/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5523 - accuracy: 0.7200\n",
      "Epoch 00404: loss did not improve from 0.44356\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4471 - accuracy: 0.7852\n",
      "Epoch 405/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3839 - accuracy: 0.8400\n",
      "Epoch 00405: loss did not improve from 0.44356\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4557 - accuracy: 0.7682\n",
      "Epoch 406/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4281 - accuracy: 0.8000\n",
      "Epoch 00406: loss did not improve from 0.44356\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4608 - accuracy: 0.7734\n",
      "Epoch 407/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4568 - accuracy: 0.8400\n",
      "Epoch 00407: loss did not improve from 0.44356\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.4554 - accuracy: 0.7839\n",
      "Epoch 408/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4784 - accuracy: 0.7800\n",
      "Epoch 00408: loss did not improve from 0.44356\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4619 - accuracy: 0.7734\n",
      "Epoch 409/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5208 - accuracy: 0.7400\n",
      "Epoch 00409: loss did not improve from 0.44356\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4655 - accuracy: 0.7773\n",
      "Epoch 410/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4550 - accuracy: 0.8000\n",
      "Epoch 00410: loss did not improve from 0.44356\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.4568 - accuracy: 0.7812\n",
      "Epoch 411/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4618 - accuracy: 0.7600\n",
      "Epoch 00411: loss did not improve from 0.44356\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4680 - accuracy: 0.7721\n",
      "Epoch 412/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4786 - accuracy: 0.7600\n",
      "Epoch 00412: loss did not improve from 0.44356\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4725 - accuracy: 0.7760\n",
      "Epoch 413/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3216 - accuracy: 0.8800\n",
      "Epoch 00413: loss did not improve from 0.44356\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4565 - accuracy: 0.7904\n",
      "Epoch 414/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5102 - accuracy: 0.7400\n",
      "Epoch 00414: loss did not improve from 0.44356\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4460 - accuracy: 0.7917\n",
      "Epoch 415/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5075 - accuracy: 0.7600\n",
      "Epoch 00415: loss did not improve from 0.44356\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4461 - accuracy: 0.7865\n",
      "Epoch 416/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4325 - accuracy: 0.7600\n",
      "Epoch 00416: loss did not improve from 0.44356\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4539 - accuracy: 0.7656\n",
      "Epoch 417/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3804 - accuracy: 0.8600\n",
      "Epoch 00417: loss did not improve from 0.44356\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4440 - accuracy: 0.7891\n",
      "Epoch 418/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4783 - accuracy: 0.7600\n",
      "Epoch 00418: loss did not improve from 0.44356\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4469 - accuracy: 0.7930\n",
      "Epoch 419/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3636 - accuracy: 0.8600\n",
      "Epoch 00419: loss did not improve from 0.44356\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4468 - accuracy: 0.8021\n",
      "Epoch 420/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3398 - accuracy: 0.8400\n",
      "Epoch 00420: loss did not improve from 0.44356\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4477 - accuracy: 0.7878\n",
      "Epoch 421/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4033 - accuracy: 0.8400\n",
      "Epoch 00421: loss did not improve from 0.44356\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4500 - accuracy: 0.7878\n",
      "Epoch 422/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5154 - accuracy: 0.7800\n",
      "Epoch 00422: loss improved from 0.44356 to 0.44321, saving model to ./pima_model\\422-0.4432.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4432 - accuracy: 0.7891\n",
      "Epoch 423/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6150 - accuracy: 0.6600\n",
      "Epoch 00423: loss did not improve from 0.44321\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4475 - accuracy: 0.7852\n",
      "Epoch 424/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3827 - accuracy: 0.8400\n",
      "Epoch 00424: loss did not improve from 0.44321\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4532 - accuracy: 0.7878\n",
      "Epoch 425/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4398 - accuracy: 0.8200\n",
      "Epoch 00425: loss did not improve from 0.44321\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4502 - accuracy: 0.7917\n",
      "Epoch 426/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4631 - accuracy: 0.7400\n",
      "Epoch 00426: loss did not improve from 0.44321\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4446 - accuracy: 0.7839\n",
      "Epoch 427/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5007 - accuracy: 0.7600\n",
      "Epoch 00427: loss improved from 0.44321 to 0.44207, saving model to ./pima_model\\427-0.4421.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4421 - accuracy: 0.7904\n",
      "Epoch 428/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4027 - accuracy: 0.8400\n",
      "Epoch 00428: loss did not improve from 0.44207\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4546 - accuracy: 0.7839\n",
      "Epoch 429/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3387 - accuracy: 0.9000\n",
      "Epoch 00429: loss did not improve from 0.44207\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4520 - accuracy: 0.7930\n",
      "Epoch 430/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4450 - accuracy: 0.8200\n",
      "Epoch 00430: loss did not improve from 0.44207\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.4567 - accuracy: 0.7826\n",
      "Epoch 431/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4515 - accuracy: 0.8200\n",
      "Epoch 00431: loss did not improve from 0.44207\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4546 - accuracy: 0.7786\n",
      "Epoch 432/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4896 - accuracy: 0.7400\n",
      "Epoch 00432: loss did not improve from 0.44207\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4506 - accuracy: 0.7839\n",
      "Epoch 433/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5549 - accuracy: 0.6400\n",
      "Epoch 00433: loss did not improve from 0.44207\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4471 - accuracy: 0.7760\n",
      "Epoch 434/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5092 - accuracy: 0.7200\n",
      "Epoch 00434: loss did not improve from 0.44207\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4476 - accuracy: 0.7839\n",
      "Epoch 435/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5099 - accuracy: 0.7600\n",
      "Epoch 00435: loss did not improve from 0.44207\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4505 - accuracy: 0.7865\n",
      "Epoch 436/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3415 - accuracy: 0.9000\n",
      "Epoch 00436: loss did not improve from 0.44207\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4475 - accuracy: 0.7826\n",
      "Epoch 437/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3446 - accuracy: 0.8400\n",
      "Epoch 00437: loss did not improve from 0.44207\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4521 - accuracy: 0.7747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 438/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4304 - accuracy: 0.8200\n",
      "Epoch 00438: loss improved from 0.44207 to 0.44060, saving model to ./pima_model\\438-0.4406.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4406 - accuracy: 0.7878\n",
      "Epoch 439/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3377 - accuracy: 0.8600\n",
      "Epoch 00439: loss did not improve from 0.44060\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4523 - accuracy: 0.7799\n",
      "Epoch 440/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5971 - accuracy: 0.7200\n",
      "Epoch 00440: loss did not improve from 0.44060\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4556 - accuracy: 0.7773\n",
      "Epoch 441/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4826 - accuracy: 0.7400\n",
      "Epoch 00441: loss did not improve from 0.44060\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4767 - accuracy: 0.7617\n",
      "Epoch 442/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4845 - accuracy: 0.7800\n",
      "Epoch 00442: loss did not improve from 0.44060\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4511 - accuracy: 0.7826\n",
      "Epoch 443/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4243 - accuracy: 0.7400\n",
      "Epoch 00443: loss did not improve from 0.44060\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4451 - accuracy: 0.7865\n",
      "Epoch 444/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4769 - accuracy: 0.7200\n",
      "Epoch 00444: loss did not improve from 0.44060\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4468 - accuracy: 0.7747\n",
      "Epoch 445/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4486 - accuracy: 0.7600\n",
      "Epoch 00445: loss did not improve from 0.44060\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4527 - accuracy: 0.7852\n",
      "Epoch 446/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4607 - accuracy: 0.7800\n",
      "Epoch 00446: loss did not improve from 0.44060\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4409 - accuracy: 0.7904\n",
      "Epoch 447/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4288 - accuracy: 0.8000\n",
      "Epoch 00447: loss improved from 0.44060 to 0.43778, saving model to ./pima_model\\447-0.4378.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4378 - accuracy: 0.7969\n",
      "Epoch 448/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3414 - accuracy: 0.8600\n",
      "Epoch 00448: loss did not improve from 0.43778\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4390 - accuracy: 0.7943\n",
      "Epoch 449/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4538 - accuracy: 0.7800\n",
      "Epoch 00449: loss did not improve from 0.43778\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4531 - accuracy: 0.7878\n",
      "Epoch 450/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4640 - accuracy: 0.8200\n",
      "Epoch 00450: loss did not improve from 0.43778\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4421 - accuracy: 0.7878\n",
      "Epoch 451/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4737 - accuracy: 0.8200\n",
      "Epoch 00451: loss did not improve from 0.43778\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4396 - accuracy: 0.7930\n",
      "Epoch 452/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4081 - accuracy: 0.8200\n",
      "Epoch 00452: loss did not improve from 0.43778\n",
      "16/16 [==============================] - 0s 797us/step - loss: 0.4464 - accuracy: 0.7865\n",
      "Epoch 453/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4484 - accuracy: 0.7800\n",
      "Epoch 00453: loss did not improve from 0.43778\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4512 - accuracy: 0.7760\n",
      "Epoch 454/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3755 - accuracy: 0.8000\n",
      "Epoch 00454: loss did not improve from 0.43778\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4404 - accuracy: 0.7839\n",
      "Epoch 455/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3557 - accuracy: 0.9000\n",
      "Epoch 00455: loss did not improve from 0.43778\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4451 - accuracy: 0.7943\n",
      "Epoch 456/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3695 - accuracy: 0.8800\n",
      "Epoch 00456: loss did not improve from 0.43778\n",
      "16/16 [==============================] - 0s 797us/step - loss: 0.4491 - accuracy: 0.8021\n",
      "Epoch 457/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3472 - accuracy: 0.8200\n",
      "Epoch 00457: loss did not improve from 0.43778\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4495 - accuracy: 0.7878\n",
      "Epoch 458/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4067 - accuracy: 0.8200\n",
      "Epoch 00458: loss improved from 0.43778 to 0.43601, saving model to ./pima_model\\458-0.4360.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4360 - accuracy: 0.7904\n",
      "Epoch 459/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3155 - accuracy: 0.8600\n",
      "Epoch 00459: loss did not improve from 0.43601\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4393 - accuracy: 0.7995\n",
      "Epoch 460/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4071 - accuracy: 0.8400\n",
      "Epoch 00460: loss did not improve from 0.43601\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4595 - accuracy: 0.7734\n",
      "Epoch 461/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4824 - accuracy: 0.7600\n",
      "Epoch 00461: loss did not improve from 0.43601\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4813 - accuracy: 0.7565\n",
      "Epoch 462/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3351 - accuracy: 0.8200\n",
      "Epoch 00462: loss did not improve from 0.43601\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4499 - accuracy: 0.7917\n",
      "Epoch 463/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4249 - accuracy: 0.7800\n",
      "Epoch 00463: loss did not improve from 0.43601\n",
      "16/16 [==============================] - 0s 797us/step - loss: 0.4457 - accuracy: 0.7786\n",
      "Epoch 464/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5323 - accuracy: 0.7400\n",
      "Epoch 00464: loss did not improve from 0.43601\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4588 - accuracy: 0.7786\n",
      "Epoch 465/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4633 - accuracy: 0.7800\n",
      "Epoch 00465: loss did not improve from 0.43601\n",
      "16/16 [==============================] - 0s 797us/step - loss: 0.4465 - accuracy: 0.7891\n",
      "Epoch 466/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4744 - accuracy: 0.7600\n",
      "Epoch 00466: loss did not improve from 0.43601\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4422 - accuracy: 0.7917\n",
      "Epoch 467/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4642 - accuracy: 0.8200\n",
      "Epoch 00467: loss did not improve from 0.43601\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4390 - accuracy: 0.7969\n",
      "Epoch 468/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4372 - accuracy: 0.8200\n",
      "Epoch 00468: loss did not improve from 0.43601\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4500 - accuracy: 0.7826\n",
      "Epoch 469/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3720 - accuracy: 0.8800\n",
      "Epoch 00469: loss did not improve from 0.43601\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4430 - accuracy: 0.7943\n",
      "Epoch 470/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3733 - accuracy: 0.8600\n",
      "Epoch 00470: loss did not improve from 0.43601\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4499 - accuracy: 0.7852\n",
      "Epoch 471/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4652 - accuracy: 0.7800\n",
      "Epoch 00471: loss did not improve from 0.43601\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4400 - accuracy: 0.7865\n",
      "Epoch 472/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5380 - accuracy: 0.7000\n",
      "Epoch 00472: loss did not improve from 0.43601\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4436 - accuracy: 0.7865\n",
      "Epoch 473/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5308 - accuracy: 0.7200\n",
      "Epoch 00473: loss did not improve from 0.43601\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4549 - accuracy: 0.7865\n",
      "Epoch 474/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5295 - accuracy: 0.7600\n",
      "Epoch 00474: loss did not improve from 0.43601\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4570 - accuracy: 0.7747\n",
      "Epoch 475/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5834 - accuracy: 0.6600\n",
      "Epoch 00475: loss did not improve from 0.43601\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.4472 - accuracy: 0.7721\n",
      "Epoch 476/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3642 - accuracy: 0.8600\n",
      "Epoch 00476: loss did not improve from 0.43601\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4422 - accuracy: 0.7956\n",
      "Epoch 477/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3068 - accuracy: 0.9000\n",
      "Epoch 00477: loss improved from 0.43601 to 0.43537, saving model to ./pima_model\\477-0.4354.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4354 - accuracy: 0.7943\n",
      "Epoch 478/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4806 - accuracy: 0.7600\n",
      "Epoch 00478: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4463 - accuracy: 0.7943\n",
      "Epoch 479/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3484 - accuracy: 0.8400\n",
      "Epoch 00479: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4396 - accuracy: 0.7904\n",
      "Epoch 480/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4293 - accuracy: 0.8000\n",
      "Epoch 00480: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4410 - accuracy: 0.7917\n",
      "Epoch 481/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4683 - accuracy: 0.7600\n",
      "Epoch 00481: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4377 - accuracy: 0.7904\n",
      "Epoch 482/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3382 - accuracy: 0.8400\n",
      "Epoch 00482: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4401 - accuracy: 0.7969\n",
      "Epoch 483/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4160 - accuracy: 0.8200\n",
      "Epoch 00483: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4532 - accuracy: 0.7799\n",
      "Epoch 484/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5273 - accuracy: 0.7400\n",
      "Epoch 00484: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4539 - accuracy: 0.7786\n",
      "Epoch 485/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5405 - accuracy: 0.7000\n",
      "Epoch 00485: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4477 - accuracy: 0.7839\n",
      "Epoch 486/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4166 - accuracy: 0.8600\n",
      "Epoch 00486: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4464 - accuracy: 0.7839\n",
      "Epoch 487/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4317 - accuracy: 0.7600\n",
      "Epoch 00487: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4646 - accuracy: 0.7799\n",
      "Epoch 488/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3877 - accuracy: 0.8400\n",
      "Epoch 00488: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4559 - accuracy: 0.7747\n",
      "Epoch 489/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3849 - accuracy: 0.8400\n",
      "Epoch 00489: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 733us/step - loss: 0.4497 - accuracy: 0.7865\n",
      "Epoch 490/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6674 - accuracy: 0.6600\n",
      "Epoch 00490: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 733us/step - loss: 0.4529 - accuracy: 0.7786\n",
      "Epoch 491/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4001 - accuracy: 0.8200\n",
      "Epoch 00491: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4475 - accuracy: 0.7930\n",
      "Epoch 492/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4173 - accuracy: 0.8200\n",
      "Epoch 00492: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4392 - accuracy: 0.7826\n",
      "Epoch 493/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3346 - accuracy: 0.8800\n",
      "Epoch 00493: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4359 - accuracy: 0.7995\n",
      "Epoch 494/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5069 - accuracy: 0.7600\n",
      "Epoch 00494: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4433 - accuracy: 0.7826\n",
      "Epoch 495/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4154 - accuracy: 0.8400\n",
      "Epoch 00495: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4434 - accuracy: 0.7799\n",
      "Epoch 496/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3728 - accuracy: 0.8800\n",
      "Epoch 00496: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4386 - accuracy: 0.8034\n",
      "Epoch 497/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4601 - accuracy: 0.7600\n",
      "Epoch 00497: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 733us/step - loss: 0.4421 - accuracy: 0.7982\n",
      "Epoch 498/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4975 - accuracy: 0.7400\n",
      "Epoch 00498: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4377 - accuracy: 0.8021\n",
      "Epoch 499/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4451 - accuracy: 0.7600\n",
      "Epoch 00499: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4581 - accuracy: 0.7747\n",
      "Epoch 500/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5004 - accuracy: 0.6600\n",
      "Epoch 00500: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4381 - accuracy: 0.7995\n",
      "Epoch 501/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4518 - accuracy: 0.7000\n",
      "Epoch 00501: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4538 - accuracy: 0.7799\n",
      "Epoch 502/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5663 - accuracy: 0.7000\n",
      "Epoch 00502: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4557 - accuracy: 0.7839\n",
      "Epoch 503/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4462 - accuracy: 0.8200\n",
      "Epoch 00503: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4441 - accuracy: 0.7839\n",
      "Epoch 504/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4828 - accuracy: 0.7600\n",
      "Epoch 00504: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 732us/step - loss: 0.4415 - accuracy: 0.7930\n",
      "Epoch 505/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4164 - accuracy: 0.7800\n",
      "Epoch 00505: loss did not improve from 0.43537\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4391 - accuracy: 0.7839\n",
      "Epoch 506/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4457 - accuracy: 0.8000\n",
      "Epoch 00506: loss improved from 0.43537 to 0.43501, saving model to ./pima_model\\506-0.4350.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4350 - accuracy: 0.7943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 507/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4442 - accuracy: 0.7800\n",
      "Epoch 00507: loss did not improve from 0.43501\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4374 - accuracy: 0.7917\n",
      "Epoch 508/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4353 - accuracy: 0.7600\n",
      "Epoch 00508: loss did not improve from 0.43501\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4396 - accuracy: 0.8008\n",
      "Epoch 509/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4304 - accuracy: 0.8200\n",
      "Epoch 00509: loss did not improve from 0.43501\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4418 - accuracy: 0.7891\n",
      "Epoch 510/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4366 - accuracy: 0.8200\n",
      "Epoch 00510: loss did not improve from 0.43501\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4424 - accuracy: 0.7878\n",
      "Epoch 511/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4220 - accuracy: 0.8400\n",
      "Epoch 00511: loss did not improve from 0.43501\n",
      "16/16 [==============================] - 0s 863us/step - loss: 0.4417 - accuracy: 0.7943\n",
      "Epoch 512/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4111 - accuracy: 0.8200\n",
      "Epoch 00512: loss did not improve from 0.43501\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4503 - accuracy: 0.7943\n",
      "Epoch 513/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3929 - accuracy: 0.8200\n",
      "Epoch 00513: loss did not improve from 0.43501\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4379 - accuracy: 0.8060\n",
      "Epoch 514/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4465 - accuracy: 0.8000\n",
      "Epoch 00514: loss did not improve from 0.43501\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4592 - accuracy: 0.7708\n",
      "Epoch 515/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3833 - accuracy: 0.8400\n",
      "Epoch 00515: loss did not improve from 0.43501\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4489 - accuracy: 0.7839\n",
      "Epoch 516/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4090 - accuracy: 0.8200\n",
      "Epoch 00516: loss did not improve from 0.43501\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4592 - accuracy: 0.7826\n",
      "Epoch 517/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5218 - accuracy: 0.7600\n",
      "Epoch 00517: loss did not improve from 0.43501\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4488 - accuracy: 0.7812\n",
      "Epoch 518/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4509 - accuracy: 0.7600\n",
      "Epoch 00518: loss did not improve from 0.43501\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4387 - accuracy: 0.7878\n",
      "Epoch 519/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4176 - accuracy: 0.8000\n",
      "Epoch 00519: loss did not improve from 0.43501\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4354 - accuracy: 0.7865\n",
      "Epoch 520/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3798 - accuracy: 0.8000\n",
      "Epoch 00520: loss improved from 0.43501 to 0.43309, saving model to ./pima_model\\520-0.4331.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4331 - accuracy: 0.7930\n",
      "Epoch 521/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4342 - accuracy: 0.7600\n",
      "Epoch 00521: loss did not improve from 0.43309\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4397 - accuracy: 0.7878\n",
      "Epoch 522/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4845 - accuracy: 0.8200\n",
      "Epoch 00522: loss did not improve from 0.43309\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4461 - accuracy: 0.8008\n",
      "Epoch 523/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6118 - accuracy: 0.6600\n",
      "Epoch 00523: loss did not improve from 0.43309\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4342 - accuracy: 0.7995\n",
      "Epoch 524/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3494 - accuracy: 0.8800\n",
      "Epoch 00524: loss did not improve from 0.43309\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4464 - accuracy: 0.7865\n",
      "Epoch 525/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3427 - accuracy: 0.9200\n",
      "Epoch 00525: loss did not improve from 0.43309\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4364 - accuracy: 0.7969\n",
      "Epoch 526/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3347 - accuracy: 0.8400\n",
      "Epoch 00526: loss did not improve from 0.43309\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4423 - accuracy: 0.7799\n",
      "Epoch 527/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3623 - accuracy: 0.8600\n",
      "Epoch 00527: loss improved from 0.43309 to 0.43029, saving model to ./pima_model\\527-0.4303.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4303 - accuracy: 0.7995\n",
      "Epoch 528/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3962 - accuracy: 0.8200\n",
      "Epoch 00528: loss improved from 0.43029 to 0.42961, saving model to ./pima_model\\528-0.4296.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4296 - accuracy: 0.7956\n",
      "Epoch 529/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5038 - accuracy: 0.7400\n",
      "Epoch 00529: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4333 - accuracy: 0.7995\n",
      "Epoch 530/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4609 - accuracy: 0.8400\n",
      "Epoch 00530: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4365 - accuracy: 0.7812\n",
      "Epoch 531/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3909 - accuracy: 0.8400\n",
      "Epoch 00531: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4381 - accuracy: 0.7917\n",
      "Epoch 532/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4752 - accuracy: 0.7200\n",
      "Epoch 00532: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4400 - accuracy: 0.7904\n",
      "Epoch 533/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4966 - accuracy: 0.7600\n",
      "Epoch 00533: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4421 - accuracy: 0.7956\n",
      "Epoch 534/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3935 - accuracy: 0.8000\n",
      "Epoch 00534: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4353 - accuracy: 0.7982\n",
      "Epoch 535/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4907 - accuracy: 0.8000\n",
      "Epoch 00535: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4382 - accuracy: 0.7969\n",
      "Epoch 536/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5817 - accuracy: 0.7000\n",
      "Epoch 00536: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4342 - accuracy: 0.7826\n",
      "Epoch 537/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4212 - accuracy: 0.7800\n",
      "Epoch 00537: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4364 - accuracy: 0.8021\n",
      "Epoch 538/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3839 - accuracy: 0.8000\n",
      "Epoch 00538: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4406 - accuracy: 0.7865\n",
      "Epoch 539/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3372 - accuracy: 0.8800\n",
      "Epoch 00539: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4363 - accuracy: 0.7865\n",
      "Epoch 540/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4542 - accuracy: 0.8000\n",
      "Epoch 00540: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4378 - accuracy: 0.7891\n",
      "Epoch 541/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3661 - accuracy: 0.9200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00541: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.4406 - accuracy: 0.7852\n",
      "Epoch 542/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4468 - accuracy: 0.7800\n",
      "Epoch 00542: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.4459 - accuracy: 0.7826\n",
      "Epoch 543/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4049 - accuracy: 0.8400\n",
      "Epoch 00543: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4423 - accuracy: 0.7826\n",
      "Epoch 544/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3850 - accuracy: 0.8600\n",
      "Epoch 00544: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4396 - accuracy: 0.7969\n",
      "Epoch 545/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3903 - accuracy: 0.7800\n",
      "Epoch 00545: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4420 - accuracy: 0.7865\n",
      "Epoch 546/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3307 - accuracy: 0.8800\n",
      "Epoch 00546: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4510 - accuracy: 0.7839\n",
      "Epoch 547/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4692 - accuracy: 0.7200\n",
      "Epoch 00547: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4703 - accuracy: 0.7708\n",
      "Epoch 548/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3591 - accuracy: 0.8800\n",
      "Epoch 00548: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4480 - accuracy: 0.7812\n",
      "Epoch 549/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4707 - accuracy: 0.7600\n",
      "Epoch 00549: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4486 - accuracy: 0.7760\n",
      "Epoch 550/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4060 - accuracy: 0.8200\n",
      "Epoch 00550: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4378 - accuracy: 0.7943\n",
      "Epoch 551/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3947 - accuracy: 0.8400\n",
      "Epoch 00551: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4311 - accuracy: 0.7956\n",
      "Epoch 552/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4460 - accuracy: 0.7600\n",
      "Epoch 00552: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4360 - accuracy: 0.7917\n",
      "Epoch 553/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3991 - accuracy: 0.8400\n",
      "Epoch 00553: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4358 - accuracy: 0.7917\n",
      "Epoch 554/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4142 - accuracy: 0.8000\n",
      "Epoch 00554: loss did not improve from 0.42961\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4376 - accuracy: 0.7865\n",
      "Epoch 555/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4188 - accuracy: 0.8000\n",
      "Epoch 00555: loss improved from 0.42961 to 0.42832, saving model to ./pima_model\\555-0.4283.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4283 - accuracy: 0.7969\n",
      "Epoch 556/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4806 - accuracy: 0.7600\n",
      "Epoch 00556: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4383 - accuracy: 0.7878\n",
      "Epoch 557/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4230 - accuracy: 0.7600\n",
      "Epoch 00557: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4408 - accuracy: 0.7760\n",
      "Epoch 558/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3275 - accuracy: 0.8600\n",
      "Epoch 00558: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4500 - accuracy: 0.7826\n",
      "Epoch 559/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5531 - accuracy: 0.7400\n",
      "Epoch 00559: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4384 - accuracy: 0.7943\n",
      "Epoch 560/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3590 - accuracy: 0.8600\n",
      "Epoch 00560: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4532 - accuracy: 0.7812\n",
      "Epoch 561/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5199 - accuracy: 0.7600\n",
      "Epoch 00561: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4302 - accuracy: 0.8034\n",
      "Epoch 562/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5655 - accuracy: 0.6800\n",
      "Epoch 00562: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4582 - accuracy: 0.7747\n",
      "Epoch 563/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4342 - accuracy: 0.7800\n",
      "Epoch 00563: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4314 - accuracy: 0.7969\n",
      "Epoch 564/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4430 - accuracy: 0.7600\n",
      "Epoch 00564: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4416 - accuracy: 0.7852\n",
      "Epoch 565/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4836 - accuracy: 0.8000\n",
      "Epoch 00565: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4371 - accuracy: 0.7930\n",
      "Epoch 566/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4852 - accuracy: 0.7600\n",
      "Epoch 00566: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4288 - accuracy: 0.7878\n",
      "Epoch 567/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4577 - accuracy: 0.8000\n",
      "Epoch 00567: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4388 - accuracy: 0.7865\n",
      "Epoch 568/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4293 - accuracy: 0.7400\n",
      "Epoch 00568: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 799us/step - loss: 0.4319 - accuracy: 0.7786\n",
      "Epoch 569/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4665 - accuracy: 0.7800\n",
      "Epoch 00569: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4313 - accuracy: 0.7969\n",
      "Epoch 570/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4102 - accuracy: 0.8000\n",
      "Epoch 00570: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4325 - accuracy: 0.7995\n",
      "Epoch 571/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5581 - accuracy: 0.7800\n",
      "Epoch 00571: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4459 - accuracy: 0.7826\n",
      "Epoch 572/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3828 - accuracy: 0.8400\n",
      "Epoch 00572: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4382 - accuracy: 0.7878\n",
      "Epoch 573/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4133 - accuracy: 0.8600\n",
      "Epoch 00573: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4451 - accuracy: 0.7891\n",
      "Epoch 574/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4606 - accuracy: 0.7400\n",
      "Epoch 00574: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4301 - accuracy: 0.7930\n",
      "Epoch 575/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3897 - accuracy: 0.8000\n",
      "Epoch 00575: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4318 - accuracy: 0.7826\n",
      "Epoch 576/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4220 - accuracy: 0.8000\n",
      "Epoch 00576: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4307 - accuracy: 0.7930\n",
      "Epoch 577/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4496 - accuracy: 0.7400\n",
      "Epoch 00577: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4469 - accuracy: 0.7956\n",
      "Epoch 578/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3804 - accuracy: 0.9000\n",
      "Epoch 00578: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4429 - accuracy: 0.8008\n",
      "Epoch 579/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4137 - accuracy: 0.8000\n",
      "Epoch 00579: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4326 - accuracy: 0.7995\n",
      "Epoch 580/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4178 - accuracy: 0.7800\n",
      "Epoch 00580: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4402 - accuracy: 0.7878\n",
      "Epoch 581/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4603 - accuracy: 0.7400\n",
      "Epoch 00581: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4382 - accuracy: 0.7943\n",
      "Epoch 582/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4153 - accuracy: 0.8200\n",
      "Epoch 00582: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4376 - accuracy: 0.7982\n",
      "Epoch 583/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5016 - accuracy: 0.7400\n",
      "Epoch 00583: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4316 - accuracy: 0.7917\n",
      "Epoch 584/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3722 - accuracy: 0.8400\n",
      "Epoch 00584: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4375 - accuracy: 0.7904\n",
      "Epoch 585/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3282 - accuracy: 0.8800\n",
      "Epoch 00585: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4332 - accuracy: 0.7956\n",
      "Epoch 586/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5047 - accuracy: 0.7400\n",
      "Epoch 00586: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4306 - accuracy: 0.7852\n",
      "Epoch 587/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4524 - accuracy: 0.7800\n",
      "Epoch 00587: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4318 - accuracy: 0.7956\n",
      "Epoch 588/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4160 - accuracy: 0.8800\n",
      "Epoch 00588: loss did not improve from 0.42832\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4318 - accuracy: 0.8047\n",
      "Epoch 589/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3667 - accuracy: 0.8400\n",
      "Epoch 00589: loss improved from 0.42832 to 0.42750, saving model to ./pima_model\\589-0.4275.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4275 - accuracy: 0.7917\n",
      "Epoch 590/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4076 - accuracy: 0.8400\n",
      "Epoch 00590: loss did not improve from 0.42750\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4338 - accuracy: 0.7839\n",
      "Epoch 591/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4811 - accuracy: 0.7200\n",
      "Epoch 00591: loss improved from 0.42750 to 0.42640, saving model to ./pima_model\\591-0.4264.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4264 - accuracy: 0.7930\n",
      "Epoch 592/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3605 - accuracy: 0.8400\n",
      "Epoch 00592: loss did not improve from 0.42640\n",
      "16/16 [==============================] - 0s 866us/step - loss: 0.4466 - accuracy: 0.7904\n",
      "Epoch 593/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5516 - accuracy: 0.7000\n",
      "Epoch 00593: loss did not improve from 0.42640\n",
      "16/16 [==============================] - 0s 866us/step - loss: 0.4349 - accuracy: 0.7930\n",
      "Epoch 594/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4162 - accuracy: 0.8400\n",
      "Epoch 00594: loss improved from 0.42640 to 0.42598, saving model to ./pima_model\\594-0.4260.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4260 - accuracy: 0.7995\n",
      "Epoch 595/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3610 - accuracy: 0.8600\n",
      "Epoch 00595: loss did not improve from 0.42598\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4389 - accuracy: 0.7852\n",
      "Epoch 596/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3641 - accuracy: 0.8600\n",
      "Epoch 00596: loss did not improve from 0.42598\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4449 - accuracy: 0.7878\n",
      "Epoch 597/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4391 - accuracy: 0.8200\n",
      "Epoch 00597: loss did not improve from 0.42598\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.4338 - accuracy: 0.7826\n",
      "Epoch 598/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3696 - accuracy: 0.8200\n",
      "Epoch 00598: loss did not improve from 0.42598\n",
      "16/16 [==============================] - 0s 866us/step - loss: 0.4285 - accuracy: 0.7917\n",
      "Epoch 599/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4902 - accuracy: 0.7200\n",
      "Epoch 00599: loss did not improve from 0.42598\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4373 - accuracy: 0.7826\n",
      "Epoch 600/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4279 - accuracy: 0.7800\n",
      "Epoch 00600: loss did not improve from 0.42598\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4294 - accuracy: 0.8008\n",
      "Epoch 601/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4523 - accuracy: 0.7800\n",
      "Epoch 00601: loss did not improve from 0.42598\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4435 - accuracy: 0.7852\n",
      "Epoch 602/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2722 - accuracy: 0.9000\n",
      "Epoch 00602: loss did not improve from 0.42598\n",
      "16/16 [==============================] - 0s 865us/step - loss: 0.4515 - accuracy: 0.7956\n",
      "Epoch 603/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4032 - accuracy: 0.8200\n",
      "Epoch 00603: loss did not improve from 0.42598\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.4288 - accuracy: 0.7969\n",
      "Epoch 604/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4555 - accuracy: 0.8000\n",
      "Epoch 00604: loss did not improve from 0.42598\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4281 - accuracy: 0.7891\n",
      "Epoch 605/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5101 - accuracy: 0.7000\n",
      "Epoch 00605: loss improved from 0.42598 to 0.42580, saving model to ./pima_model\\605-0.4258.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4258 - accuracy: 0.7839\n",
      "Epoch 606/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4410 - accuracy: 0.7800\n",
      "Epoch 00606: loss did not improve from 0.42580\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4356 - accuracy: 0.7878\n",
      "Epoch 607/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4279 - accuracy: 0.7400\n",
      "Epoch 00607: loss did not improve from 0.42580\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4300 - accuracy: 0.7904\n",
      "Epoch 608/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4389 - accuracy: 0.8000\n",
      "Epoch 00608: loss did not improve from 0.42580\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4265 - accuracy: 0.7995\n",
      "Epoch 609/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3655 - accuracy: 0.7600\n",
      "Epoch 00609: loss did not improve from 0.42580\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4319 - accuracy: 0.7891\n",
      "Epoch 610/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4315 - accuracy: 0.7200\n",
      "Epoch 00610: loss did not improve from 0.42580\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4473 - accuracy: 0.7891\n",
      "Epoch 611/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3839 - accuracy: 0.8600\n",
      "Epoch 00611: loss did not improve from 0.42580\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4596 - accuracy: 0.7852\n",
      "Epoch 612/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4919 - accuracy: 0.8000\n",
      "Epoch 00612: loss did not improve from 0.42580\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4300 - accuracy: 0.7865\n",
      "Epoch 613/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5849 - accuracy: 0.7200\n",
      "Epoch 00613: loss did not improve from 0.42580\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4274 - accuracy: 0.7969\n",
      "Epoch 614/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4341 - accuracy: 0.7800\n",
      "Epoch 00614: loss did not improve from 0.42580\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4284 - accuracy: 0.7865\n",
      "Epoch 615/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5278 - accuracy: 0.7000\n",
      "Epoch 00615: loss did not improve from 0.42580\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4287 - accuracy: 0.7969\n",
      "Epoch 616/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4168 - accuracy: 0.8200\n",
      "Epoch 00616: loss did not improve from 0.42580\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4363 - accuracy: 0.7956\n",
      "Epoch 617/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3694 - accuracy: 0.8600\n",
      "Epoch 00617: loss did not improve from 0.42580\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4288 - accuracy: 0.7930\n",
      "Epoch 618/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4537 - accuracy: 0.7400\n",
      "Epoch 00618: loss improved from 0.42580 to 0.42499, saving model to ./pima_model\\618-0.4250.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4250 - accuracy: 0.8008\n",
      "Epoch 619/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4900 - accuracy: 0.7800\n",
      "Epoch 00619: loss did not improve from 0.42499\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4350 - accuracy: 0.7982\n",
      "Epoch 620/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5244 - accuracy: 0.7800\n",
      "Epoch 00620: loss did not improve from 0.42499\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4269 - accuracy: 0.7995\n",
      "Epoch 621/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4373 - accuracy: 0.8600\n",
      "Epoch 00621: loss did not improve from 0.42499\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4332 - accuracy: 0.7904\n",
      "Epoch 622/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3380 - accuracy: 0.8200\n",
      "Epoch 00622: loss did not improve from 0.42499\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4338 - accuracy: 0.7943\n",
      "Epoch 623/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4446 - accuracy: 0.7800\n",
      "Epoch 00623: loss improved from 0.42499 to 0.42453, saving model to ./pima_model\\623-0.4245.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4245 - accuracy: 0.7982\n",
      "Epoch 624/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4499 - accuracy: 0.8200\n",
      "Epoch 00624: loss did not improve from 0.42453\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4342 - accuracy: 0.7969\n",
      "Epoch 625/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5253 - accuracy: 0.7200\n",
      "Epoch 00625: loss did not improve from 0.42453\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4468 - accuracy: 0.7865\n",
      "Epoch 626/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4153 - accuracy: 0.8200\n",
      "Epoch 00626: loss did not improve from 0.42453\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4260 - accuracy: 0.7995\n",
      "Epoch 627/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4674 - accuracy: 0.7600\n",
      "Epoch 00627: loss improved from 0.42453 to 0.42250, saving model to ./pima_model\\627-0.4225.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4225 - accuracy: 0.8008\n",
      "Epoch 628/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4767 - accuracy: 0.8000\n",
      "Epoch 00628: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4313 - accuracy: 0.7969\n",
      "Epoch 629/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4518 - accuracy: 0.8000\n",
      "Epoch 00629: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4349 - accuracy: 0.7865\n",
      "Epoch 630/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4606 - accuracy: 0.8000\n",
      "Epoch 00630: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4244 - accuracy: 0.7995\n",
      "Epoch 631/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5423 - accuracy: 0.7000\n",
      "Epoch 00631: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4246 - accuracy: 0.7995\n",
      "Epoch 632/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4153 - accuracy: 0.7600\n",
      "Epoch 00632: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4423 - accuracy: 0.7930\n",
      "Epoch 633/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3420 - accuracy: 0.9000\n",
      "Epoch 00633: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4244 - accuracy: 0.8008\n",
      "Epoch 634/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3190 - accuracy: 0.9000\n",
      "Epoch 00634: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4389 - accuracy: 0.7982\n",
      "Epoch 635/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5155 - accuracy: 0.7600\n",
      "Epoch 00635: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4475 - accuracy: 0.7826\n",
      "Epoch 636/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4555 - accuracy: 0.8200\n",
      "Epoch 00636: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4257 - accuracy: 0.7956\n",
      "Epoch 637/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4795 - accuracy: 0.8200\n",
      "Epoch 00637: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4301 - accuracy: 0.7969\n",
      "Epoch 638/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5723 - accuracy: 0.6600\n",
      "Epoch 00638: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4310 - accuracy: 0.7839\n",
      "Epoch 639/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3787 - accuracy: 0.8000\n",
      "Epoch 00639: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4284 - accuracy: 0.7982\n",
      "Epoch 640/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3565 - accuracy: 0.8000\n",
      "Epoch 00640: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4251 - accuracy: 0.7969\n",
      "Epoch 641/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3779 - accuracy: 0.8400\n",
      "Epoch 00641: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 796us/step - loss: 0.4259 - accuracy: 0.7891\n",
      "Epoch 642/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4253 - accuracy: 0.8400\n",
      "Epoch 00642: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4260 - accuracy: 0.8047\n",
      "Epoch 643/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5386 - accuracy: 0.7200\n",
      "Epoch 00643: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4353 - accuracy: 0.7812\n",
      "Epoch 644/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5268 - accuracy: 0.7400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00644: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4529 - accuracy: 0.7747\n",
      "Epoch 645/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4224 - accuracy: 0.8000\n",
      "Epoch 00645: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4652 - accuracy: 0.7747\n",
      "Epoch 646/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4257 - accuracy: 0.7400\n",
      "Epoch 00646: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4313 - accuracy: 0.7904\n",
      "Epoch 647/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4068 - accuracy: 0.8200\n",
      "Epoch 00647: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4313 - accuracy: 0.7904\n",
      "Epoch 648/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4597 - accuracy: 0.8000\n",
      "Epoch 00648: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4235 - accuracy: 0.7943\n",
      "Epoch 649/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3073 - accuracy: 0.8800\n",
      "Epoch 00649: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4274 - accuracy: 0.7995\n",
      "Epoch 650/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5068 - accuracy: 0.7800\n",
      "Epoch 00650: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4319 - accuracy: 0.7891\n",
      "Epoch 651/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4701 - accuracy: 0.7400\n",
      "Epoch 00651: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4344 - accuracy: 0.7695\n",
      "Epoch 652/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4933 - accuracy: 0.7400\n",
      "Epoch 00652: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4285 - accuracy: 0.7891\n",
      "Epoch 653/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3693 - accuracy: 0.8200\n",
      "Epoch 00653: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4370 - accuracy: 0.7865\n",
      "Epoch 654/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5046 - accuracy: 0.8000\n",
      "Epoch 00654: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4439 - accuracy: 0.7917\n",
      "Epoch 655/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3549 - accuracy: 0.8800\n",
      "Epoch 00655: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4351 - accuracy: 0.7943\n",
      "Epoch 656/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4015 - accuracy: 0.8000\n",
      "Epoch 00656: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4365 - accuracy: 0.7930\n",
      "Epoch 657/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3221 - accuracy: 0.9000\n",
      "Epoch 00657: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4238 - accuracy: 0.7917\n",
      "Epoch 658/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6113 - accuracy: 0.7000\n",
      "Epoch 00658: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4276 - accuracy: 0.7930\n",
      "Epoch 659/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3887 - accuracy: 0.8400\n",
      "Epoch 00659: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 664us/step - loss: 0.4281 - accuracy: 0.7891\n",
      "Epoch 660/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3577 - accuracy: 0.8200\n",
      "Epoch 00660: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4304 - accuracy: 0.7839\n",
      "Epoch 661/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4557 - accuracy: 0.7800\n",
      "Epoch 00661: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4431 - accuracy: 0.7852\n",
      "Epoch 662/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5064 - accuracy: 0.7400\n",
      "Epoch 00662: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4480 - accuracy: 0.7891\n",
      "Epoch 663/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5215 - accuracy: 0.7400\n",
      "Epoch 00663: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4481 - accuracy: 0.8008\n",
      "Epoch 664/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4336 - accuracy: 0.7400\n",
      "Epoch 00664: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4270 - accuracy: 0.7995\n",
      "Epoch 665/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4547 - accuracy: 0.7600\n",
      "Epoch 00665: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4234 - accuracy: 0.7904\n",
      "Epoch 666/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3634 - accuracy: 0.8600\n",
      "Epoch 00666: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4337 - accuracy: 0.7904\n",
      "Epoch 667/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4914 - accuracy: 0.7200\n",
      "Epoch 00667: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4359 - accuracy: 0.8086\n",
      "Epoch 668/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4068 - accuracy: 0.8200\n",
      "Epoch 00668: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4246 - accuracy: 0.7982\n",
      "Epoch 669/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4048 - accuracy: 0.7600\n",
      "Epoch 00669: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4441 - accuracy: 0.7786\n",
      "Epoch 670/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4558 - accuracy: 0.7400\n",
      "Epoch 00670: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4292 - accuracy: 0.7904\n",
      "Epoch 671/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3377 - accuracy: 0.8400\n",
      "Epoch 00671: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4233 - accuracy: 0.7956\n",
      "Epoch 672/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3138 - accuracy: 0.8600\n",
      "Epoch 00672: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4286 - accuracy: 0.7943\n",
      "Epoch 673/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4018 - accuracy: 0.8200\n",
      "Epoch 00673: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4254 - accuracy: 0.8008\n",
      "Epoch 674/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5058 - accuracy: 0.7200\n",
      "Epoch 00674: loss did not improve from 0.42250\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4324 - accuracy: 0.7904\n",
      "Epoch 675/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3784 - accuracy: 0.8400\n",
      "Epoch 00675: loss improved from 0.42250 to 0.42211, saving model to ./pima_model\\675-0.4221.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4221 - accuracy: 0.7982\n",
      "Epoch 676/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4009 - accuracy: 0.7600\n",
      "Epoch 00676: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4288 - accuracy: 0.7891\n",
      "Epoch 677/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4386 - accuracy: 0.7200\n",
      "Epoch 00677: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4281 - accuracy: 0.7943\n",
      "Epoch 678/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4922 - accuracy: 0.7000\n",
      "Epoch 00678: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4247 - accuracy: 0.7956\n",
      "Epoch 679/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3537 - accuracy: 0.8000\n",
      "Epoch 00679: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4225 - accuracy: 0.8047\n",
      "Epoch 680/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5281 - accuracy: 0.7400\n",
      "Epoch 00680: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4306 - accuracy: 0.7969\n",
      "Epoch 681/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4624 - accuracy: 0.8400\n",
      "Epoch 00681: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4242 - accuracy: 0.7969\n",
      "Epoch 682/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4156 - accuracy: 0.8400\n",
      "Epoch 00682: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 865us/step - loss: 0.4255 - accuracy: 0.7995\n",
      "Epoch 683/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4621 - accuracy: 0.7400\n",
      "Epoch 00683: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4289 - accuracy: 0.7865\n",
      "Epoch 684/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5500 - accuracy: 0.7600\n",
      "Epoch 00684: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4339 - accuracy: 0.7956\n",
      "Epoch 685/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3169 - accuracy: 0.9200\n",
      "Epoch 00685: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4399 - accuracy: 0.7852\n",
      "Epoch 686/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4566 - accuracy: 0.7800\n",
      "Epoch 00686: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4310 - accuracy: 0.7969\n",
      "Epoch 687/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3882 - accuracy: 0.7600\n",
      "Epoch 00687: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4321 - accuracy: 0.7917\n",
      "Epoch 688/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4098 - accuracy: 0.7600\n",
      "Epoch 00688: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4270 - accuracy: 0.8021\n",
      "Epoch 689/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4548 - accuracy: 0.7800\n",
      "Epoch 00689: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4295 - accuracy: 0.7917\n",
      "Epoch 690/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3470 - accuracy: 0.8800\n",
      "Epoch 00690: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4274 - accuracy: 0.8008\n",
      "Epoch 691/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4546 - accuracy: 0.8000\n",
      "Epoch 00691: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4236 - accuracy: 0.7943\n",
      "Epoch 692/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5118 - accuracy: 0.7200\n",
      "Epoch 00692: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4271 - accuracy: 0.7891\n",
      "Epoch 693/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4372 - accuracy: 0.8200\n",
      "Epoch 00693: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4264 - accuracy: 0.8008\n",
      "Epoch 694/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3846 - accuracy: 0.8200\n",
      "Epoch 00694: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4367 - accuracy: 0.7943\n",
      "Epoch 695/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3514 - accuracy: 0.8400\n",
      "Epoch 00695: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4293 - accuracy: 0.7917\n",
      "Epoch 696/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4554 - accuracy: 0.7800\n",
      "Epoch 00696: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 664us/step - loss: 0.4340 - accuracy: 0.7956\n",
      "Epoch 697/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4060 - accuracy: 0.8000\n",
      "Epoch 00697: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4395 - accuracy: 0.7852\n",
      "Epoch 698/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3816 - accuracy: 0.8400\n",
      "Epoch 00698: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4243 - accuracy: 0.8086\n",
      "Epoch 699/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5781 - accuracy: 0.7200\n",
      "Epoch 00699: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4385 - accuracy: 0.7773\n",
      "Epoch 700/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3623 - accuracy: 0.8400\n",
      "Epoch 00700: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4495 - accuracy: 0.8021\n",
      "Epoch 701/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4318 - accuracy: 0.7600\n",
      "Epoch 00701: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4379 - accuracy: 0.7943\n",
      "Epoch 702/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5206 - accuracy: 0.7000\n",
      "Epoch 00702: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4296 - accuracy: 0.7956\n",
      "Epoch 703/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5009 - accuracy: 0.8000\n",
      "Epoch 00703: loss did not improve from 0.42211\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.4249 - accuracy: 0.8021\n",
      "Epoch 704/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4213 - accuracy: 0.8000\n",
      "Epoch 00704: loss improved from 0.42211 to 0.42091, saving model to ./pima_model\\704-0.4209.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4209 - accuracy: 0.7995\n",
      "Epoch 705/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3797 - accuracy: 0.8400\n",
      "Epoch 00705: loss did not improve from 0.42091\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4284 - accuracy: 0.7969\n",
      "Epoch 706/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4379 - accuracy: 0.8200\n",
      "Epoch 00706: loss did not improve from 0.42091\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4218 - accuracy: 0.8008\n",
      "Epoch 707/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3857 - accuracy: 0.8000\n",
      "Epoch 00707: loss did not improve from 0.42091\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4211 - accuracy: 0.7995\n",
      "Epoch 708/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4600 - accuracy: 0.7200\n",
      "Epoch 00708: loss did not improve from 0.42091\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4256 - accuracy: 0.7956\n",
      "Epoch 709/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3916 - accuracy: 0.8200\n",
      "Epoch 00709: loss did not improve from 0.42091\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4229 - accuracy: 0.7995\n",
      "Epoch 710/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5487 - accuracy: 0.7200\n",
      "Epoch 00710: loss did not improve from 0.42091\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4224 - accuracy: 0.7982\n",
      "Epoch 711/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4713 - accuracy: 0.7600\n",
      "Epoch 00711: loss improved from 0.42091 to 0.41911, saving model to ./pima_model\\711-0.4191.hdf5\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4191 - accuracy: 0.8034\n",
      "Epoch 712/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4425 - accuracy: 0.8400\n",
      "Epoch 00712: loss did not improve from 0.41911\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4231 - accuracy: 0.7930\n",
      "Epoch 713/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3755 - accuracy: 0.8800\n",
      "Epoch 00713: loss did not improve from 0.41911\n",
      "16/16 [==============================] - 0s 730us/step - loss: 0.4210 - accuracy: 0.8021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 714/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4596 - accuracy: 0.7600\n",
      "Epoch 00714: loss did not improve from 0.41911\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4225 - accuracy: 0.7995\n",
      "Epoch 715/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5123 - accuracy: 0.7400\n",
      "Epoch 00715: loss did not improve from 0.41911\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4273 - accuracy: 0.7969\n",
      "Epoch 716/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2628 - accuracy: 0.9000\n",
      "Epoch 00716: loss did not improve from 0.41911\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4587 - accuracy: 0.7760\n",
      "Epoch 717/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2974 - accuracy: 0.8400\n",
      "Epoch 00717: loss did not improve from 0.41911\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4236 - accuracy: 0.7917\n",
      "Epoch 718/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3444 - accuracy: 0.8600\n",
      "Epoch 00718: loss did not improve from 0.41911\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4285 - accuracy: 0.7865\n",
      "Epoch 719/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3815 - accuracy: 0.8400\n",
      "Epoch 00719: loss did not improve from 0.41911\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4244 - accuracy: 0.7956\n",
      "Epoch 720/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4364 - accuracy: 0.7600\n",
      "Epoch 00720: loss did not improve from 0.41911\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4255 - accuracy: 0.7930\n",
      "Epoch 721/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4182 - accuracy: 0.7800\n",
      "Epoch 00721: loss did not improve from 0.41911\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4220 - accuracy: 0.7930\n",
      "Epoch 722/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4807 - accuracy: 0.7600\n",
      "Epoch 00722: loss did not improve from 0.41911\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4291 - accuracy: 0.7852\n",
      "Epoch 723/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5594 - accuracy: 0.7000\n",
      "Epoch 00723: loss did not improve from 0.41911\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4404 - accuracy: 0.7917\n",
      "Epoch 724/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4400 - accuracy: 0.8000\n",
      "Epoch 00724: loss did not improve from 0.41911\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4332 - accuracy: 0.7891\n",
      "Epoch 725/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4707 - accuracy: 0.8200\n",
      "Epoch 00725: loss did not improve from 0.41911\n",
      "16/16 [==============================] - 0s 797us/step - loss: 0.4242 - accuracy: 0.8099\n",
      "Epoch 726/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4634 - accuracy: 0.7600\n",
      "Epoch 00726: loss did not improve from 0.41911\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4262 - accuracy: 0.7982\n",
      "Epoch 727/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3644 - accuracy: 0.8200\n",
      "Epoch 00727: loss did not improve from 0.41911\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4206 - accuracy: 0.7930\n",
      "Epoch 728/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4282 - accuracy: 0.8400\n",
      "Epoch 00728: loss did not improve from 0.41911\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4264 - accuracy: 0.7943\n",
      "Epoch 729/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5654 - accuracy: 0.7200\n",
      "Epoch 00729: loss did not improve from 0.41911\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4205 - accuracy: 0.7982\n",
      "Epoch 730/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3296 - accuracy: 0.8600\n",
      "Epoch 00730: loss improved from 0.41911 to 0.41885, saving model to ./pima_model\\730-0.4189.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4189 - accuracy: 0.8047\n",
      "Epoch 731/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4483 - accuracy: 0.8200\n",
      "Epoch 00731: loss did not improve from 0.41885\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4223 - accuracy: 0.8060\n",
      "Epoch 732/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4970 - accuracy: 0.7600\n",
      "Epoch 00732: loss did not improve from 0.41885\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4231 - accuracy: 0.8047\n",
      "Epoch 733/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4202 - accuracy: 0.8000\n",
      "Epoch 00733: loss did not improve from 0.41885\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4437 - accuracy: 0.7747\n",
      "Epoch 734/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6137 - accuracy: 0.6600\n",
      "Epoch 00734: loss did not improve from 0.41885\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4360 - accuracy: 0.8021\n",
      "Epoch 735/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4585 - accuracy: 0.8400\n",
      "Epoch 00735: loss did not improve from 0.41885\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4560 - accuracy: 0.7812\n",
      "Epoch 736/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4122 - accuracy: 0.8000\n",
      "Epoch 00736: loss improved from 0.41885 to 0.41642, saving model to ./pima_model\\736-0.4164.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4164 - accuracy: 0.7956\n",
      "Epoch 737/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3646 - accuracy: 0.8400\n",
      "Epoch 00737: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4252 - accuracy: 0.7930\n",
      "Epoch 738/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6526 - accuracy: 0.7200\n",
      "Epoch 00738: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4204 - accuracy: 0.8099\n",
      "Epoch 739/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3705 - accuracy: 0.8400\n",
      "Epoch 00739: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4175 - accuracy: 0.8021\n",
      "Epoch 740/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4182 - accuracy: 0.7600\n",
      "Epoch 00740: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4212 - accuracy: 0.7930\n",
      "Epoch 741/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4201 - accuracy: 0.7800\n",
      "Epoch 00741: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 732us/step - loss: 0.4236 - accuracy: 0.7982\n",
      "Epoch 742/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3992 - accuracy: 0.8000\n",
      "Epoch 00742: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4271 - accuracy: 0.7969\n",
      "Epoch 743/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4115 - accuracy: 0.7800\n",
      "Epoch 00743: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4324 - accuracy: 0.7839\n",
      "Epoch 744/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3903 - accuracy: 0.8200\n",
      "Epoch 00744: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4270 - accuracy: 0.8073\n",
      "Epoch 745/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4392 - accuracy: 0.8200\n",
      "Epoch 00745: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.4210 - accuracy: 0.8021\n",
      "Epoch 746/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4643 - accuracy: 0.7400\n",
      "Epoch 00746: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4254 - accuracy: 0.7956\n",
      "Epoch 747/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4536 - accuracy: 0.8400\n",
      "Epoch 00747: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4363 - accuracy: 0.7904\n",
      "Epoch 748/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4646 - accuracy: 0.7600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00748: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4334 - accuracy: 0.7826\n",
      "Epoch 749/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4615 - accuracy: 0.7400\n",
      "Epoch 00749: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4254 - accuracy: 0.7917\n",
      "Epoch 750/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3508 - accuracy: 0.8200\n",
      "Epoch 00750: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4248 - accuracy: 0.8073\n",
      "Epoch 751/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4892 - accuracy: 0.7600\n",
      "Epoch 00751: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4224 - accuracy: 0.8034\n",
      "Epoch 752/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3757 - accuracy: 0.8600\n",
      "Epoch 00752: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4251 - accuracy: 0.7917\n",
      "Epoch 753/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3596 - accuracy: 0.8400\n",
      "Epoch 00753: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4229 - accuracy: 0.8021\n",
      "Epoch 754/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3462 - accuracy: 0.8400\n",
      "Epoch 00754: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4223 - accuracy: 0.7956\n",
      "Epoch 755/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3463 - accuracy: 0.8800\n",
      "Epoch 00755: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4303 - accuracy: 0.7878\n",
      "Epoch 756/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4387 - accuracy: 0.8000\n",
      "Epoch 00756: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4361 - accuracy: 0.7799\n",
      "Epoch 757/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4213 - accuracy: 0.8400\n",
      "Epoch 00757: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4523 - accuracy: 0.7904\n",
      "Epoch 758/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3267 - accuracy: 0.8200\n",
      "Epoch 00758: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4311 - accuracy: 0.7878\n",
      "Epoch 759/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4311 - accuracy: 0.7800\n",
      "Epoch 00759: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4233 - accuracy: 0.7930\n",
      "Epoch 760/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4431 - accuracy: 0.8200\n",
      "Epoch 00760: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4229 - accuracy: 0.7995\n",
      "Epoch 761/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5430 - accuracy: 0.7400\n",
      "Epoch 00761: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4207 - accuracy: 0.8047\n",
      "Epoch 762/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4039 - accuracy: 0.8600\n",
      "Epoch 00762: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4245 - accuracy: 0.7982\n",
      "Epoch 763/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4391 - accuracy: 0.7600\n",
      "Epoch 00763: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4211 - accuracy: 0.7969\n",
      "Epoch 764/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4691 - accuracy: 0.8000\n",
      "Epoch 00764: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4267 - accuracy: 0.8021\n",
      "Epoch 765/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4467 - accuracy: 0.7800\n",
      "Epoch 00765: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4164 - accuracy: 0.8021\n",
      "Epoch 766/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4628 - accuracy: 0.8000\n",
      "Epoch 00766: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4348 - accuracy: 0.7826\n",
      "Epoch 767/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3901 - accuracy: 0.8600\n",
      "Epoch 00767: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4213 - accuracy: 0.8008\n",
      "Epoch 768/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4212 - accuracy: 0.7600\n",
      "Epoch 00768: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4425 - accuracy: 0.7826\n",
      "Epoch 769/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4126 - accuracy: 0.8200\n",
      "Epoch 00769: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4764 - accuracy: 0.7826\n",
      "Epoch 770/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3618 - accuracy: 0.8200\n",
      "Epoch 00770: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4487 - accuracy: 0.7865\n",
      "Epoch 771/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4277 - accuracy: 0.8000\n",
      "Epoch 00771: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4257 - accuracy: 0.8047\n",
      "Epoch 772/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6215 - accuracy: 0.7200\n",
      "Epoch 00772: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4333 - accuracy: 0.7799\n",
      "Epoch 773/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4337 - accuracy: 0.8000\n",
      "Epoch 00773: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4214 - accuracy: 0.8047\n",
      "Epoch 774/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4376 - accuracy: 0.8000\n",
      "Epoch 00774: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4241 - accuracy: 0.8073\n",
      "Epoch 775/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.6218 - accuracy: 0.6400\n",
      "Epoch 00775: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4244 - accuracy: 0.7995\n",
      "Epoch 776/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4858 - accuracy: 0.7800\n",
      "Epoch 00776: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4204 - accuracy: 0.7982\n",
      "Epoch 777/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4564 - accuracy: 0.8000\n",
      "Epoch 00777: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4206 - accuracy: 0.8008\n",
      "Epoch 778/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4132 - accuracy: 0.8400\n",
      "Epoch 00778: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4187 - accuracy: 0.8008\n",
      "Epoch 779/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4162 - accuracy: 0.7600\n",
      "Epoch 00779: loss did not improve from 0.41642\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4249 - accuracy: 0.8021\n",
      "Epoch 780/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3280 - accuracy: 0.8800\n",
      "Epoch 00780: loss improved from 0.41642 to 0.41628, saving model to ./pima_model\\780-0.4163.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4163 - accuracy: 0.8073\n",
      "Epoch 781/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4935 - accuracy: 0.7800\n",
      "Epoch 00781: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4199 - accuracy: 0.8034\n",
      "Epoch 782/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5482 - accuracy: 0.7000\n",
      "Epoch 00782: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4206 - accuracy: 0.8060\n",
      "Epoch 783/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4318 - accuracy: 0.8200\n",
      "Epoch 00783: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4300 - accuracy: 0.7969\n",
      "Epoch 784/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3288 - accuracy: 0.8200\n",
      "Epoch 00784: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 664us/step - loss: 0.4416 - accuracy: 0.7852\n",
      "Epoch 785/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4981 - accuracy: 0.7600\n",
      "Epoch 00785: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4368 - accuracy: 0.7982\n",
      "Epoch 786/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4589 - accuracy: 0.7600\n",
      "Epoch 00786: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4253 - accuracy: 0.7865\n",
      "Epoch 787/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5029 - accuracy: 0.8200\n",
      "Epoch 00787: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4307 - accuracy: 0.7891\n",
      "Epoch 788/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4492 - accuracy: 0.7600\n",
      "Epoch 00788: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4364 - accuracy: 0.7852\n",
      "Epoch 789/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3756 - accuracy: 0.8200\n",
      "Epoch 00789: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4242 - accuracy: 0.7930\n",
      "Epoch 790/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3924 - accuracy: 0.8400\n",
      "Epoch 00790: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4224 - accuracy: 0.8021\n",
      "Epoch 791/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3786 - accuracy: 0.8200\n",
      "Epoch 00791: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4186 - accuracy: 0.7943\n",
      "Epoch 792/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4797 - accuracy: 0.7400\n",
      "Epoch 00792: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4384 - accuracy: 0.8008\n",
      "Epoch 793/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4272 - accuracy: 0.7800\n",
      "Epoch 00793: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4237 - accuracy: 0.8060\n",
      "Epoch 794/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4071 - accuracy: 0.8200\n",
      "Epoch 00794: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4177 - accuracy: 0.8034\n",
      "Epoch 795/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4277 - accuracy: 0.7800\n",
      "Epoch 00795: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4204 - accuracy: 0.8086\n",
      "Epoch 796/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4146 - accuracy: 0.8000\n",
      "Epoch 00796: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4240 - accuracy: 0.8034\n",
      "Epoch 797/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4018 - accuracy: 0.8400\n",
      "Epoch 00797: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4234 - accuracy: 0.8086\n",
      "Epoch 798/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4032 - accuracy: 0.8200\n",
      "Epoch 00798: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 732us/step - loss: 0.4333 - accuracy: 0.8021\n",
      "Epoch 799/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3216 - accuracy: 0.8400\n",
      "Epoch 00799: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 732us/step - loss: 0.4174 - accuracy: 0.8034\n",
      "Epoch 800/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4779 - accuracy: 0.7600\n",
      "Epoch 00800: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4164 - accuracy: 0.8060\n",
      "Epoch 801/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3808 - accuracy: 0.8200\n",
      "Epoch 00801: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4177 - accuracy: 0.8047\n",
      "Epoch 802/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4525 - accuracy: 0.8600\n",
      "Epoch 00802: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4367 - accuracy: 0.7969\n",
      "Epoch 803/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4135 - accuracy: 0.8200\n",
      "Epoch 00803: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4175 - accuracy: 0.7943\n",
      "Epoch 804/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4429 - accuracy: 0.8400\n",
      "Epoch 00804: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4310 - accuracy: 0.7891\n",
      "Epoch 805/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5270 - accuracy: 0.7400\n",
      "Epoch 00805: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4227 - accuracy: 0.8008\n",
      "Epoch 806/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4415 - accuracy: 0.8600\n",
      "Epoch 00806: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4221 - accuracy: 0.8125\n",
      "Epoch 807/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4172 - accuracy: 0.8200\n",
      "Epoch 00807: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4189 - accuracy: 0.7982\n",
      "Epoch 808/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4012 - accuracy: 0.7800\n",
      "Epoch 00808: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4254 - accuracy: 0.7995\n",
      "Epoch 809/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4081 - accuracy: 0.8000\n",
      "Epoch 00809: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4267 - accuracy: 0.8008\n",
      "Epoch 810/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3154 - accuracy: 0.8800\n",
      "Epoch 00810: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4248 - accuracy: 0.7956\n",
      "Epoch 811/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5075 - accuracy: 0.7200\n",
      "Epoch 00811: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4263 - accuracy: 0.8086\n",
      "Epoch 812/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4357 - accuracy: 0.7800\n",
      "Epoch 00812: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4187 - accuracy: 0.8021\n",
      "Epoch 813/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5718 - accuracy: 0.7000\n",
      "Epoch 00813: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.4367 - accuracy: 0.7839\n",
      "Epoch 814/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4084 - accuracy: 0.8000\n",
      "Epoch 00814: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4181 - accuracy: 0.8060\n",
      "Epoch 815/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4512 - accuracy: 0.7800\n",
      "Epoch 00815: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.4239 - accuracy: 0.8047\n",
      "Epoch 816/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4492 - accuracy: 0.7600\n",
      "Epoch 00816: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4379 - accuracy: 0.7917\n",
      "Epoch 817/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3642 - accuracy: 0.8400\n",
      "Epoch 00817: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4237 - accuracy: 0.7969\n",
      "Epoch 818/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5528 - accuracy: 0.7600\n",
      "Epoch 00818: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4283 - accuracy: 0.7943\n",
      "Epoch 819/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4931 - accuracy: 0.7400\n",
      "Epoch 00819: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4264 - accuracy: 0.7943\n",
      "Epoch 820/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5086 - accuracy: 0.8000\n",
      "Epoch 00820: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4271 - accuracy: 0.7930\n",
      "Epoch 821/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5129 - accuracy: 0.6400\n",
      "Epoch 00821: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4326 - accuracy: 0.7865\n",
      "Epoch 822/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4184 - accuracy: 0.8400\n",
      "Epoch 00822: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4283 - accuracy: 0.7930\n",
      "Epoch 823/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5335 - accuracy: 0.7400\n",
      "Epoch 00823: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4229 - accuracy: 0.7969\n",
      "Epoch 824/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4839 - accuracy: 0.7400\n",
      "Epoch 00824: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4232 - accuracy: 0.8034\n",
      "Epoch 825/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5927 - accuracy: 0.7200\n",
      "Epoch 00825: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4298 - accuracy: 0.7995\n",
      "Epoch 826/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3839 - accuracy: 0.8000\n",
      "Epoch 00826: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4243 - accuracy: 0.7891\n",
      "Epoch 827/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4934 - accuracy: 0.7800\n",
      "Epoch 00827: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4262 - accuracy: 0.8047\n",
      "Epoch 828/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5051 - accuracy: 0.8000\n",
      "Epoch 00828: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 664us/step - loss: 0.4194 - accuracy: 0.7995\n",
      "Epoch 829/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3979 - accuracy: 0.8000\n",
      "Epoch 00829: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4269 - accuracy: 0.7878\n",
      "Epoch 830/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3815 - accuracy: 0.8600\n",
      "Epoch 00830: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4214 - accuracy: 0.8034\n",
      "Epoch 831/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3364 - accuracy: 0.8400\n",
      "Epoch 00831: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4166 - accuracy: 0.8034\n",
      "Epoch 832/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3944 - accuracy: 0.8400\n",
      "Epoch 00832: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4256 - accuracy: 0.7969\n",
      "Epoch 833/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4103 - accuracy: 0.7800\n",
      "Epoch 00833: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4253 - accuracy: 0.7969\n",
      "Epoch 834/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4781 - accuracy: 0.7600\n",
      "Epoch 00834: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4175 - accuracy: 0.8073\n",
      "Epoch 835/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4200 - accuracy: 0.8400\n",
      "Epoch 00835: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4205 - accuracy: 0.7982\n",
      "Epoch 836/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3533 - accuracy: 0.8400\n",
      "Epoch 00836: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4300 - accuracy: 0.7943\n",
      "Epoch 837/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3072 - accuracy: 0.8800\n",
      "Epoch 00837: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4177 - accuracy: 0.8086\n",
      "Epoch 838/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5497 - accuracy: 0.8000\n",
      "Epoch 00838: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4196 - accuracy: 0.8034\n",
      "Epoch 839/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4715 - accuracy: 0.8400\n",
      "Epoch 00839: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4244 - accuracy: 0.7982\n",
      "Epoch 840/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3169 - accuracy: 0.8400\n",
      "Epoch 00840: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4177 - accuracy: 0.8099\n",
      "Epoch 841/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3261 - accuracy: 0.8400\n",
      "Epoch 00841: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4193 - accuracy: 0.7930\n",
      "Epoch 842/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3850 - accuracy: 0.8200\n",
      "Epoch 00842: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 732us/step - loss: 0.4215 - accuracy: 0.8008\n",
      "Epoch 843/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4222 - accuracy: 0.8200\n",
      "Epoch 00843: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4168 - accuracy: 0.8034\n",
      "Epoch 844/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4087 - accuracy: 0.8000\n",
      "Epoch 00844: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4290 - accuracy: 0.7969\n",
      "Epoch 845/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2974 - accuracy: 0.8800\n",
      "Epoch 00845: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4231 - accuracy: 0.7982\n",
      "Epoch 846/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4093 - accuracy: 0.8000\n",
      "Epoch 00846: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4380 - accuracy: 0.7982\n",
      "Epoch 847/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4822 - accuracy: 0.7400\n",
      "Epoch 00847: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 797us/step - loss: 0.4475 - accuracy: 0.7852\n",
      "Epoch 848/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4292 - accuracy: 0.7600\n",
      "Epoch 00848: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4164 - accuracy: 0.7943\n",
      "Epoch 849/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4633 - accuracy: 0.7600\n",
      "Epoch 00849: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.4235 - accuracy: 0.7812\n",
      "Epoch 850/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5042 - accuracy: 0.8000\n",
      "Epoch 00850: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4255 - accuracy: 0.7943\n",
      "Epoch 851/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3714 - accuracy: 0.8200\n",
      "Epoch 00851: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4207 - accuracy: 0.7969\n",
      "Epoch 852/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4096 - accuracy: 0.7800\n",
      "Epoch 00852: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4281 - accuracy: 0.8021\n",
      "Epoch 853/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3384 - accuracy: 0.8400\n",
      "Epoch 00853: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4177 - accuracy: 0.7995\n",
      "Epoch 854/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4001 - accuracy: 0.8200\n",
      "Epoch 00854: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4229 - accuracy: 0.7982\n",
      "Epoch 855/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4312 - accuracy: 0.8600\n",
      "Epoch 00855: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4252 - accuracy: 0.7969\n",
      "Epoch 856/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3376 - accuracy: 0.8600\n",
      "Epoch 00856: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4278 - accuracy: 0.7982\n",
      "Epoch 857/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4096 - accuracy: 0.8000\n",
      "Epoch 00857: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4163 - accuracy: 0.8047\n",
      "Epoch 858/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4480 - accuracy: 0.7600\n",
      "Epoch 00858: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4249 - accuracy: 0.8021\n",
      "Epoch 859/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2908 - accuracy: 0.8600\n",
      "Epoch 00859: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4199 - accuracy: 0.7943\n",
      "Epoch 860/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4842 - accuracy: 0.8000\n",
      "Epoch 00860: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4394 - accuracy: 0.8008\n",
      "Epoch 861/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4148 - accuracy: 0.8400\n",
      "Epoch 00861: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4235 - accuracy: 0.7995\n",
      "Epoch 862/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5857 - accuracy: 0.7000\n",
      "Epoch 00862: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4302 - accuracy: 0.7917\n",
      "Epoch 863/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3793 - accuracy: 0.8600\n",
      "Epoch 00863: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4189 - accuracy: 0.8060\n",
      "Epoch 864/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4349 - accuracy: 0.8000\n",
      "Epoch 00864: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4211 - accuracy: 0.8021\n",
      "Epoch 865/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4032 - accuracy: 0.8400\n",
      "Epoch 00865: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4168 - accuracy: 0.8034\n",
      "Epoch 866/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2970 - accuracy: 0.8800\n",
      "Epoch 00866: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4252 - accuracy: 0.8073\n",
      "Epoch 867/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5462 - accuracy: 0.7200\n",
      "Epoch 00867: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4232 - accuracy: 0.8060\n",
      "Epoch 868/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3345 - accuracy: 0.8600\n",
      "Epoch 00868: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 598us/step - loss: 0.4404 - accuracy: 0.7943\n",
      "Epoch 869/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4425 - accuracy: 0.7800\n",
      "Epoch 00869: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4203 - accuracy: 0.8034\n",
      "Epoch 870/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4636 - accuracy: 0.7200\n",
      "Epoch 00870: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4201 - accuracy: 0.7891\n",
      "Epoch 871/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4458 - accuracy: 0.7800\n",
      "Epoch 00871: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4195 - accuracy: 0.7943\n",
      "Epoch 872/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4631 - accuracy: 0.7400\n",
      "Epoch 00872: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4302 - accuracy: 0.8060\n",
      "Epoch 873/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5734 - accuracy: 0.7600\n",
      "Epoch 00873: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4262 - accuracy: 0.8021\n",
      "Epoch 874/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4901 - accuracy: 0.7400\n",
      "Epoch 00874: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4311 - accuracy: 0.8138\n",
      "Epoch 875/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4280 - accuracy: 0.8200\n",
      "Epoch 00875: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4212 - accuracy: 0.8034\n",
      "Epoch 876/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4043 - accuracy: 0.8800\n",
      "Epoch 00876: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4265 - accuracy: 0.7930\n",
      "Epoch 877/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3461 - accuracy: 0.8600\n",
      "Epoch 00877: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4234 - accuracy: 0.7995\n",
      "Epoch 878/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3496 - accuracy: 0.8600\n",
      "Epoch 00878: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 665us/step - loss: 0.4315 - accuracy: 0.8060\n",
      "Epoch 879/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3704 - accuracy: 0.8200\n",
      "Epoch 00879: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.4261 - accuracy: 0.8008\n",
      "Epoch 880/1000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4098 - accuracy: 0.8400\n",
      "Epoch 00880: loss did not improve from 0.41628\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.4261 - accuracy: 0.8008\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, Y, epochs=1000, batch_size=50, callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "90ef0d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "accuracy = history.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5eb2c67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "58f598a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss, marker='.', c='r', label='loss')\n",
    "plt.plot(accuracy, marker='.', c='g', label='accuracy')\n",
    "plt.ylim(0,1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4d96a674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 564us/step - loss: 0.4132 - accuracy: 0.8099\n",
      "Loss; 41.32, Accuracy: 80.99\n"
     ]
    }
   ],
   "source": [
    "loss , accuracy = model.evaluate(X, Y)\n",
    "print('Loss; %.2f, Accuracy: %.2f' % (loss*100, accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5942485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.where(model.predict(X)>0.5, 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "848c8cc6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "db6fcc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0] => 1 (expected 1)\n",
      "[1.0, 85.0, 66.0, 29.0, 0.0, 26.6, 0.351, 31.0] => 0 (expected 0)\n",
      "[8.0, 183.0, 64.0, 0.0, 0.0, 23.3, 0.672, 32.0] => 1 (expected 1)\n",
      "[1.0, 89.0, 66.0, 23.0, 94.0, 28.1, 0.167, 21.0] => 0 (expected 0)\n",
      "[0.0, 137.0, 40.0, 35.0, 168.0, 43.1, 2.288, 33.0] => 1 (expected 1)\n"
     ]
    }
   ],
   "source": [
    "predictions = (model.predict(X) > 0.5).astype(int)\n",
    "for i in range(5):\n",
    "    print('%s => %d (expected %d)' % (X[i].tolist(), predictions[i], Y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b03da401",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('first_model.h5')  # save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "616fea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model('first_model.h5')  # load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db1fc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15714ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
